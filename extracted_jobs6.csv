job_id,job_title,company_name,location,workplace_type,employment_type,industry,job_description,seniority_level,experience_required,skills,salary_range
4251280296,SQL Developer Remote Trainee,Innovate Solutions,India (Remote),Save SQL Developer Remote Trainee at Innovate Solutions,Full-time,,"About the job Job Title: SQL Developer Trainee Location: Remote Job Type: Internship (Full-Time) Duration: 1‚Äì3 Months Stipend: ‚Çπ25,000/month Department: Data & Engineering Job Summary: We are seeking a detail-oriented and motivated SQL Developer Trainee to join our team remotely. This internship is designed for recent graduates or students who want to gain practical experience in database development, writing SQL queries, and working with data in real-world applications. Key Responsibilities: Write, test, and optimize SQL queries for data extraction and reporting Assist in designing and maintaining database structures (tables, views, indexes, etc.) Help ensure data integrity, accuracy, and security across systems Support the team in troubleshooting and debugging database-related issues Collaborate with developers and analysts to fulfill data requirements for projects Document query logic and database-related processes Qualifications: Bachelor‚Äôs degree (or final year student) in Computer Science, Information Technology, or related field Strong understanding of SQL and relational databases (e.g., MySQL, PostgreSQL, SQL Server) Familiarity with database design and normalization Analytical mindset with good problem-solving skills Ability to work independently in a remote setting Eagerness to learn and grow in a data-driven environment Preferred Skills (Nice to Have): Experience with procedures, triggers, or functions in SQL Exposure to BI/reporting tools (Power BI, Tableau, etc.) Understanding of data warehousing concepts Familiarity with cloud-based databases or platforms What We Offer: Monthly stipend of ‚Çπ25,000 Remote work opportunity Hands-on experience with real-world datasets and projects Mentorship and structured learning sessions Certificate of Completion Potential for full-time employment based on performance",,,"SQL, Tableau, Power BI",
4250160059,Python Engineer Intern,Robro Systems,"Indore, Madhya Pradesh, India (On-site)",On-site,Internship,,"About the job Job Title: Python Engineer Intern Location: Indore (On-site) Duration: 6 Months Stipend: 8000/- Company: Robro Systems About Robro Systems: Robro Systems is a fast-growing industrial automation company specializing in AI-driven solutions for quality inspection, material handling, and production line optimization. We combine cutting-edge technologies with practical industry experience to solve real-world manufacturing challenges. Role Overview: We are looking for a motivated Python Intern to join our development team. The intern will assist in building backend services, automation scripts, and AI data pipelines that power our industrial solutions. This is a great opportunity to work with a team of engineers and innovators in a fast-paced startup environment. Key Responsibilities: Write clean, efficient, and reusable Python code Support in developing and maintaining internal tools and APIs Work on integration of third-party APIs, hardware, and sensors Assist in testing and debugging software modules Collaborate with AI and automation teams on data processing and backend tasks Maintain documentation and follow best coding practices Requirements Requirements: Strong understanding of Python Familiarity with libraries Basic knowledge of databases Good problem-solving skills and a learning mindset Preferred Skills (Good to Have): Exposure to version control (Git) Knowledge of hardware integration Experience with OpenCV or basic ML frameworks What You‚Äôll Get: Real-world project experience with industrial applications Mentorship from experienced engineers and founders Certificate of completion and Letter of Recommendation Opportunity for full-time placement based on performance Desired Skills and Experience Requirements: Strong understanding of Python Familiarity with libraries like. Basic knowledge of databases Good problem-solving skills and a learning mindset Preferred Skills (Good to Have): Exposure to version control (Git) Knowledge of hardware integration Experience with OpenCV or basic ML frameworks",,,Python,
4245421566,Data Engineer-Data Platforms,IBM,"Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution. Your primary responsibilities include: Lead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements. Strive for continuous improvements by testing the build solution and working under an agile framework. Discover and implement the latest technologies trends to maximize and build creative solutions Preferred Education Master's Degree Required Technical And Professional Expertise Experience with Apache Spark (PySpark): In-depth knowledge of Spark‚Äôs architecture, core APIs, and PySpark for distributed data processing. Big Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modelling, and data warehousing concepts. Strong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy. SQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation. Cloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems Preferred Technical And Professional Experience Define, drive, and implement an architecture strategy and standards for end-to-end monitoring. Partner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering, Good to have detection and prevention tools for Company products and Platform and customer-facing",,,"Python, SQL, Data Analysis",
4230050481,Data Engineer,Uplers,"Patna, Bihar, India (Remote)",Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4246265831,Data Engineer (GCP Cloud),True Tech Professionals,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job Job Title: GCP Data Engineer Experience: 3‚Äì4 Years Location: Gurgaon (On-site) Shift: Rotational (Night shift allowance applicable) Joiners: Immediate only Role Overview: We are looking for a skilled and motivated GCP Data Engineer with 3‚Äì4 years of experience to join our growing team in Gurgaon . This role involves working in rotational shifts , including night shifts , for which additional allowance will be provided . The ideal candidate should have a strong command over Python, SQL, GCP, and BigQuery . Key Responsibilities: Develop, manage, and optimize data pipelines and workflows on Google Cloud Platform (GCP) Write and optimize complex SQL queries to transform and extract data Build scalable and efficient solutions using BigQuery Collaborate with data scientists and business teams to understand requirements and deliver data-driven solutions Monitor and troubleshoot production jobs and ensure data quality Work in rotational shifts to support global data operations Required Skills: Proficient in Python for data manipulation and scripting Strong in SQL ‚Äì writing, debugging, and optimizing queries Hands-on experience with Google Cloud Platform (GCP) Expertise in BigQuery for data warehousing and analytics Good understanding of data pipelines, ETL/ELT processes, and cloud data architecture Nice to Have: Experience working in a production environment with monitoring and alerting tools Knowledge of GCP services like Dataflow, Cloud Storage, and Pub/Sub Exposure to CI/CD pipelines and version control systems (e.g., Git) Additional Information: Location: Gurgaon (on-site role) Shift: Rotational (Night shift allowance will be provided) Notice Period: Immediate Joiners Preferred",,,"Python, SQL",
4251119410,Data Engineer,Pernod Ricard India,"Gurugram, Haryana, India (On-site)",On-site,Part-time,,"About the job JOB SUMMARY: The Data Engineer interprets data requirements for a specific data/tech product & drives the design, development & implementation of relevant data models based on both external/internal assets. They develop & maintain required enablers and platforms in data lake environment, securing data accessibility & integrity throughout all relevant processes KEY RESPONSIBILITIES: Engage with key stakeholders to identify data requirements for a specific data/tech product Design, build & maintain systems that capture, collect, manage, and convert raw data into usable information, securing quality & integrity (implementation of specific software for appropriate data management) Develop mechanisms to ingest, analyze, validate, normalize and clean data , supporting key user needs (standardization, customization), build interfaces & retention models which requires synthesizing or anonymizing Implement & maintain relevant procedures to secure data accessibility & quality (on new data sources uncovered by data scientists) Secure effective integration of built models/systems within PR environment, connecting with relevant architects/engineers, and drive continuous improvement initiatives (including maintenance.) Support data teams at key steps , sharing relevant insights/expertise (advice on data sourcing and preparation to data scientists, on data analytics & visualization concepts, methods & techniques.) Provide data engineering best practices & bring forward new ways of thinking around data to improve business outcomes Mentor other Data Engineers supporting them in complex scenarios leveraging past experiences and developing new standards Participate in transversal data engineering initiatives (market intelligence, cross-product/family initiatives.) as needed, and continuously develop their own skills based on industry trends/enterprise needs GEOGRAPHICAL SCOPE : Scope : Global Travel : Very Limited INTERACTIONS : Reporting Line (direct/indirect) : Reports to Data Engineering Chapter Lead, working in a matrix organization Key internal stakeholders : Squad Members (Data or GES, including Data Scientists/Analysts, Data Architect), BI Analysts, Data Governance Team, Product Owners. Product Managers etc. Key external stakeholders : Data Engineering Supplier, External Data Providers for product scopes FUNCTIONAL SKILLS: Core On-Cloud Data Engineering skills, including data extracting & storage, data transform & load. Data tools: Azure, SQL, Snowflake, Python, DBT, Lakehouse Architecture, Databricks, ADF, LogicApp, API Mgmt. and Azure Functions Project management & support : JIRA projects & service desk, Confluence, Sharepoint Mastery of data governance, architecture & security principles Background in software engineering/development (scripting & querying...) Knowledge of innovative technologies is a plus Strong communication skills, with the ability to talk with both technical & non-technical stakeholders Agile ways of working (collaboration, CD/CI) PAST EXPERIENCE: Bachelors or Masters in Computer Sciences 8 Years of experience as Data Engineer Experience in an FMCG/CPG company is a strong plus Lead & Co-ordination experience for other data engineers",Manager,,"Python, SQL",
4245900793,"Data Engineer, WW Returns & ReComm Tech& Inn",Amazon,"Hyderabad, Telangana, India",,Full-time,,"About the job Description The Data Engineer will own the data infrastructure for the Reverse Logistics Team which includes collaboration with software development teams to build the data infrastructure and maintain a highly scalable, reliable and efficient data system to support the fast growing business. You will work with analytic tools, can write excellent SQL scripts, optimize performance of SQL queries and can partner with internal customers to answer key business questions. We look for candidates who are self-motivated, flexible, hardworking and who like to have fun. About The Team Reverse Logistics team at Amazon Hyderabad Development Center is an agile team whose charter is to deliver the next generation of Reverse Logistics platform. As a member of this team, your mission will be to design, develop, document and support massively scalable, distributed data warehousing, querying and reporting system. Basic Qualifications 2+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Knowledge of AWS Infrastructure Knowledge of writing and optimizing SQL queries in a business environment with large-scale, complex datasets Strong analytical and problem solving skills. Curious, self-motivated & a self-starter with a ‚Äòcan do attitude‚Äô. Comfortable working in fast paced dynamic environment Preferred Qualifications Bachelor's degree in a quantitative/technical field such as computer science, engineering, statistics Proven track record of strong interpersonal and communication (verbal and written) skills. Experience developing insights across various areas of customer-related data: financial, product, and marketing Proven problem solving skills, attention to detail, and exceptional organizational skills Ability to deal with ambiguity and competing objectives in a fast paced environment Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing and operations Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - ADCI HYD 13 SEZ Job ID: A3001619",,,"Python, SQL",
4247925795,AWS Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Primary Skills Data Engineer , JD Responsible to lead the team and at the same time canidate should be able to get his/her hands dirty by writing code or contributing to any of the development lifecyle.AWS ,RedshiftEMR Cloud ETL Tools S3 We are looking for a Senior Consultant with at least 5 years of experience to join our team. The ideal candidate should have strong leadership skills and be able to lead a team effectively. At the same time, the candidate should also be willing to get their hands dirty by writing code and contributing to the development lifecycle. The primary skills required for this role include expertise in AWS Redshift and AWS Native Services, as well as experience with EMR, cloud ETL tools, and S3. The candidate should have a strong understanding of these tools and be able to utilize them effectively to meet project goals. As a Senior Consultant, the candidate will be responsible for providing guidance and support to the team, as well as ensuring the successful completion of projects. This is a hybrid work mode position, requiring the candidate to work both remotely and in the office as needed. Desired Skills and Experience Redshift, EMR, S3",,,,
4252085915,Lemon Tree Hotels - Data Engineer/Senior Engineer - ETL/Data Warehousing,Lemon Tree Hotels,"Delhi, Delhi, India (On-site)",On-site,Full-time,,"About the job This job is sourced from a job board. Learn More Key Responsibilities Design, build, and maintain scalable, reliable, and efficient data pipelines to support data analytics and business intelligence needs. Optimize and automate data workflows, enhancing the efficiency of data processing and reducing latency. Implement and maintain data storage solutions, ensuring that data is organized, secure, and readily accessible. Provide expertise in ETL processes, data wrangling, and data transformation techniques. Collaborate with technology teams to ensure that data engineering solutions align with overall business goals. Stay current with industry best practices and emerging technologies in data engineering, implementing improvements as : Bachelors or Masters degree in Computer Science, Information Technology, Engineering, or a related field. Experience with Agile methodologies and software development project : Proven experience in data engineering, with expertise in building and managing data pipelines, ETL processes, and data warehousing. Proficiency in SQL, Python, and other programming languages commonly used in data engineering. Experience with cloud platforms such as AWS, Azure, or Google Cloud, and familiarity with cloud-based data storage and processing tools (e.g., S3, Redshift, BigQuery, etc.). Good to have familiarity with big data technologies (e.g., Hadoop, Spark) and real-time data processing. Strong understanding of database management systems and data modeling techniques. Experience with BI tools like Tableau, Power BI along with ETL tools like Alteryx, or similar, and ability to work closely with analytics teams. High attention to detail and commitment to data quality and accuracy. Ability to work independently and as part of a team, with strong collaboration skills. Highly adaptive and comfortable working within a complex, fast-paced environment. (ref:hirist.tech)",,,"Python, SQL, Tableau, Power BI",
4245226143,Big Data Engineer,Weekday (YC W21),"Bengaluru, Karnataka, India (On-site)",On-site,‚Çπ300K/yr - ‚Çπ1M/yr,"Technology, Information and Media Seniority level: Mid-Senior level Min Experience: 5 years Location: Bengaluru, India, Karnataka JobType: full-time We are seeking a Big Data Engineer with deep technical expertise to join our fast-paced, data-driven team. In this role, you will be responsible for designing and building robust, scalable, and high-performance data pipelines that fuel real-time analytics, business intelligence, and machine learning applications across the organization. If you thrive on working with large datasets, cutting-edge technologies, and solving complex data engineering challenges, this is the opportunity for you. What You‚Äôll Do Design & Build Pipelines : Develop efficient, reliable, and scalable data pipelines that process large volumes of structured and unstructured data using big data tools. Distributed Data Processing : Leverage the Hadoop ecosystem (HDFS, Hive, MapReduce) to manage and transform massive datasets. Starburst (Trino) Integration : Design and optimize federated queries using Starburst, enabling seamless access across diverse data platforms. Databricks Lakehouse Development : Utilize Spark, Delta Lake, and MLflow on the Databricks Lakehouse Platform to enable unified analytics and AI workloads. Data Modeling & Architecture : Work with stakeholders to translate business requirements into flexible, scalable data models and architecture. Performance & Optimization : Monitor, troubleshoot, and fine-tune pipeline performance to ensure efficiency, reliability, and data integrity. Security & Compliance : Implement and enforce best practices for data privacy, security, and compliance with global regulations like GDPR and CCPA. Collaboration : Partner with data scientists, product teams, and business users to deliver impactful data solutions and improve decision-making. What You Bring Must-Have Skills 5+ years of hands-on experience in big data engineering, data platform development, or similar roles. Strong experience with Hadoop , including HDFS, Hive, HBase, and MapReduce. Deep understanding and practical use of Starburst (Trino) or Presto for large-scale querying. Hands-on experience with Databricks Lakehouse Platform , Spark, and Delta Lake. Proficient in SQL and programming languages like Python or Scala . Strong knowledge of data warehousing, ETL/ELT workflows, and schema design. Familiarity with CI/CD tools, version control (Git), and workflow orchestration tools (Airflow or similar). Nice-to-Have Skills Experience with cloud environments such as AWS , Azure , or GCP . Exposure to Docker , Kubernetes , or infrastructure-as-code tools. Understanding of data governance and metadata management platforms. Experience supporting AI/ML initiatives with curated datasets and pipelines. Job search faster with Premium Access company insights like strategic priorities, headcount trends, and more Sivaraman and millions of other members use Premium Retry Premium for ‚Çπ0 1-month free trial. Cancel whenever. We‚Äôll remind you 7 days before your trial ends. About the company Weekday (YC W21) 326,167 followers Follow Human Resources Services 11-50 employees 134 on LinkedIn We help companies hire engineers vouched by other techies. We are a sourcing engine on auto-pilot; where startups can not only find candidates and do outreach to them automatically but also get reference checks on them in an instant. After 4 days of signing up, they start getting candidates with ready back channel references on their calendar to interview. We are backed by Y-Combinator and were also ranked #1 on Product Hunt. ‚Ä¶ show more Show more","About the job This role is for one of our clients Industry: Technology, Information and Media Seniority level: Mid-Senior level Min Experience: 5 years Location: Bengaluru, India, Karnataka JobType: full-time We are seeking a Big Data Engineer with deep technical expertise to join our fast-paced, data-driven team. In this role, you will be responsible for designing and building robust, scalable, and high-performance data pipelines that fuel real-time analytics, business intelligence, and machine learning applications across the organization. If you thrive on working with large datasets, cutting-edge technologies, and solving complex data engineering challenges, this is the opportunity for you. What You‚Äôll Do Design & Build Pipelines : Develop efficient, reliable, and scalable data pipelines that process large volumes of structured and unstructured data using big data tools. Distributed Data Processing : Leverage the Hadoop ecosystem (HDFS, Hive, MapReduce) to manage and transform massive datasets. Starburst (Trino) Integration : Design and optimize federated queries using Starburst, enabling seamless access across diverse data platforms. Databricks Lakehouse Development : Utilize Spark, Delta Lake, and MLflow on the Databricks Lakehouse Platform to enable unified analytics and AI workloads. Data Modeling & Architecture : Work with stakeholders to translate business requirements into flexible, scalable data models and architecture. Performance & Optimization : Monitor, troubleshoot, and fine-tune pipeline performance to ensure efficiency, reliability, and data integrity. Security & Compliance : Implement and enforce best practices for data privacy, security, and compliance with global regulations like GDPR and CCPA. Collaboration : Partner with data scientists, product teams, and business users to deliver impactful data solutions and improve decision-making. What You Bring Must-Have Skills 5+ years of hands-on experience in big data engineering, data platform development, or similar roles. Strong experience with Hadoop , including HDFS, Hive, HBase, and MapReduce. Deep understanding and practical use of Starburst (Trino) or Presto for large-scale querying. Hands-on experience with Databricks Lakehouse Platform , Spark, and Delta Lake. Proficient in SQL and programming languages like Python or Scala . Strong knowledge of data warehousing, ETL/ELT workflows, and schema design. Familiarity with CI/CD tools, version control (Git), and workflow orchestration tools (Airflow or similar). Nice-to-Have Skills Experience with cloud environments such as AWS , Azure , or GCP . Exposure to Docker , Kubernetes , or infrastructure-as-code tools. Understanding of data governance and metadata management platforms. Experience supporting AI/ML initiatives with curated datasets and pipelines.",,,"Python, SQL, Machine Learning",
4240956531,Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Join Amgen‚Äôs Mission of Serving Patients At Amgen, if you feel like you‚Äôre part of something bigger, it‚Äôs because you are. Our shared mission‚Äîto serve patients living with serious illnesses‚Äîdrives all that we do. Since 1980, we‚Äôve helped pioneer the world of biotech in our fight against the world‚Äôs toughest diseases. With our focus on four therapeutic areas ‚ÄìOncology, Inflammation, General Medicine, and Rare Disease‚Äì we reach millions of patients each year. As a member of the Amgen team, you‚Äôll help make a lasting impact on the lives of patients as we research, manufacture, and deliver innovative medicines to help people live longer, fuller happier lives. Our award-winning culture is collaborative, innovative, and science based. If you have a passion for challenges and the opportunities that lay within them, you‚Äôll thrive as part of the Amgen team. Join us and transform the lives of patients while transforming your career. Data Engineer What You Will Do Let‚Äôs do this. Let‚Äôs change the world. In this vital role you will be responsible for designing, building, maintaining, analyzing, and interpreting data to provide actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and performing data governance initiatives and, visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has deep technical skills, experience with big data technologies, and a deep understanding of data architecture and ETL processes. Roles & Responsibilities: Design, develop, and maintain data solutions for data generation, collection, and processing Be a crucial team member that assists in design and development of the data pipeline Build data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems Contribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions Take ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs Develop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency Implement data security and privacy measures to protect sensitive data Leverage cloud platforms (AWS preferred) to build scalable and efficient data solutions Collaborate and communicate effectively with product teams Collaborate with Data Architects, Business SMEs, and Data Scientists to design and develop end-to-end data pipelines to meet fast-paced business needs across geographic regions Identify and resolve complex data-related challenges Adhere to best practices for coding, testing, and designing reusable code/component Explore new tools and technologies that will help to improve ETL platform performance Participate in sprint planning meetings and provide estimations on technical implementation What We Expect Of You We are all different, yet we all use our unique contributions to serve patients. Basic Qualifications: Master‚Äôs degree and 1 to 3 years of Computer Science, IT or related field experience OR Bachelor‚Äôs degree and 3 to 5 years of Computer Science, IT or related field experience OR Diploma and 7 to 9 years of Computer Science, IT or related field experience Preferred Qualifications: Must-Have Skills: Hands-on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing Proficiency in data analysis tools (eg. SQL) and experience with data visualization tools Excellent problem-solving skills and the ability to work with large, complex datasets Solid understanding of data governance frameworks, tools, and best practices. Knowledge of data protection regulations and compliance requirements Good-to-Have Skills: Experience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development Good understanding of data modeling, data warehousing, and data integration concepts Knowledge of Python/R, Databricks, SageMaker, cloud data platforms Professional Certifications Certified Data Engineer / Data Analyst (preferred on Databricks or cloud environments) Soft Skills: Excellent critical-thinking and problem-solving skills Good communication and collaboration skills Demonstrated awareness of how to function in a team setting Demonstrated presentation skills What You Can Expect Of Us As we work to develop treatments that take care of others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we‚Äôll support your journey every step of the way. In addition to the base salary, Amgen offers competitive and comprehensive Total Rewards Plans that are aligned with local industry standards. Apply now and make a lasting impact with the Amgen team. careers.amgen.com As an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease. Amgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Associate,,"Python, SQL, R, Machine Learning, Data Analysis",
4240956531,Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Join Amgen‚Äôs Mission of Serving Patients At Amgen, if you feel like you‚Äôre part of something bigger, it‚Äôs because you are. Our shared mission‚Äîto serve patients living with serious illnesses‚Äîdrives all that we do. Since 1980, we‚Äôve helped pioneer the world of biotech in our fight against the world‚Äôs toughest diseases. With our focus on four therapeutic areas ‚ÄìOncology, Inflammation, General Medicine, and Rare Disease‚Äì we reach millions of patients each year. As a member of the Amgen team, you‚Äôll help make a lasting impact on the lives of patients as we research, manufacture, and deliver innovative medicines to help people live longer, fuller happier lives. Our award-winning culture is collaborative, innovative, and science based. If you have a passion for challenges and the opportunities that lay within them, you‚Äôll thrive as part of the Amgen team. Join us and transform the lives of patients while transforming your career. Data Engineer What You Will Do Let‚Äôs do this. Let‚Äôs change the world. In this vital role you will be responsible for designing, building, maintaining, analyzing, and interpreting data to provide actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and performing data governance initiatives and, visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has deep technical skills, experience with big data technologies, and a deep understanding of data architecture and ETL processes. Roles & Responsibilities: Design, develop, and maintain data solutions for data generation, collection, and processing Be a crucial team member that assists in design and development of the data pipeline Build data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems Contribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions Take ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs Develop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency Implement data security and privacy measures to protect sensitive data Leverage cloud platforms (AWS preferred) to build scalable and efficient data solutions Collaborate and communicate effectively with product teams Collaborate with Data Architects, Business SMEs, and Data Scientists to design and develop end-to-end data pipelines to meet fast-paced business needs across geographic regions Identify and resolve complex data-related challenges Adhere to best practices for coding, testing, and designing reusable code/component Explore new tools and technologies that will help to improve ETL platform performance Participate in sprint planning meetings and provide estimations on technical implementation What We Expect Of You We are all different, yet we all use our unique contributions to serve patients. Basic Qualifications: Master‚Äôs degree and 1 to 3 years of Computer Science, IT or related field experience OR Bachelor‚Äôs degree and 3 to 5 years of Computer Science, IT or related field experience OR Diploma and 7 to 9 years of Computer Science, IT or related field experience Preferred Qualifications: Must-Have Skills: Hands-on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing Proficiency in data analysis tools (eg. SQL) and experience with data visualization tools Excellent problem-solving skills and the ability to work with large, complex datasets Solid understanding of data governance frameworks, tools, and best practices. Knowledge of data protection regulations and compliance requirements Good-to-Have Skills: Experience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development Good understanding of data modeling, data warehousing, and data integration concepts Knowledge of Python/R, Databricks, SageMaker, cloud data platforms Professional Certifications Certified Data Engineer / Data Analyst (preferred on Databricks or cloud environments) Soft Skills: Excellent critical-thinking and problem-solving skills Good communication and collaboration skills Demonstrated awareness of how to function in a team setting Demonstrated presentation skills What You Can Expect Of Us As we work to develop treatments that take care of others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we‚Äôll support your journey every step of the way. In addition to the base salary, Amgen offers competitive and comprehensive Total Rewards Plans that are aligned with local industry standards. Apply now and make a lasting impact with the Amgen team. careers.amgen.com As an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease. Amgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Associate,,"Python, SQL, R, Machine Learning, Data Analysis",
4249544592,Data Engineer,Tata Consultancy Services,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Greetings from Tata Consultancy Services Join the Walk-in Drive on 21st June 2025 and Pave your path to value with TCS AI Cloud Team We are Hiring for Below Skills Exp : 4 yrs to 12 yrs Azure Data Engineer Required:Implementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW",,,SQL,
4249544592,Data Engineer,Tata Consultancy Services,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Greetings from Tata Consultancy Services Join the Walk-in Drive on 21st June 2025 and Pave your path to value with TCS AI Cloud Team We are Hiring for Below Skills Exp : 4 yrs to 12 yrs Azure Data Engineer Required:Implementation, and operations of OLTP, OLAP, DW technologies such as Azure SQL, Azure SQL DW",,,SQL,
4248826228,Data Engineer,RemoteStar,India (Remote),Save Data Engineer at RemoteStar,Full-time,,"About the job About the role: As a Data Engineer, you will be instrumental in managing our extensive soil carbon dataset and creating robust data systems. You are expected to be involved in the full project lifecycle, from planning and design, through development, and onto maintenance, including pipelines and dashboards. You‚Äôll interact with Product Managers, Project Managers, Business Development and Operations teams to understand business demands and translate them into technical solutions. Your goal is to provide an organisation-wide source of truth for various downstream activities while also working towards improving and modernising our current platform. Key responsibilities: Design, develop, and maintain scalable data pipelines to process soil carbon and agricultural data Create and optimise database schemas and queries Implement data quality controls and validation processes Adapt existing data flows and schemas to new products and services under development Required qualifications: BS/B. Tech in Computer Science or equivalent practical experience, with 5-7 years as a Data Engineer or similar role. Strong SQL skills and experience optimising complex queries Proficiency with relational databases, preferably MySQL Experience building data pipelines, transformations, and dashboards Ability to troubleshoot and fix performance and data issues across the database Experience with AWS services (especially Glue, S3, RDS) Exposure to big data eco-system ‚Äì Snowflake/Redshift/Tableau/Looker Python programming skills Excellent written and verbal communication skills in English An ideal candidate would also have: High degree of attention to detail to uncover data discrepancies and fix them Familiarity with geospatial data Experience with scientific or environmental datasets Some understanding of the agritech or environmental sustainability sectors",Manager,,"Python, SQL, Tableau",
4248826228,Data Engineer,RemoteStar,India (Remote),Save Data Engineer at RemoteStar,Full-time,,"About the job About the role: As a Data Engineer, you will be instrumental in managing our extensive soil carbon dataset and creating robust data systems. You are expected to be involved in the full project lifecycle, from planning and design, through development, and onto maintenance, including pipelines and dashboards. You‚Äôll interact with Product Managers, Project Managers, Business Development and Operations teams to understand business demands and translate them into technical solutions. Your goal is to provide an organisation-wide source of truth for various downstream activities while also working towards improving and modernising our current platform. Key responsibilities: Design, develop, and maintain scalable data pipelines to process soil carbon and agricultural data Create and optimise database schemas and queries Implement data quality controls and validation processes Adapt existing data flows and schemas to new products and services under development Required qualifications: BS/B. Tech in Computer Science or equivalent practical experience, with 5-7 years as a Data Engineer or similar role. Strong SQL skills and experience optimising complex queries Proficiency with relational databases, preferably MySQL Experience building data pipelines, transformations, and dashboards Ability to troubleshoot and fix performance and data issues across the database Experience with AWS services (especially Glue, S3, RDS) Exposure to big data eco-system ‚Äì Snowflake/Redshift/Tableau/Looker Python programming skills Excellent written and verbal communication skills in English An ideal candidate would also have: High degree of attention to detail to uncover data discrepancies and fix them Familiarity with geospatial data Experience with scientific or environmental datasets Some understanding of the agritech or environmental sustainability sectors",Manager,,"Python, SQL, Tableau",
4247044255,Risk Data Engineer/ Leads,EY,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job At EY, you‚Äôll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we‚Äôre counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. RCE - Risk Data Engineer/Leads Job Description: Our Technology team builds innovative digital solutions rapidly and at scale to deliver the next generation of Financial and Non- Financial services across the globe. The Position is a senior technical, hands-on delivery role, requiring the knowledge of data engineering, cloud infrastructure and platform engineering, platform operations and production support using ground-breaking cloud and big data technologies. The ideal candidate with 3-6 years of relevant experience, will possess strong technical skills, an eagerness to learn, a keen interest on 3 keys pillars that our team support i.e. Financial Crime, Financial Risk and Compliance technology transformation, the ability to work collaboratively in fast-paced environment, and an aptitude for picking up new tools and techniques on the job, building on existing skillsets as a foundation. Senior Data & BI Analyst Job Summary: The Senior Data & BI Analyst is a key contributor to the Enterprise Analytics Center of Excellence (COE), responsible for promoting and supporting self-service analytics across the firm using tools such as ThoughtSpot and Tableau. This role requires a strong technical foundation in data analytics and BI tools, combined with strategic consulting skills to guide users in developing scalable, high-quality analytics. The ideal candidate has a passion for data, an understanding of financial services, and the ability to collaborate across teams to enable data-driven decision-making. Responsibilities: Contribute to the buildout and ongoing evolution of the Enterprise Analytics COE, shaping best practices and governance standards. Act as a subject matter expert (SME) in enterprise lakehouse data, guiding end users in identifying and leveraging key data sources. Work closely with business users to understand their analytics needs, providing consultation on BI tool selection and data strategies. Lead training sessions for ThoughtSpot (and Tableau) users to enhance adoption and proficiency in self-service analytics. Perform hands-on data analysis against enterprise lakehouse data to uncover insights and demonstrate tool capabilities. Develop reusable analytics templates, dashboards, and frameworks that drive consistency and efficiency across the organization. Support initial dashboard and analytics development efforts, assisting teams with prototyping and best practices implementation. Stay up to date on emerging BI trends, ThoughtSpot and Tableau advancements, and AI-driven analytics approaches to drive innovation. Required Qualifications: Experience working as a Technical Data Analyst, BI Analyst, or Business Analyst in an enterprise environment. Strong SQL skills and experience handling large datasets. Proficiency in data warehousing concepts, ETL processes, and BI reporting tools such as Tableau, Power BI, Qlik, ThoughtSpot. Excellent analytical and problem-solving skills with the ability to collaborate across cross-functional teams. Strong stakeholder management abilities, with the ability to effectively communicate technical insights to non-technical audiences. Preferred Qualifications: Hands-on experience with ThoughtSpot BI Platform. Financial services experience and understanding of financial data structures. Familiarity with cloud-based BI platforms (AWS, Azure, etc.). Knowledge of Natural Language Processing (NLP), AI-driven data aggregation, and automated reporting technologies. EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",,,"SQL, Tableau, Power BI, Data Analysis",
4247044255,Risk Data Engineer/ Leads,EY,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job At EY, you‚Äôll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we‚Äôre counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. RCE - Risk Data Engineer/Leads Job Description: Our Technology team builds innovative digital solutions rapidly and at scale to deliver the next generation of Financial and Non- Financial services across the globe. The Position is a senior technical, hands-on delivery role, requiring the knowledge of data engineering, cloud infrastructure and platform engineering, platform operations and production support using ground-breaking cloud and big data technologies. The ideal candidate with 3-6 years of relevant experience, will possess strong technical skills, an eagerness to learn, a keen interest on 3 keys pillars that our team support i.e. Financial Crime, Financial Risk and Compliance technology transformation, the ability to work collaboratively in fast-paced environment, and an aptitude for picking up new tools and techniques on the job, building on existing skillsets as a foundation. Senior Data & BI Analyst Job Summary: The Senior Data & BI Analyst is a key contributor to the Enterprise Analytics Center of Excellence (COE), responsible for promoting and supporting self-service analytics across the firm using tools such as ThoughtSpot and Tableau. This role requires a strong technical foundation in data analytics and BI tools, combined with strategic consulting skills to guide users in developing scalable, high-quality analytics. The ideal candidate has a passion for data, an understanding of financial services, and the ability to collaborate across teams to enable data-driven decision-making. Responsibilities: Contribute to the buildout and ongoing evolution of the Enterprise Analytics COE, shaping best practices and governance standards. Act as a subject matter expert (SME) in enterprise lakehouse data, guiding end users in identifying and leveraging key data sources. Work closely with business users to understand their analytics needs, providing consultation on BI tool selection and data strategies. Lead training sessions for ThoughtSpot (and Tableau) users to enhance adoption and proficiency in self-service analytics. Perform hands-on data analysis against enterprise lakehouse data to uncover insights and demonstrate tool capabilities. Develop reusable analytics templates, dashboards, and frameworks that drive consistency and efficiency across the organization. Support initial dashboard and analytics development efforts, assisting teams with prototyping and best practices implementation. Stay up to date on emerging BI trends, ThoughtSpot and Tableau advancements, and AI-driven analytics approaches to drive innovation. Required Qualifications: Experience working as a Technical Data Analyst, BI Analyst, or Business Analyst in an enterprise environment. Strong SQL skills and experience handling large datasets. Proficiency in data warehousing concepts, ETL processes, and BI reporting tools such as Tableau, Power BI, Qlik, ThoughtSpot. Excellent analytical and problem-solving skills with the ability to collaborate across cross-functional teams. Strong stakeholder management abilities, with the ability to effectively communicate technical insights to non-technical audiences. Preferred Qualifications: Hands-on experience with ThoughtSpot BI Platform. Financial services experience and understanding of financial data structures. Familiarity with cloud-based BI platforms (AWS, Azure, etc.). Knowledge of Natural Language Processing (NLP), AI-driven data aggregation, and automated reporting technologies. EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",,,"SQL, Tableau, Power BI, Data Analysis",
4245417840,Data Engineer-Data Platforms,IBM,"Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution. Your primary responsibilities include: Lead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements. Strive for continuous improvements by testing the build solution and working under an agile framework. Discover and implement the latest technologies trends to maximize and build creative solutions Preferred Education Master's Degree Required Technical And Professional Expertise Experience with Apache Spark (PySpark): In-depth knowledge of Spark‚Äôs architecture, core APIs, and PySpark for distributed data processing. Big Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts. Strong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy. SQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation. Cloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems Preferred Technical And Professional Experience Define, drive, and implement an architecture strategy and standards for end-to-end monitoring. Partner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering, Good to have detection and prevention tools for Company products and Platform and customer-facing",,,"Python, SQL, Data Analysis",
4245417840,Data Engineer-Data Platforms,IBM,"Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution. Your primary responsibilities include: Lead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements. Strive for continuous improvements by testing the build solution and working under an agile framework. Discover and implement the latest technologies trends to maximize and build creative solutions Preferred Education Master's Degree Required Technical And Professional Expertise Experience with Apache Spark (PySpark): In-depth knowledge of Spark‚Äôs architecture, core APIs, and PySpark for distributed data processing. Big Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts. Strong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy. SQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation. Cloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems Preferred Technical And Professional Experience Define, drive, and implement an architecture strategy and standards for end-to-end monitoring. Partner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering, Good to have detection and prevention tools for Company products and Platform and customer-facing",,,"Python, SQL, Data Analysis",
4245419551,AWS data engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Minimum 6 years of hands-on experience in data engineering or big data development roles. Strong programming skills in Python and experience with Apache Spark (PySpark preferred). Proficient in writing and optimizing complex SQL queries. Hands-on experience with Apache Airflow for orchestration of data workflows. Deep Understanding And Practical Experience With AWS Services Data Storage & Processing: S3, Glue, EMR, Athena Compute & Execution: Lambda, Step Functions Databases: RDS, DynamoDB Monitoring: CloudWatch Experience with distributed data processing, parallel computing, and performance tuning. Strong analytical and problem-solving skills. Familiarity with CI/CD pipelines and DevOps practices is a plus. Desired Skills and Experience RDBMS",,,"Python, SQL",
4245419551,AWS data engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Minimum 6 years of hands-on experience in data engineering or big data development roles. Strong programming skills in Python and experience with Apache Spark (PySpark preferred). Proficient in writing and optimizing complex SQL queries. Hands-on experience with Apache Airflow for orchestration of data workflows. Deep Understanding And Practical Experience With AWS Services Data Storage & Processing: S3, Glue, EMR, Athena Compute & Execution: Lambda, Step Functions Databases: RDS, DynamoDB Monitoring: CloudWatch Experience with distributed data processing, parallel computing, and performance tuning. Strong analytical and problem-solving skills. Familiarity with CI/CD pipelines and DevOps practices is a plus. Desired Skills and Experience RDBMS",,,"Python, SQL",
4231646736,Looker Data Engineer/Architect,myKaarma,"Noida, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Job Title : Looker Data Engineer Job Type : Full-Time, Salaried Job Location : Hybrid: Noida, India Company Overview: At myKaarma, we‚Äôre not just leading the way in fixed ops solutions for the automotive industry‚Äîwe‚Äôre redefining what‚Äôs possible for dealership service centers. Headquartered in Long Beach, California, and powered by a global team, our industry-leading SaaS platform combines communication, scheduling, and payment tools in one seamless solution that keeps dealerships and vehicle owners connected. With myKaarma, every service interaction flows effortlessly, bringing good karma to customers and service teams. Rooted in the principles of the Toyota Production System, we operate with precision, efficiency, and a relentless focus on continuous improvement to deliver a better experience for all. We‚Äôre looking for innovators, problem-solvers, and tech enthusiasts passionate about building solutions people love to use. If you‚Äôre ready to make an impact in an industry ripe for change, join us at myKaarma and help shape the future of automotive service. At myKaarma, exceptional customer interactions are the cornerstone of our success. Our platform is meticulously engineered to communicate with customers at every aspect of the service, repair, and maintenance journey, ensuring positive and memorable experiences. Role Description: We are building a modern data lake architecture centered around BigQuery and Looker, and we‚Äôre looking for a hands-on Looker Data Engineer/Architect to help us shape and scale our data platform. In this role, you‚Äôll own the design and implementation of Looker Views, Explores, and Dashboards, working closely with data stakeholders to ensure accurate, efficient, and business-relevant insights. You‚Äôll play a critical role in modelling our existing data architecture into LookML, and driving modelling and visualization best practices across the organization. This will also include reviewing our existing data lake models and identifying inefficiencies/areas of improvement. This role also offers the opportunity to integrate AI/ML in our data lake and provide intelligent insights and recommendations to our internal as well as external customers. Key Responsibilities: Design and develop LookML models, views, and explores based on our legacy data warehouse in MariaDB Create and maintain high-quality dashboards and visualizations in Looker that deliver actionable business insights Collaborate with engineers, product managers, and business stakeholders to gather requirements and translate them into scalable data models Guide other engineers and non-technical staff on how to build and maintain Looker dashboards and models. Ensure data accuracy, performance, and efficiency across our Looker and BigQuery resources Maintain strong ownership over the Looker platform, proactively improving structure, documentation, and data usability Monitor and troubleshoot data issues in Looker and BigQuery Required Skills and Qualifications : 5+ years of experience in data engineering and 2+ years of hands-on experience with Looker, including LookML modeling and dashboard development Strong experience with Google BigQuery, including writing and optimizing complex SQL queries, and managing BigQuery costs Experience with building and maintaining projects in Google Cloud Experience implementing row-level security, access controls, or data governance in Looker Proven ability to manage and own end-to-end Looker projects with minimal supervision Experience with source control systems, preferably git Excellent communication skills and a strong sense of ownership and accountability Comfortable working in a fast-paced, collaborative environment Nice to Have Skills & Qualifications Familiarity with batch processing, stream processing and real-time analytics Familiarity with MySQL queries and syntax Being able to understand and write java code We value diverse experiences and backgrounds, so we encourage you to apply if you meet some but not all of the listed qualifications. Total Rewards at myKaarma At myKaarma, we offer a comprehensive Total Rewards package that extends beyond the base salary. Our commitment to competitive compensation includes bonuses and benefits that support both personal and professional well-being: Flexible Work Environment : We embrace a high-performance, flexible structure that values freedom and responsibility. Our ‚ÄúHighly Aligned, Loosely Coupled‚Äù model empowers teams to innovate and continuously improve using data-driven insights. Health and Wellness : Comprehensive medical, life, and disability benefits Time Off: Generous vacation time to recharge and balance life outside work. In-Office Perks: Work in an agile office space with perks like ping pong and foosball to unwind and connect and unlimited lunch, snacks or refreshments onsite. Our Commitment to Inclusion At myKaarma, diverse perspectives drive innovation and success. We are committed to creating a safe, welcoming, and inclusive workplace where every employee feels valued, empowered, and can do meaningful work. Our mission to deliver exceptional solutions to our clients is strengthened by the unique contributions and perspectives of our team members from all backgrounds. As an equal opportunity employer, myKaarma prohibits any form of unlawful discrimination or harassment based on race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, family or parental status, disability, age, veteran status, or any other status protected by applicable laws in the regions where we operate. We adhere to all EEOC regulations and actively promote an environment that celebrates and supports diversity, equity, and inclusion for all. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done, which will ensure an equal employment opportunity without imposing undue hardship on myKaarma. Please let us know if you require reasonable accommodations during the application or interview process by filling out this form . myKaarma participates in the E-Verify Program .",manager,,SQL,
4231646736,Looker Data Engineer/Architect,myKaarma,"Noida, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Job Title : Looker Data Engineer Job Type : Full-Time, Salaried Job Location : Hybrid: Noida, India Company Overview: At myKaarma, we‚Äôre not just leading the way in fixed ops solutions for the automotive industry‚Äîwe‚Äôre redefining what‚Äôs possible for dealership service centers. Headquartered in Long Beach, California, and powered by a global team, our industry-leading SaaS platform combines communication, scheduling, and payment tools in one seamless solution that keeps dealerships and vehicle owners connected. With myKaarma, every service interaction flows effortlessly, bringing good karma to customers and service teams. Rooted in the principles of the Toyota Production System, we operate with precision, efficiency, and a relentless focus on continuous improvement to deliver a better experience for all. We‚Äôre looking for innovators, problem-solvers, and tech enthusiasts passionate about building solutions people love to use. If you‚Äôre ready to make an impact in an industry ripe for change, join us at myKaarma and help shape the future of automotive service. At myKaarma, exceptional customer interactions are the cornerstone of our success. Our platform is meticulously engineered to communicate with customers at every aspect of the service, repair, and maintenance journey, ensuring positive and memorable experiences. Role Description: We are building a modern data lake architecture centered around BigQuery and Looker, and we‚Äôre looking for a hands-on Looker Data Engineer/Architect to help us shape and scale our data platform. In this role, you‚Äôll own the design and implementation of Looker Views, Explores, and Dashboards, working closely with data stakeholders to ensure accurate, efficient, and business-relevant insights. You‚Äôll play a critical role in modelling our existing data architecture into LookML, and driving modelling and visualization best practices across the organization. This will also include reviewing our existing data lake models and identifying inefficiencies/areas of improvement. This role also offers the opportunity to integrate AI/ML in our data lake and provide intelligent insights and recommendations to our internal as well as external customers. Key Responsibilities: Design and develop LookML models, views, and explores based on our legacy data warehouse in MariaDB Create and maintain high-quality dashboards and visualizations in Looker that deliver actionable business insights Collaborate with engineers, product managers, and business stakeholders to gather requirements and translate them into scalable data models Guide other engineers and non-technical staff on how to build and maintain Looker dashboards and models. Ensure data accuracy, performance, and efficiency across our Looker and BigQuery resources Maintain strong ownership over the Looker platform, proactively improving structure, documentation, and data usability Monitor and troubleshoot data issues in Looker and BigQuery Required Skills and Qualifications : 5+ years of experience in data engineering and 2+ years of hands-on experience with Looker, including LookML modeling and dashboard development Strong experience with Google BigQuery, including writing and optimizing complex SQL queries, and managing BigQuery costs Experience with building and maintaining projects in Google Cloud Experience implementing row-level security, access controls, or data governance in Looker Proven ability to manage and own end-to-end Looker projects with minimal supervision Experience with source control systems, preferably git Excellent communication skills and a strong sense of ownership and accountability Comfortable working in a fast-paced, collaborative environment Nice to Have Skills & Qualifications Familiarity with batch processing, stream processing and real-time analytics Familiarity with MySQL queries and syntax Being able to understand and write java code We value diverse experiences and backgrounds, so we encourage you to apply if you meet some but not all of the listed qualifications. Total Rewards at myKaarma At myKaarma, we offer a comprehensive Total Rewards package that extends beyond the base salary. Our commitment to competitive compensation includes bonuses and benefits that support both personal and professional well-being: Flexible Work Environment : We embrace a high-performance, flexible structure that values freedom and responsibility. Our ‚ÄúHighly Aligned, Loosely Coupled‚Äù model empowers teams to innovate and continuously improve using data-driven insights. Health and Wellness : Comprehensive medical, life, and disability benefits Time Off: Generous vacation time to recharge and balance life outside work. In-Office Perks: Work in an agile office space with perks like ping pong and foosball to unwind and connect and unlimited lunch, snacks or refreshments onsite. Our Commitment to Inclusion At myKaarma, diverse perspectives drive innovation and success. We are committed to creating a safe, welcoming, and inclusive workplace where every employee feels valued, empowered, and can do meaningful work. Our mission to deliver exceptional solutions to our clients is strengthened by the unique contributions and perspectives of our team members from all backgrounds. As an equal opportunity employer, myKaarma prohibits any form of unlawful discrimination or harassment based on race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, family or parental status, disability, age, veteran status, or any other status protected by applicable laws in the regions where we operate. We adhere to all EEOC regulations and actively promote an environment that celebrates and supports diversity, equity, and inclusion for all. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done, which will ensure an equal employment opportunity without imposing undue hardship on myKaarma. Please let us know if you require reasonable accommodations during the application or interview process by filling out this form . myKaarma participates in the E-Verify Program .",manager,,SQL,
4230051376,Data Engineer,Uplers,"Jamshedpur, Jharkhand, India (Remote)",Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4230051376,Data Engineer,Uplers,"Jamshedpur, Jharkhand, India (Remote)",Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4236562147,Data Engineer-Azure Data Bricks,Tata Consultancy Services,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Build the solution for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure data ingestion and transformation components. Following technology skills are required ‚Äì Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience with ADF, Dataflow Experience with big data tools like Delta Lake, Azure Databricks Experience with Synapse Designing an Azure Data Solution skills Assemble large, complex data sets that meet functional / non-functional business requirements.",,,SQL,
4236562147,Data Engineer-Azure Data Bricks,Tata Consultancy Services,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Build the solution for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure data ingestion and transformation components. Following technology skills are required ‚Äì Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience with ADF, Dataflow Experience with big data tools like Delta Lake, Azure Databricks Experience with Synapse Designing an Azure Data Solution skills Assemble large, complex data sets that meet functional / non-functional business requirements.",,,SQL,
4119504199,Data Engineer,upGrad Rekrut,Mumbai Metropolitan Region (On-site),On-site,Full-time,,"About the job Job Role: ‚Ä¢ Minimum 3 years of previous industry work experience will be preferred ‚Ä¢In-depth understanding of database structure principles. ‚Ä¢ Knowledge of data mining and segmentation techniques, expertise in SQL and Oracle. ‚Ä¢ Familiarity with data visualization and data oriented. ‚Ä¢ Ability to document complex business processes and handle all types of customer requests. ‚Ä¢ Good communication skill in English; Math & Statistical analysis, ability to interpret and collate relevant data. ‚Ä¢ Should have working experience on on-premises and cloud-based data infrastructure handling large and diverse datasets ‚Ä¢ Experience in one or more of the below technologies is preferred ‚Ä¢ AWS/GCP/Azure ‚Ä¢ Kubernetes/Docker Swarm ‚Ä¢ Apache Hadoop & Apache Spark ‚Ä¢ Elastic Stack/Elk ‚Ä¢ Airflow / Prefect ‚Ä¢ MongoDB, Cassandra, Redis, Memcached and DynamoDB ‚Ä¢ MySQL, Cassandra, and Oracle SQL ‚Ä¢ PowerBI/Tableau/Qlik view",,,"SQL, Tableau",
4119504199,Data Engineer,upGrad Rekrut,Mumbai Metropolitan Region (On-site),On-site,Full-time,,"About the job Job Role: ‚Ä¢ Minimum 3 years of previous industry work experience will be preferred ‚Ä¢In-depth understanding of database structure principles. ‚Ä¢ Knowledge of data mining and segmentation techniques, expertise in SQL and Oracle. ‚Ä¢ Familiarity with data visualization and data oriented. ‚Ä¢ Ability to document complex business processes and handle all types of customer requests. ‚Ä¢ Good communication skill in English; Math & Statistical analysis, ability to interpret and collate relevant data. ‚Ä¢ Should have working experience on on-premises and cloud-based data infrastructure handling large and diverse datasets ‚Ä¢ Experience in one or more of the below technologies is preferred ‚Ä¢ AWS/GCP/Azure ‚Ä¢ Kubernetes/Docker Swarm ‚Ä¢ Apache Hadoop & Apache Spark ‚Ä¢ Elastic Stack/Elk ‚Ä¢ Airflow / Prefect ‚Ä¢ MongoDB, Cassandra, Redis, Memcached and DynamoDB ‚Ä¢ MySQL, Cassandra, and Oracle SQL ‚Ä¢ PowerBI/Tableau/Qlik view",,,"SQL, Tableau",
4248772470,PySpark Data Engineer,Viraaj HR Solutions Private Limited,"Pune, Maharashtra, India (On-site)",On-site,‚Çπ1.2M/yr - ‚Çπ2M/yr,,"About the job Company Overview Viraaj HR Solutions is a leading recruitment firm in India, dedicated to connecting top talent with industry-leading companies. We focus on understanding the unique needs of each client, providing tailored HR solutions that enhance their workforce capabilities. Our mission is to empower organizations by bridging the gap between talent and opportunity. We value integrity, collaboration, and excellence in service delivery, ensuring a seamless experience for both candidates and employers. Job Title: PySpark Data Engineer Work Mode: On-Site Location: India Role Responsibilities Design, develop, and maintain data pipelines using PySpark. Collaborate with data scientists and analysts to gather data requirements. Optimize data processing workflows for efficiency and performance. Implement ETL processes to integrate data from various sources. Create and maintain data models that support analytical reporting. Ensure data quality and accuracy through rigorous testing and validation. Monitor and troubleshoot production data pipelines to resolve issues. Work with SQL databases to extract and manipulate data as needed. Utilize cloud technologies for data storage and processing solutions. Participate in code reviews and provide constructive feedback. Document technical specifications and processes clearly for team reference. Stay updated with industry trends and emerging technologies in big data. Collaborate with cross-functional teams to deliver data solutions. Support the data governance initiatives to ensure compliance. Provide training and mentorship to junior data engineers. Qualifications Bachelor's degree in Computer Science, Information Technology, or related field. Proven experience as a Data Engineer, preferably with PySpark. Strong understanding of data warehousing concepts and architecture. Hands-on experience with ETL tools and frameworks. Proficiency in SQL and NoSQL databases. Familiarity with cloud platforms like AWS, Azure, or Google Cloud. Experience with Python programming for data manipulation. Knowledge of data modeling techniques and best practices. Ability to work in a fast-paced environment and juggle multiple tasks. Excellent problem-solving skills and attention to detail. Strong communication and interpersonal skills. Ability to work independently and as part of a team. Experience in Agile methodologies and practices. Knowledge of data governance and compliance standards. Familiarity with BI tools such as Tableau or Power BI is a plus. Skills: data modeling,python programming,pyspark,bi tools,sql proficiency,sql,cloud technologies,nosql databases,etl processes,data warehousing,agile methodologies,cloud computing,data engineer",,,"Python, SQL, Tableau, Power BI",
4243676009,Data Engineer,NatWest Group,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Join us as a Data Engineer This is an exciting opportunity to use your technical expertise to collaborate with colleagues and build effortless, digital first customer experiences You‚Äôll be simplifying the bank by developing innovative data driven solutions, using insight to be commercially successful, and keeping our customers‚Äô and the bank‚Äôs data safe and secure Participating actively in the data engineering community, you‚Äôll deliver opportunities to support the bank‚Äôs strategic direction while building your network across the bank We're offering this role at associate level What you'll do As a Data Engineer, you‚Äôll play a key role in driving value for our customers by building data solutions. You‚Äôll be carrying out data engineering tasks to build, maintain, test and optimise a scalable data architecture, as well as carrying out data extractions, transforming data to make it usable to data analysts and scientists, and loading data into data platforms. You‚Äôll Also Be Developing comprehensive knowledge of the bank‚Äôs data structures and metrics, advocating change where needed for product development Practicing DevOps adoption in the delivery of data engineering, proactively performing root cause analysis and resolving issues Collaborating closely with core technology and architecture teams in the bank to build data knowledge and data solutions Developing a clear understanding of data platform cost levers to build cost effective and strategic solutions Sourcing new data using the most appropriate tooling and integrating it into the overall solution to deliver for our customers The skills you'll need To be successful in this role, you‚Äôll need a good understanding of data usage and dependencies with wider teams and the end customer, as well as experience of extracting value and features from large scale data. You‚Äôll Also Demonstrate Experience of ETL technical design, including data quality testing, cleansing and monitoring, and data warehousing and data modelling capabilities Experience of using programming languages alongside knowledge of data and software engineering fundamentals Good knowledge of modern code development practices Strong communication skills with the ability to proactively engage with a wide range of stakeholders",associate,,,
4245215896,Associate Data Engineer,Tredence Inc.,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job About Tredence: Tredence is a global data science solutions provider founded in 2013 by Shub Bhowmick, Sumit Mehra, and Shashank Dubey focused on solving the last-mile problem in AI. Headquartered in San Jose, California, the company embraces a vertical-first approach and an outcome-driven mindset to help clients win and accelerate value realization from their analytics investments. The aim is to bridge the gap between insight delivery and value realization by providing customers with a differentiated approach to data and analytics through tailor-made solutions. Tredence is 3500-plus employees strong with offices in San Jose, Foster City, Chicago, London, Toranto, and Bangalore, with the largest companies in retail, CPG, hi-tech, telecom, healthcare, travel, and industrials as clients. As we complete 12 years of Tredence this year, we are on the cusp of an ambitious and exciting phase of expansion and growth. ¬∑Tredence recently closed a USD 175 million Series B funding , which will help us build on growth momentum, strengthen vertical capabilities, and reach a broader customer base. ¬∑Apart from our geographic footprint in the US, Canada & UK, we plan to open offices in Kolkata and a few tier 2 cities in India. In 2023, we also plan to hire more than 1000 employees across markets. ¬∑Tredence is a Great Place to Work (GPTW) certified company that values its employees and creates a positive work culture by providing opportunities for professional development and promoting work-life balance. ¬∑ At Tredence, nothing is impossible; we believe in pushing ourselves to limitless possibilities and staying true to our tagline, Beyond Possible This position requires someone with good problem solving, business understanding and client presence. Overall professional experience of the candidate should be atleast 5 years with a maximum experience upto 15 years. The candidate must understand the usage of data Engineering tools for solving business problems and help clients in their data journey. Must have knowledge of emerging technologies used in companies for data management including data governance, data quality, security, data integration, processing, and provisioning. The candidate must possess required soft skills to work with teams and lead medium to large teams. Candidate should be comfortable with taking leadership roles, in client projects, pre-sales/consulting, solutioning, business development conversations, execution on data engineering projects. Job Summary: We are seeking a skilled Data Engineer with strong hands-on experience in SQL , PySpark , and cloud-based data platforms such as Azure Databricks/SNOWFLAKE/GCP . The ideal candidate should have a solid understanding of data warehousing principles , data modeling , and ETL/ELT pipelines , and must be comfortable working in client-facing consulting environments. Key Responsibilities: Design, build, and optimize scalable data pipelines using PySpark within Databricks/Azure Databricks Write complex SQL queries for data transformation, validation, and analytics Implement ETL/ELT workflows for both full and incremental data loads Model data for analytics using Star Schema , Snowflake Schema , Fact/Dimension tables Work with CDC (Change Data Capture) and SCD (Slowly Changing Dimensions) concepts Collaborate with cross-functional teams to understand data requirements and provide technical solutions Ensure data quality, consistency, and integrity across systems Communicate effectively with technical and non-technical stakeholders Required Skills & Experience: ‚úÖ Foundational Strong proficiency in SQL (joins, aggregations, window functions) Solid understanding of DBMS concepts ‚Äì normalization, ER models, indexes, partitioning, PK/FK Proficient in Python or PySpark programming for data processing Knowledge of data warehousing concepts (OLTP vs OLAP, Star/Snowflake schema) Experience with ETL/ELT pipelines and incremental/history loads ‚úÖ Tech-Specific Hands-on experience with one or more of the following: Databricks / Azure Databricks Snowflake , Redshift , BigQuery or similar Orchestration tools (e.g., Airflow , Azure Data Factory , dbt )",Associate,,"Python, SQL",
4240039510,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You‚Äôll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you‚Äôll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you‚Äôll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour‚Äôs. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization. Strong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources. Proficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities. Hands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements Preferred Technical And Professional Experience Understanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling. Ability to implement robust data validation, cleansing, and governance frameworks within ETL processes. Proficiency in SQL and/or Shell scripting for custom transformations and automation tasks",,,"SQL, Machine Learning",
4229448752,Data Engineer,bp,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Senior Data Engineer will be Responsible for delivering business analysis and consulting activities for the defined specialism using sophisticated technical capabilities, building and maintaining effective working relationships with a range of customers, ensuring relevant standards are defined and maintained, and implementing process and system improvements to deliver business value. Specialisms: Business Analysis; Data Management and Data Science; Digital Innovation!!! Senior Data Engineer will work as part of an Agile software delivery team; typically delivering within an Agile Scrum framework. Duties will include attending daily scrums, sprint reviews, retrospectives, backlog prioritisation and improvements! Will coach, mentor and support the data engineering squad on the full range of data engineering and solutions development activities covering requirements gathering and analysis, solutions design, coding and development, testing, implementation and operational support. Will work closely with the Product Owner to understand requirements / user stories and have the ability to plan and estimate the time taken to deliver the user stories. Proactively collaborate with the Product Owner, Data Architects, Data Scientists, Business Analysts, and Visualisation developers to meet the acceptance criteria Will be very highly skilled and experienced in use of tools and techniques such as AWS Data Lake technologies, Redshift, Glue, Spark SQL, Athena Years of Experience: 8- 12 Essential domain expertise: Experience in Big Data Technologies ‚Äì AWS, Redshift, Glue, Py-spark Experience of MPP (Massive Parallel Processing) databases helpful ‚Äì e.g. Teradata, Netezza Challenges involved in Big Data ‚Äì large table sizes (e.g. depth/width), even distribution of data Experience of programming- SQL, Python Data Modelling experience/awareness ‚Äì Third Normal Form, Dimensional Modelling Data Pipelining skills ‚Äì Data blending, etc Visualisation experience ‚Äì Tableau, PBI, etc Data Management experience ‚Äì e.g. Data Quality, Security, etc Experience of working in a cloud environment - AWS Development/Delivery methodologies ‚Äì Agile, SDLC. Experience working in a geographically disparate team",,,"Python, SQL, Tableau",
4246345682,Cloud Data Engineer,Google,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Gurugram, Haryana, India; Bengaluru, Karnataka, India; Hyderabad, Telangana, India . Minimum qualifications: Bachelor‚Äôs degree in Engineering, Computer Science, a related field, or equivalent practical experience. Experience coding with one or more programming languages (e.g., Java, C/C++, Python). Experience troubleshooting technical issues for internal/external partners or customers. Preferred qualifications: Experience in distributed data processing frameworks and modern age investigative and transactional data stores. Experience in working with/on data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools, environments, and data structures. Experience in big data, information retrieval, data mining. Experience in building multi-tier, high availability applications with modern technologies such as NoSQL, MongoDB. Experience with Infrastructure as Code (IaC) and Continuous Integration/Continuous Delivery (CICD) tools like Terraform, Ansible, Jenkins etc. Understanding of at least one database type with the ability to write complex SQLs. About the job The Google Cloud Platform team helps customers transform and build what's next for their business ‚Äî all with technology built in the cloud. Our products are developed for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. Our teams are dedicated to helping our customers ‚Äî developers, small and large businesses, educational institutions and government agencies ‚Äî see the benefits of our technology come to life. As part of an entrepreneurial team in this rapidly growing business, you will play a key role in understanding the needs of our customers and help shape the future of businesses of all sizes use technology to connect with customers, employees and partners. As a Strategic Cloud Data Engineer, you will guide customers on how to ingest, store, process, analyze, and explore/visualize data on the Google Cloud Platform. You will work on data migrations and modernization projects, and with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform/product challenges. You will have an in-depth understanding of data governance and security controls. You will travel to customer sites to deploy solutions and deliver workshops to educate and empower customers. Additionally, you will work closely with Product Management and Product Engineering teams to build and constantly drive excellence in our products.Google Cloud accelerates every organization‚Äôs ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google‚Äôs cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems. Responsibilities Interact with stakeholders to translate complex customer requirements into recommendations for appropriate solution architectures and advisory services. Engage with technical leads, and partners to lead high velocity migration and modernization to Google Cloud Platform (GCP). Design, Migrate/Build and Operationalize data storage and processing infrastructure using Cloud native products. Develop and implement data quality and governance procedures to ensure the accuracy and reliability of data. Take various project requirements and organize them into clear goals and objectives, and create a work breakdown structure to manage internal and external stakeholders. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",,,Python,
4245414960,Data Engineer-Data Platforms,IBM,"Navi Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs. Your Primary Responsibilities Include Design, build, optimize and support new and existing data models and ETL processes based on our clients business requirements. Build, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization. Coordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too Preferred Education Master's Degree Required Technical And Professional Expertise Must have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python Hbase, Hive Good to have Aws -S3, athena ,Dynomo DB, Lambda, Jenkins GIT Developed Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine). Developed Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations Preferred Technical And Professional Experience Understanding of Devops. Experience in building scalable end-to-end data ingestion and processing solutions Experience with object-oriented and/or functional programming languages, such as Python, Java and Scala",,,"Python, Data Analysis",
4244574269,Cloud Data Engineer,Uplers,"Ranchi, Jharkhand, India (Remote)",Remote,‚Çπ1.8M/yr - ‚Çπ2.8M/yr,,"About the job Experience : 5.00 + years Salary : INR 1800000-2800000 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Intelebee LLC) (*Note: This is a requirement for one of Uplers' client - Intelebee LLC) What do you need for this opportunity? Must have skills required: Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory Intelebee LLC is Looking for: Data Engineer:We are seeking a skilled and hands-on Cloud Data Engineer with 5-8 years of experience to drive end-to-end data engineering solutions. The ideal candidate will have a deep understanding of dimensional modeling, data warehousing (DW), Lakehouse architecture, and the Medallion architecture. This role will focus on leveraging Azure's/AWS ecosystem to build scalable, efficient, and secure data solutions. You will work closely with customers to understand requirements, create technical specifications, and deliver solutions that scale across both on-premise and cloud environments. Key Responsibilities: End-to-End Data Engineering Lead the design and development of data pipelines for large-scale data processing, utilizing Azure/AWS tools such as Azure Data Factory, Azure Synapse, Azure functions, Logic Apps , Azure Databricks, and Data Lake Storage. Tools, AWS Lambda, AWS Glue Develop and implement dimensional modeling techniques and data warehousing solutions for effective data analysis and reporting. Build and maintain Lakehouse and Medallion architecture solutions for streamlined, high-performance data processing. Implement and manage Data Lakes on Azure/AWS, ensuring that data storage and processing is both scalable and secure. Handle large-scale databases (both on-prem and cloud) ensuring high availability, security, and performance. Design and enforce data governance policies for data security, privacy, and compliance within the Azure ecosystem. 5 How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory",,,Data Analysis,
4251679780,Generative AI Engineer,HCLTech,"Noida, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job HCL is hiring for AIML Developer role Location: Noida (Hybrid) Must have skills: Generative AI - GPT3, ML Ops and Python Proficient in Python, with experience in machine learning, deep learning, and NLP processing. Experience in developing and implementing generative AI models, with a strong understanding of deep learning techniques such as GPT, VAE, and GANs. Proficient in Langchain, LLM Prompt Engineering: The engineer prompts and optimizes few-shot techniques to enhance LLM's performance on specific tasks, e.g. personalized recommendations. Model Evaluation & Optimization: Evaluate LLM's zero-shot and few-shot capabilities, fine-tuning hyperparameters, ensuring task generalization, and exploring model interpretability for robust web app integration. Response Quality: Collaborate with ML and Integration engineers to leverage LLM's pre-trained potential, delivering contextually appropriate responses in a user-friendly web app. It is essential to have a solid understanding of data structures, algorithms, and principles of software engineering. Experience with vector databases RDBMS, MongoDB and NoSQL databases. Proficiency in working with embeddings. Strong distributed systems skills and system architecture skills Experienced in building and running a large platform at scale. Hands-on experience with Python, Hugging Face, TensorFlow, Keras, PyTorch, Spark, or similar statistical tools. Experience as data modeling ML/NLP scientist. including, but not limited to, Performance tuning, fine-tuning, RLHF, and performance optimization. Validated background with ML toolkits, such as PyTorch, TensorFlow, Keras, Langchain, Llamadindex, SparkML, or Databricks. Proficient with the integration of data from multiple data sources Experience with NoSQL databases, such as HBase, ElasticSearch, and MongoDB API Design. API/Data mapping to schema. Experienced in and strong knowledge of using AI/ML and more particularly LLMs eager to apply this rapidly changing technology. Good Knowledge of Kubernetes, and RESTful design. Prior experience in developing public cloud services or open-source ML software is an advantage",,,"Python, Machine Learning",
4248204759,Data Visualization Engineer (Freelancer),Soul AI,India (Remote),Remote,Part-time,,"About the job Step into the world of AI innovation with the Deccan AI Experts Community (By Soul AI), where you become a creator, not just a consumer. We are reaching out to the top 1% of Soul AI‚Äôs Data Visualization Engineers like YOU for a unique job opportunity to work with the industry leaders. What‚Äôs in it for you? pay above market standards The role is going to be contract based with project timelines from 2 - 6 months, or freelancing. Be a part of an Elite Community of professionals who can solve complex AI challenges. Work location could be: Remote Onsite on client location: US, UAE, UK, India etc. Deccan AI‚Äôs Office: Hyderabad or Bangalore Responsibilities: Architect and implement enterprise-level BI solutions to support strategic decision-making along with data democratization by enabling self-service analytics for non-technical users. Lead data governance and data quality initiatives to ensure consistency and design data pipelines and automated reporting solutions using SQL and Python. Optimize big data queries and analytics workloads for cost efficiency and Implement real-time analytics dashboards and interactive reports. Mentor junior analysts and establish best practices for data visualization. Required Skills: Advanced SQL, Python (Pandas, NumPy), and BI tools (Tableau, Power BI, Looker). Expertise in AWS (Athena, Redshift), GCP (Big Query), or Snowflake. Experience with data governance, lineage tracking, and big data tools (Spark, Kafka). Exposure to machine learning and AI-powered analytics. Nice to Have: Experience with graph analytics, geospatial data, and visualization libraries (D3.js, Plotly). Hands-on experience with BI automation and AI-driven analytics. Who can be a part of the community? We are looking for top-tier Data Visualization Engineers with expertise in analyzing and visualizing complex datasets. Proficiency in SQL, Tableau, Power BI, and Python (Pandas, NumPy, Matplotlib) is a plus. If you have experience in this field then this is your chance to collaborate with industry leaders. What are the next steps? 1. Register on our Soul AI website. 2. Our team will review your profile. 3. Clear all the screening rounds: Clear the assessments once you are shortlisted. As soon as you qualify all the screening rounds (assessments, interviews) you will be added to our Expert Community! 4. Profile matching: Be patient while we align your skills and preferences with the available project. 5 . Project Allocation: You‚Äôll be deployed on your preferred project! Skip the Noise. Focus on Opportunities Built for You!",,,"Python, SQL, Tableau, Power BI, Machine Learning",
4229006415,Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job 1: Airflow with Pyspark Emphasize expertise in designing, developing, and deploying data pipelines using Apache Airflow. The focus is on creating, managing, and monitoring workflows, ensuring data quality, and collaborating with other data teams. Desired Skills and Experience Airflow, PySpark",,,,
4246105481,"Data Engineer II, Payroll Tech",Amazon,"Hyderabad, Telangana, India",,Full-time,,"About the job Description Payroll Technology at Amazon is all about enabling our business to perform at scale as efficiently as possible with no defects. As Amazon's workforce grows, both in size and geography, Amazon's payroll operations become increasingly complex, and our customers are asked to do more with less. Process can only get them so far, and that's where we come in with technology solutions to integrate and automate systems, detect defects before payment, and provide insights. As a data engineer in payroll, you will have to onboard payroll vendors across various geographies by building versatile and scalable design solutions. Having strong written and verbal communication, and the ability to communicate with end users in non-technical terms, is vital to your long-term success. The ideal candidate will have experience working with large datasets, distributed computing technologies and service-oriented architecture. The candidate should relish working with large volumes of data, and enjoys the challenge of highly complex technical contexts. He/she should be an expert with data modeling, ETL design and business intelligence tools and has hand-on knowledge on columnar databases. He/she is a self-starter, comfortable with ambiguity, able to think big and enjoys working in a fast-paced team. Responsibilities Design, build and own all the components of a high-volume data warehouse end to end. Build efficient data models using industry best practices and metadata for ad-hoc and pre-built reporting Provide wing-to-wing data engineering support for project lifecycle execution (design, execution and risk assessment) Interface with business customers, gathering requirements and delivering complete data & reporting solutions owning the design, development, and maintenance of ongoing metrics, reports, dashboards, etc. to drive key business decisions Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources Own the functional and nonfunctional scaling of software systems in your ownership area. Implement big data solutions for distributed computing. Willing to learn and develop strong skill set in AWS technologies Key job responsibilities As a DE on our team, you will be responsible for leading the data modelling, database design, and launch of some of the core data pipelines. You will have significant influence on our overall strategy by helping define the data model, drive the database design, and spearhead the best practices to delivery high quality products. A day in the life You are expected to do data modelling, database design, build data pipelines as per Amazon standards, design reviews, and supporting data privacy and security initiatives. You will attend regular stand-up meetings and provide your updates. You will keep an eye out for opportunities to improve the product or user experience and suggest those enhancements. You will participate in requirement grooming meetings to ensure the use cases we deliver are complete and functional. You will take your turn at on-call and own production operational maintenance. You will respond to customer issues and monitor databases for healthy state and performance. About The Team Our mission is to build applications which can solve challenges Global Payroll Operations teams face on daily basis, automate the tasks they perform manually, provide them seamless experience by integrating with other dependent systems, and eventually reduce Pay Defects and improve pay accuracy Basic Qualifications 3+ years of data engineering experience 4+ years of SQL experience Experience with data modeling, warehousing and building ETL pipelines Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - ADCI HYD 13 SEZ Job ID: A3002941",,,SQL,
4247112451,Data lineage Engineer,LSEG,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Data Lineage Engineer ABOUT US: LSEG (London Stock Exchange Group) is more than a diversified global financial markets infrastructure and data business. We are dedicated, open-access partners with a dedication to excellence in delivering the services our customers expect from us. With extensive experience, deep knowledge and worldwide presence across financial markets, we enable businesses and economies around the world to fund innovation, manage risk and create jobs. It‚Äôs how we‚Äôve contributed to supporting the financial stability and growth of communities and economies globally for more than 300 years. Through a comprehensive suite of trusted financial market infrastructure services ‚Äì and our open-access model ‚Äì we provide the flexibility, stability and trust that enable our customers to pursue their ambitions with confidence and clarity. LSEG is headquartered in the United Kingdom, with significant operations in 65 countries across EMEA, North America, Latin America and Asia Pacific. We employ 25,000 people globally, more than half located in Asia Pacific. LSEG‚Äôs ticker symbol is LSEG. OUR PEOPLE: People are at the heart of what we do and drive the success of our business. Our values of Integrity, Partnership, Excellence and Change shape how we think, how we do things and how we help our people fulfil their potential. We embrace diversity and actively seek to attract individuals with unique backgrounds and perspectives. We break down barriers and encourage teamwork, enabling innovation and rapid development of solutions that make a difference. Our workplace generates an enriching and rewarding experience for our people and customers alike. Our vision is to build an inclusive culture in which everyone feels encouraged to fulfil their potential. We know that real personal growth cannot be achieved by simply climbing a career ladder ‚Äì which is why we encourage and enable a wealth of avenues and interesting opportunities for everyone to broaden and deepen their skills and expertise. As a global organisation spanning 65 countries and one rooted in a culture of growth, opportunity, diversity and innovation, LSEG is a place where everyone can grow, develop and fulfil your potential with meaningful careers. ROLE PROFILE: This role is critical for the effective execution and delivery of the data governance programme named Data Trust within LSEG. The Data Lineage Engineer , as a member of the Data Governance Execution Team, is responsible for building and maintaining end-to-end data element level data lineage models that are central to the team‚Äôs function. This is to ensure that execution of the data governance conformance activities can be automated and supported in a sustainable way through connecting to, and scanning, the physical data pipelines and relevant reference models. The role will need to work closely with Key internal stakeholders and with architects of Data Bases of Records and tooling development teams to be able to successfully architect governance process and automate for sustainability. ROLE SUMMARY: As a Data Lineage Engineer within the Data Governance Execution team, you will collaborate closely with the wider Data Trust team and key divisional stakeholders to develop the data pipeline models and workflows that will improve overall data governance process and deliver data trust objectives. You will be responsible for capturing requirements, delivering automation, and create a sustainable set of complex lineage models for D&A datasets. To be successful in the role you will require the resourcefulness and curiosity to work through problems and learn how to solve sometimes highly complex problems. A passion for, and deep interest in, data is also needed as well as the ability to pick up new and sometimes niche tools to complete the tasks. WHAT YOU'LL BE DOING: Develop, construct, test, and maintain metadata repositories and data lineage models. Design and implement efficient ETL processes for loading metadata into Data Governance tools. Integrate different data storage solutions. Identify, design, and implement internal process improvements. Work with various stakeholders to assist with data-related technical issues. Build and maintain reliable data pipelines to move data across systems WHAT YOU'LL BRING: 1-2 years of relevant technical experience A keen interest into data and the way it is processed to create value to a business A base understanding of Java and confidence coding in Python Strong understanding of different data formats like XML and JSON Knowledge of relational SQL and NoSQL databases(Oracle, MSSQL, Snowflake) An awareness and base understanding of cloud services (AWS, Azure) Familiarity and some basic experience with data visualization tools (Tableau, Power BI, etc.) Strong analytical and problem-solving skills Excellent communication and teamwork abilities Education: Bachelor‚Äôs degree in Computer Science, Engineering, or a related field Preferred Qualifications: Exposure to NLP and LLM technologies and approaches Experience with machine learning and data mining techniques Familiarity with data security and privacy concerns Knowledge of data warehousing and business intelligence concepts Degree in Computer Science, Engineering, or a related field LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth. Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership , Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions. Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity. LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives. We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone‚Äôs race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants' and employees' religious practices and beliefs, as well as mental health or physical disability needs. Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it‚Äôs used for, and how it‚Äôs obtained, your rights and how to contact us as a data subject . If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.",,,"Python, SQL, Tableau, Power BI, Machine Learning",
4242046461,Data Engineer-Azure Databricks,Tata Consultancy Services,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Good Analytical and problem-solving skills Need to understand end-to-end IT lifecycle (analysis, design/architecture, coding, testing and implementation). Responsible for the delivery of a business end-to-end need starting from understanding the requirements to deploying the software into production. Experience developing in an Agile development environment, and comfortable with Agile terminology and ceremonies Strong development knowledge in DATABRICKS. Strong knowledge in SQL. Familiarity with code versioning using GIT, Jenkins and code migration through udeploy. Exposure to Jira or Rally. Identifying and implementing opportunities for automation and CI/CD Excellent communication skills",,,SQL,
4249718047,SQL Developer,Vista Applied Solutions Group Inc,India (Remote),Remote,Contract,,"About the job Hiring| SQL Development | Remote | Contract Hi All, Please find the below Job Description. Role: Healthcare SQL Development Expert LOA: 6 mos+ Loc: Ahmedabad, Gujarat, India (Northwest India), All remote, but would be nice if resource could come onsite if milestone meetings Client is building a Health Information Exchange (HIE) and they are looking for an Backend Application Developer with SQL development and healthcare data experience. Must have an understanding of HL7 and FHIR interoperability standards. They need someone who can code and write efficient queries using healthcare data. This person will be working close to the database/middleware/server side. Must: SQL development Healthcare data interoperability knowledge (FHIR and HL7) .NET Cloud architecture Familiar with NoSQL",,,SQL,
4237635693,Data Engineer,Redica Systems,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job About Us Redica Systems is a SaaS start-up serving more than 200 customers within the life science sector, with a specific focus on Pharmaceuticals and MedTech. Our workforce is distributed globally, with headquarters in Pleasanton, CA. Redica's data analytics platform empowers companies to improve product quality and navigate evolving regulations. Using proprietary processes, we harness one of the industry's most comprehensive datasets, sourced from hundreds of health agencies and the Freedom of Information Act. Our customers use Redica Systems to more effectively and efficiently manage their inspection preparation, monitor their supplier quality, and perform regulatory surveillance. More information is available at redica.com . The Role We‚Äôre looking for an experienced Data Engineer II to join our team as we continue to develop the first-of-its-kind quality and regulatory intelligence (QRI) platform for the life science industry. Core Responsibilities Creates new documentation for the components they work on May provide on-call support for their team Writes correct and clean code with minimal guidance; consistently follows best practices Rarely makes the same technical mistake twice, begins to focus on attaining expertise in one or more areas, from Data engineering, Data modeling, Data testing, Data pipeline orchestration, etc. Contributes to code used by others Makes steady progress on tasks and asks for help when blocked Ensures adequate testing (automated and manual) Prioritizes tasks and focuses on important details Deep expertise in the company's technology stack and programming languages relevant to their team High-level understanding of how their team's technology stack interacts with other teams' systems and tools About you Tech Savvy : Actively adopting and incorporating new tools and frameworks into projects Manages Complexity : Proficient in managing complexity, integrating various systems seamlessly Plans and Aligns : Able to create and follow a project plan and align tasks with organizational goals Collaborates : Able to work effectively with team members and contribute to shared objectives Manages Ambiguity : Able to handle moderate ambiguity, adapt to changing requirements Engaged: You share our values and possess the essential competencies needed to thrive at Redica, as outlined here: https://redica.com/about-us/careers/ Qualifications 2-4 years of developer experience with an emphasis on code/system architecture and quality output Experience designing and building data pipelines, data APIs, and ETL/ELT processes Exposure to data modelling and data warehouse concepts Hands-on experience in Python Hands-on experience with Snowflake & Airflow is a must-have Hands-on experience setting up, configuring, and maintaining SQL databases (MySQL/MariaDB, PostgreSQL) Computer Science, Computer Engineering, or a similar technical degree Bonus Points Experience with DBT is a major plus Experience with the data engineering stack within AWS is a major plus (S3, Lake Formation, Lambda, Fargate, Kinesis Data Streams/Data Firehose) Experience with both batch and event-driven data architectures Hands-on experience with NoSQL databases like DynamoDB and MongoDB Exposure to a start-up engineering environment is a plus Additional Information If you are a motivated individual with a passion for data engineering, we encourage you to apply for this exciting opportunity. We offer competitive salaries, comprehensive benefits packages, and a dynamic work environment where you can grow and develop your skills. Top Pharma Companies, Food Manufacturers, MedTech Companies, and Service firms from around the globe rely on Redica Systems to mine and process government inspection, enforcement, and registration data to quantify risk signals about their suppliers, identify market opportunities, benchmark against their peers, and prepare for the latest inspection trends. Our data and analytics have been cited by major media outlets such as MSNBC, WSJ, and the Boston Globe. We are committed to creating a diverse and inclusive workplace where everyone feels welcomed and valued. We believe that diversity of perspectives, backgrounds, and experiences is essential to our success. We are always looking for talented individuals who can bring unique skills and perspectives to our team. All your information will be kept confidential according to EEO guidelines.",,,"Python, SQL",
4248009265,Data Engineer,Tredence Inc.,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Job Title: Data Engineer Experience: 12 to 20 months Work Mode: Work from Office Locations: Bangalore, Chennai, Kolkata, Pune, Gurgaon About Tredence Tredence focuses on last-mile delivery of powerful insights into profitable actions by uniting its strengths in business analytics, data science, and software engineering. The largest companies across industries are engaging with us and deploying their prediction and optimization solutions at scale. Headquartered in the San Francisco Bay Area, we serve clients in the US, Canada, Europe, and Southeast Asia. Tredence is an equal opportunity employer. We celebrate and support diversity and are committed to creating an inclusive environment for all employees. Visit our website for more details: https://www.tredence.com Role Overview We are seeking a driven and hands-on Data Engineer with 12 to 20 months of experience to support modern data pipeline development and transformation initiatives. The role requires solid technical skills in SQL , Python , and PySpark , with exposure to cloud platforms such as Azure or GCP . As a Data Engineer at Tredence, you will work on ingesting, processing, and modeling large-scale data, implementing scalable data pipelines, and applying foundational data warehousing principles. This role also includes direct collaboration with cross-functional teams and client stakeholders. Key Responsibilities Develop robust and scalable data pipelines using PySpark in cloud platforms like Azure Databricks or GCP Dataflow . Write optimized SQL queries for data transformation, analysis, and validation. Implement and support data warehouse models and principles, including: Fact and Dimension modeling Star and Snowflake schemas Slowly Changing Dimensions (SCD) Change Data Capture (CDC) Medallion Architecture Monitor, troubleshoot, and improve pipeline performance and data quality. Work with teams across analytics, business, and IT functions to deliver data-driven solutions. Communicate technical updates and contribute to sprint-level delivery. Mandatory Skills Strong hands-on experience with SQL and Python Working knowledge of PySpark for data transformation Exposure to at least one cloud platform: Azure or GCP . Good understanding of data engineering and warehousing fundamentals Excellent debugging and problem-solving skills Strong written and verbal communication skills Preferred Skills Experience working with Databricks Community Edition or enterprise version Familiarity with data orchestration tools like Airflow or Azure Data Factory Exposure to CI/CD processes and version control (e.g., Git) Understanding of Agile/Scrum methodology and collaborative development Basic knowledge of handling structured and semi-structured data (JSON, Parquet, etc.)",,,"Python, SQL",
4223469693,GCP Data Engineer,Fractal,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job We are seeking an experienced GCP Data Engineer with 6+ years of experience to design, develop, and manage cloud-based data solutions on Google Cloud Platform (GCP). The ideal candidate will have expertise in BigQuery, Dataflow, Pub/Sub, Cloud Composer (Apache Airflow), and Terraform, along with strong experience in ETL/ELT pipelines, data modeling, and performance optimization. Experience: 6-14 years Locations: Bangalore, Mumbai, Pune, Chennai, Gurgaon, Noida Key Responsibilities: Design & Implement Data Pipelines: Develop and optimize ETL/ELT pipelines using Dataflow, BigQuery, and Cloud Composer (Airflow). Data Integration: Work with structured and unstructured data sources, integrating data from on-premise and cloud-based systems. Data Warehousing & Modeling: Design high-performance data models in BigQuery, ensuring scalability and cost efficiency. Automation & Infrastructure as Code (IaC): Implement Terraform for provisioning GCP resources and automate deployments. Streaming & Batch Processing: Work with Pub/Sub, Dataflow (Apache Beam), and Kafka for real-time and batch data processing. Required Skills & Qualifications: Education: Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Data Engineering, or a related field. 6+ years of experience in data engineering, cloud data solutions, and pipeline development. GCP Expertise: Hands-on experience with BigQuery, Dataflow, Pub/Sub, Cloud Storage, Cloud Composer (Airflow), Vertex AI, and IAM Policies. Programming: Proficiency in Python, SQL, and Apache Beam (Java or Scala is a plus).",,,"Python, SQL",
4244574260,Cloud Data Engineer,Uplers,"Patna, Bihar, India (Remote)",Remote,‚Çπ1.8M/yr - ‚Çπ2.8M/yr,,"About the job Experience : 5.00 + years Salary : INR 1800000-2800000 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Intelebee LLC) (*Note: This is a requirement for one of Uplers' client - Intelebee LLC) What do you need for this opportunity? Must have skills required: Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory Intelebee LLC is Looking for: Data Engineer:We are seeking a skilled and hands-on Cloud Data Engineer with 5-8 years of experience to drive end-to-end data engineering solutions. The ideal candidate will have a deep understanding of dimensional modeling, data warehousing (DW), Lakehouse architecture, and the Medallion architecture. This role will focus on leveraging Azure's/AWS ecosystem to build scalable, efficient, and secure data solutions. You will work closely with customers to understand requirements, create technical specifications, and deliver solutions that scale across both on-premise and cloud environments. Key Responsibilities: End-to-End Data Engineering Lead the design and development of data pipelines for large-scale data processing, utilizing Azure/AWS tools such as Azure Data Factory, Azure Synapse, Azure functions, Logic Apps , Azure Databricks, and Data Lake Storage. Tools, AWS Lambda, AWS Glue Develop and implement dimensional modeling techniques and data warehousing solutions for effective data analysis and reporting. Build and maintain Lakehouse and Medallion architecture solutions for streamlined, high-performance data processing. Implement and manage Data Lakes on Azure/AWS, ensuring that data storage and processing is both scalable and secure. Handle large-scale databases (both on-prem and cloud) ensuring high availability, security, and performance. Design and enforce data governance policies for data security, privacy, and compliance within the Azure ecosystem. 5 How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory",,,Data Analysis,
4216689948,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You‚Äôll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you‚Äôll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you‚Äôll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization. Strong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources. Proficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities. Hands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements Preferred Technical And Professional Experience Understanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling. Ability to implement robust data validation, cleansing, and governance frameworks within ETL processes. Proficiency in SQL and/or Shell scripting for custom transformations and automation tasks",,,"SQL, Machine Learning",
4246997892,MDM Associate Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About Amgen Amgen harnesses the best of biology and technology to fight the world‚Äôs toughest diseases, and make people‚Äôs lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what‚Äôs known today. About The Role Role Description: We are seeking an MDM Associate Data Engineer with 2‚Äì5 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment. To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python, PySpark, Databricks, AWS etc), along with knowledge of MDM (Master Data Management) Roles & Responsibilities: Analyze and manage customer master data using Reltio or Informatica MDM solutions. Perform advanced SQL queries and data analysis to validate and ensure master data integrity. Leverage Python, PySpark, and Databricks for scalable data processing and automation. Collaborate with business and data engineering teams for continuous improvement in MDM solutions. Implement data stewardship processes and workflows, including approval and DCR mechanisms. Utilize AWS cloud services for data storage and compute processes related to MDM. Contribute to metadata and data modeling activities. Track and manage data issues using tools such as JIRA and document processes in Confluence. Apply Life Sciences/Pharma industry context to ensure data standards and compliance. Basic Qualifications and Experience: Master‚Äôs degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR Bachelor‚Äôs degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR Diploma with 6 - 8 years of experience in Business, Engineering, IT or related field Functional Skills: Must-Have Skills: Advanced SQL expertise and data wrangling. Strong experience in Python and PySpark for data transformation workflows. Strong experience with Databricks and AWS architecture. Must have knowledge of MDM, data governance, stewardship, and profiling practices. In addition to above, candidates having experience with Informatica or Reltio MDM platforms will be preferred. Good-to-Have Skills: Experience with IDQ, data modeling and approval workflow/DCR. Background in Life Sciences/Pharma industries. Familiarity with project tools like JIRA and Confluence. Strong grip on data engineering concepts. Professional Certifications: Any ETL certification (e.g. Informatica) Any Data Analysis certification (SQL, Python, Databricks) Any cloud certification (AWS or AZURE) Soft Skills: Strong analytical abilities to assess and improve master data processes and solutions. Excellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders. Effective problem-solving skills to address data-related issues and implement scalable solutions. Ability to work effectively with global, virtual teams EQUAL OPPORTUNITY STATEMENT Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Associate,,"Python, SQL, Data Analysis",
4245293505,GCP Data Engineer,TELUS Digital,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job About Us : At TELUS Digital, we enable customer experience innovation through spirited teamwork, agile thinking, and a caring culture that puts customers first. TELUS Digital is the global arm of TELUS Corporation, one of the largest telecommunications service providers in Canada. We deliver contact center and business process outsourcing (BPO) solutions to some of the world's largest corporations in the consumer electronics, finance, telecommunications and utilities sectors. With global call center delivery capabilities, our multi-shore, multi-language programs offer safe, secure infrastructure, value-based pricing, skills-based resources and exceptional customer service - all backed by TELUS, our multi-billion dollar telecommunications parent. Required Skills: Design, develop, and support data pipelines and related data products and platforms. Design and build data extraction, loading, and transformation pipelines and data products across on-prem and cloud platforms. Perform application impact assessments, requirements reviews, and develop work estimates. Develop test strategies and site reliability engineering measures for data products and solutions. Participate in agile development ""scrums"" and solution reviews. Mentor junior Data Engineers. Lead the resolution of critical operations issues, including post-implementation reviews. Perform technical data stewardship tasks, including metadata management, security, and privacy by design. Design and build data extraction, loading, and transformation pipelines using Python and other GCP Data Technologies Demonstrate SQL and database proficiency in various data engineering tasks. Automate data workflows by setting up DAGs in tools like Control-M, Apache Airflow, and Prefect. Develop Unix scripts to support various data operations. Model data to support business intelligence and analytics initiatives. Utilize infrastructure-as-code tools such as Terraform, Puppet, and Ansible for deployment automation. Expertise in GCP data warehousing technologies, including BigQuery, Cloud SQL, Dataflow, Data Catalog, Cloud Composer, Google Cloud Storage, IAM, Compute Engine, Cloud Data Fusion and Dataproc (good to have). Qualifications: Bachelor's degree in Software Engineering, Computer Science, Business, Mathematics, or related field. 4+ years of data engineering experience. 2 years of data solution architecture and design experience. GCP Certified Data Engineer (preferred).",,,"Python, SQL",
4236678554,Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job ob Title: Data Engineer Experience: 6-8 years Technology Stack: PySpark, AWS EMR, Apache Airflow Job Overview We are seeking an experienced Data Engineer to join our team in an offshore capacity. The ideal candidate will have 5-8 years of hands-on experience in building, deploying, and maintaining scalable data pipelines and processing frameworks using PySpark, AWS EMR, and Apache Airflow. This role will require collaborating with cross-functional teams, understanding business needs, and designing robust solutions for large-scale data processing. Key Responsibilities Design, develop, and maintain efficient, scalable data pipelines for batch and real-time processing. Use PySpark to process large datasets and perform transformations, ensuring high performance and optimized workflows. Build and manage data workflows with Apache Airflow, ensuring smooth scheduling and execution of ETL pipelines. Implement AWS EMR clusters for big data processing, ensuring efficient scaling, cost optimization, and high availability. Develop automated solutions for data extraction, transformation, and loading (ETL) across various sources and sinks. Collaborate with data architects, analysts, and other stakeholders to gather requirements and ensure smooth integration of data solutions. Monitor and troubleshoot data pipelines, ensuring the system runs efficiently and without disruptions. Optimize complex queries, algorithms, and processing logic to meet performance and scalability requirements. Perform data validation and quality checks to ensure the accuracy and consistency of the data. Stay updated with the latest advancements in big data technologies and cloud infrastructure to suggest improvements in processes. Required Skills 6-8 years of experience in data engineering, with strong expertise in data pipeline design and big data processing. Proficiency in PySpark for distributed data processing. Experience working with AWS EMR for big data processing and managing clusters. Hands-on experience with Apache Airflow for orchestration and scheduling of data workflows. Solid understanding of data warehousing concepts, ETL processes, and data integration. Strong experience with SQL for querying and optimizing large datasets. Familiarity with other AWS services like Lambda, RDS, and Glue is a plus. Strong troubleshooting, debugging, and problem-solving skills. Ability to work independently in an offshore setup, collaborating effectively with remote teams. Preferred Skills Experience in Data Lakes, Redshift, or other cloud-based data storage and processing systems. Understanding of data security and privacy best practices for handling sensitive data. Familiar with machine learning concepts and data science workflows. Education cheloror Master degree in Computer Science, Engineering, or a related field. Additional Information This position is offshore (India) and will require remote collaboration with teams based in other regions. Opportunity to work on challenging data engineering projects with a global team. Desired Skills and Experience Python",,,"Python, SQL, Machine Learning",
4095160746,Assistant Product Data Engineer,KSB Company,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Flair for data preparation and product data upkeep. Good PC skills with expertise in MS- Office. Knowledge of valve construction and mfg .processes followed. Hands on experience with any CAD or Configuration software is plus Exposure to working in SAP / ECTR Environment. Good communication skills with ability to communicate in English. Good team player having willingness to learn new things, apply them and share. To assist in preparation of process documents and their regular updates. To carry out material number classification related tasks using Classmate Finder f Good analytical ,communication and time management skills ; ability to speak in English fluently Customer centric approach. Good team player with interactive approach and flair to explore, apply , suggest new possibilities and share them. To guide, train and assist other Support engineers in their in-hand tasks of Product data preparation; Interact with internal customers with respect to entrusted projects and tasks. Assist in work planning and assigning tasks to Support Engineers in co-ordination with senior. To support in knowledgebase / skill updates , applying them further to related tasks and maintaining process documentation.",,,,
4239123182,AWS Data Engineer - ETL/SQL/Python,Deqode,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job This job is sourced from a job board. Learn More Job Responsibilities Design, develop, and maintain scalable data pipelines using AWS Glue and other ETL tools. Optimize and manage data models in Amazon Redshift and RDS for performance and cost-efficiency. Write complex SQL queries for data extraction, transformation, and reporting needs. Work extensively with Athena and Python for data analysis and transformation. Leverage AWS serverless services (Lambda, DMS, Glue, etc.) to build end-to-end data workflows. Collaborate with stakeholders to understand data requirements and deliver actionable insights. Manage and provision infrastructure using AWS CloudFormation. Ensure best practices in security, monitoring, and data governance. Required Skills Programming : Python, PySpark AWS Services AWS Glue Amazon Redshift AWS Lambda AWS DMS (Database Migration Service) Amazon RDS AWS Athena AWS CloudFormation Database & Querying : SQL, Data Modeling ETL Development : Designing and implementing ETL pipelines Preferred Qualifications Experience with data lakes and data warehouse architecture. Familiarity with CI/CD processes for data infrastructure. Strong analytical and problem-solving skills. Ability to work independently in a fast-paced environment (ref:hirist.tech)",,,"Python, SQL, Data Analysis",
4229165062,Financial Markets- Data Engineer,PwC Acceleration Centers in India,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job At PwC, our people in finance consulting specialise in providing consulting services related to financial management and strategy. These individuals analyse client needs, develop financial solutions, and offer guidance and support to help clients optimise their financial performance, improve decision-making, and achieve their financial goals. As a finance consulting generalist at PwC, you will possess a broad understanding of various aspects of finance consulting. Your work will involve providing comprehensive guidance and support to clients in optimising their financial performance, improving decision-making, and achieving their financial goals. You will be responsible for analysing client needs, developing financial solutions, and offering recommendations tailored to specific business requirements. Driven by curiosity, you are a reliable, contributing member of a team. In our fast-paced environment, you are expected to adapt to working with a variety of clients and team members, each presenting varying challenges and scope. Every experience is an opportunity to learn and grow. You are expected to take ownership and consistently deliver quality work that drives value for our clients and success as a team. As you navigate through the Firm, you build a brand for yourself, opening doors to more opportunities. Skills Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to: Apply a learning mindset and take ownership for your own development. Appreciate diverse perspectives, needs, and feelings of others. Adopt habits to sustain high performance and develop your potential. Actively listen, ask questions to check understanding, and clearly express ideas. Seek, reflect, act on, and give feedback. Gather information from a range of sources to analyse facts and discern patterns. Commit to understanding how the business works and building commercial awareness. Learn and apply professional and technical standards (e.g. refer to specific PwC tax and audit guidance), uphold the Firm's code of conduct and independence requirements. Role Overview We are seeking a highly motivated Data Engineer - Associate to join our dynamic team. The ideal candidate will have a strong foundation in data engineering, particularly with Python and SQL, and will have exposure to cloud technologies and data visualization tools such as Power BI, Tableau, or QuickSight. The Data Engineer will work closely with data architects and business stakeholders to support the design and implementation of data pipelines and analytics solutions. This role o∆Øers an opportunity to grow technical expertise in cloud and data solutions, contributing to projects that drive business insights and innovation. Key Responsibilities Data Engineering: ÔÇ∑ Develop, optimize, and maintain data pipelines and workflows to ensure e∆Øicient data integration from multiple sources. ÔÇ∑ Use Python and SQL to design and implement scalable data processing solutions. ÔÇ∑ Ensure data quality and consistency throughout data transformation and storage processes. ÔÇ∑ Collaborate with data architects and senior engineers to build data solutions that meet business and technical requirements. Cloud Technologies ÔÇ∑ Work with cloud platforms (e.g., AWS, Azure, or Google Cloud) to deploy and maintain data solutions. ÔÇ∑ Support the migration of on-premise data infrastructure to the cloud environment when needed. ÔÇ∑ Assist in implementing cloud-based data storage solutions, such as data lakes and data warehouses. Data Visualization ÔÇ∑ Provide data to business stakeholders for visualizations using tools such as Power BI, Tableau, or QuickSight. ÔÇ∑ Collaborate with analysts to understand their data needs and optimize data structures for reporting. Collaboration And Support ÔÇ∑ Work closely with cross-functional teams, including data scientists and business analysts, to support data-driven decision-making. ÔÇ∑ Troubleshoot and resolve issues in the data pipeline and ensure timely data delivery. ÔÇ∑ Document processes, data flows, and infrastructure for team knowledge sharing. Required Skills And Experience ÔÇ∑ 0+ years of experience in data engineering, working with Python and SQL. ÔÇ∑ Exposure to cloud platforms such as AWS, Azure, or Google Cloud is preferred. ÔÇ∑ Familiarity with data visualization tools (e.g., Power BI, Tableau, QuickSight) is a plus. ÔÇ∑ Basic understanding of data modeling, ETL processes, and data warehousing concepts. ÔÇ∑ Strong analytical and problem-solving skills, with attention to detail. Qualifications ÔÇ∑ Bachelor‚Äôs degree in Computer Science, Data Science, Information Technology, or related fields. ÔÇ∑ Basic knowledge of cloud platforms and services is advantageous. ÔÇ∑ Strong communication skills and the ability to work in a team-oriented environment.",Associate,,"Python, SQL, Tableau, Power BI",
4230294362,AWS Data Engineer,Tata Consultancy Services,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Dear Associate Greetings from TATA Consultancy Services!! Thank you for expressing your interest in exploring a career possibility with the TCS Family. We have a job opportunity for AWS Data Engineer at Tata Consultancy Services at 24th May 2025. Hiring For : AWS Data Engineer Mandatory Skills: AWS Glue, Redshift, spotfire, Lambda, Python , Pyspark, DevOps practices and tools for CI/CD Walk in Location: Bangalore Experience : 5-15 years Mode of interview: in-person walk in drive Date of interview: 24 May 2025 Venue : Crescent Prestige Shantiniketan, Sadaramanagala South Taluk, Bengaluru, 3,, Thigalarapalya,, 3, ITPL Main Road, Maruthi Nagar, Krishnarajapuram, Bengaluru, Karnataka 560048. If you are interested in this exciting opportunity, Please share your updated resume on jeena.james1@tcs.com along with the additional information mentioned below: Name: Preferred Location: Contact No: Email id: Highest Qualification: Current Organization Total Experience: Relevant Experience in AWS glue: Relevant Experience in AWS redshift: Current CTC: Expected CTC: Notice Period: Gap Duration: Gap Details: Attended interview with TCS in past(details): Please share your I begin portal EP id if already registered: Willing to attend walk in on 24th May: (Yes/No) Note: only Eligible candidates with Relevant experience will be contacted further Thanks & Regards, Jeena James, Website: http://www.tcs.com Email: jeena.james1@tcs.com Human Resource - Talent Acquisition Group, Tata Consultancy Services",Associate,,Python,
3939981290,Data Engineer- SQL+PySpark,Fragma Data Systems,"Bengaluru, Karnataka, India (On-site)",On-site,‚Çπ500K/yr - ‚Çπ1.5M/yr,,"About the job This job is sourced from a job board. Learn More Must-Have Skills Good experience in Pyspark - Including Dataframe core functions and Spark SQL Good experience in SQL DBs - Be able to write queries including fair complexity. Should have excellent experience in Big Data programming for data transformation and aggregations Good at ELT architecture. Business rules processing and data extraction from Data Lake into data streams for business consumption. Good customer communication. Good Analytical skill Technology Skills (Good To Have) Building and operationalizing large scale enterprise data solutions and applications using one or more of AZURE data and analytics services in combination with custom solutions - Azure Synapse/Azure SQL DWH, Azure Data Lake, Azure Blob Storage, Spark, HDInsights, Databricks, CosmosDB, EventHub/IOTHub. Experience in migrating on-premise data warehouses to data platforms on AZURE cloud. Designing and implementing data engineering, ingestion, and transformation functions Azure Synapse or Azure SQL data warehouse Spark on Azure is available in HD insights and data bricks Skills:- Python, SQL, Spark, Windows Azure and PySpark",,,"Python, SQL",
4249148113,Data Migration Engineer /Lead,vueverse.,India (Remote),Remote,Full-time,"expertise: ‚Ä¢  Energy Sector: Renewable energy, Nuclear energy and Fossil fuel sectors. ‚Ä¢  Pharma, MedTech, and Life Sciences: Pharmaceutical, Clinical, and Medical device industries ‚Ä¢  Emerging Technologies: AI, Digital transformation and Data science. ‚Ä¶ show more Show more","About the job Data Migration Engineer /Lead Position Overview: The Data Migration Lead Engineer will lead and execute data migration to Veeva Vault, ensuring data integrity and quality while developing strategies and mapping data from legacy systems. Key Responsibilities: Develop and lead data migration strategies for Veeva Vault from QMS (Caliber, Trackwise, homegrown), DMS (Biovia, Trackwise, Omnidocs), and LMS systems. Oversee end-to-end data migration processes, ensuring quality and integrity pre- and post-migration. Map master data elements from legacy to target systems with precision. Manage and mentor migration teams to achieve seamless execution. Ensure compliance with data migration standards and best practices. Qualifications: Proven experience migrating data to Veeva Vault from diverse QMS, DMS, and LMS systems. Expertise in data migration strategy development and execution. Strong skills in mapping master data elements across systems. Leadership and team management experience. Deep understanding of data integrity and quality assurance processes.",Executive,12 years experience,,
4167820367,"Data Engineer I, ITA - Workforce Intelligence, Core Data Acquisition",Amazon,"Bengaluru, Karnataka, India",,Full-time,,"About the job Description Do you want a role with deep meaning and the ability to make a massive impact? Hiring top talent is not only critical to Amazon‚Äôs success‚Äîit can literally change the world. It took a lot of great hires to deliver innovations like AWS, Prime, and Alexa, which make life better for millions of customers around the world every day. As part of the Intelligent Talent Acquisition (ITA) team, you'll have the opportunity to reinvent the hiring process and deliver unprecedented scale, sophistication, and accuracy that Amazon's Talent Acquisition operations need. ITA is an industry-leading people science and technology organization made up of scientists, engineers, analysts, product professionals and more. Our shared goal is to fairly and precisely connect the right people to the right jobs. Last year, we delivered over 6 million online candidate assessments, replacing the ""game of chance"" with a merit-based approach that gives candidates the chance to showcase their true skills. Each year we help Amazon deliver billions of packages around the world by making it possible to hire hundreds of thousands of associates in the right quantity, at the right location and at exactly the right time. You‚Äôll work on state-of-the-art research, advanced software tools, new AI systems, and machine learning algorithms to solve complex hiring challenges. Leveraging Amazon's in-house tech stack built on AWS, you'll have the autonomy and flexibility to bring innovative solutions to life. One day, we can bring these solutions to the rest of the world. Join ITA in using technologies to transform the hiring landscape and make a meaningful difference in people's lives. Together, we can solve the world's toughest hiring problems. Within ITA, the Data team delivers high-quality recruiting data, reusable tools, and analytics reporting to Amazon Talent Acquisition customers. The Data team is the engine behind ITA‚Äôs success, creating pathways to information upon which all of the organization‚Äôs products are built and enabling Amazon to make correct hiring decisions a million times over. Focused on building scalable long term solutions, the team empowers engineers and consumers to develop, access, and analyze data independently with a high degree of data privacy and integrity. As a member of the Data team, you will deliver robust data solutions that drive fair and efficient hiring and rapidly evolve with the needs of the business. Key job responsibilities Develop and maintain end to end scalable data infrastructure and data pipelines Work closely with business owners, developers, Business Intelligence Engineers to explore new data sources and deliver the data. Empower the team for self-servicing by providing the necessary skills to handle complex tasks independently. Ensure that the team can efficiently manage and troubleshoot data-related issues, leading to increased autonomy and productivity. along with fostering a culture of self-reliance. A day in the life As a Data Engineer with Workforce Intelligence, you will partner with Software Engineers, Data Scientists and Business Intelligence Engineers. You will gain a deep understanding of our services and the data they produce, and become our resident expert in how to transform that data into a format that is useful for analytics and business intelligence. You will proactively help to identify new data for integration with our platform, and propose and implement new technologies to help us better understand our data. In this role, you will serve as the expert in designing, implementing, and operating a stable, scalable, low cost environment to flow information from the source systems to data warehouse into end-user facing reporting applications such as Tableau or AWS QuickSight. Above all, you will bring large datasets together to answer business questions and drive data-driven decision making. About The Team ITA-Data team delivers high-quality recruiting data, reusable tools, and analytics reporting to Amazon Talent Acquisition customers. The Data team is the engine behind ITA‚Äôs success, creating pathways to information upon which all of the organization‚Äôs products are built and enabling Amazon to make correct hiring decisions a million times over. Focused on building scalable long term solutions, the team empowers engineers and consumers to develop, access, and analyze data independently with a high degree of data privacy and integrity. As a member of the Data team, you will deliver robust data solutions that drive fair and efficient hiring and rapidly evolve with the needs of the business. Basic Qualifications 1+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Preferred Qualifications Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc. Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - ADCI - Karnataka Job ID: A2910690",associate,,"Python, SQL, Tableau, Machine Learning",
4235452058,Sr GCP Data Engineer,LTIMindtree,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Experience: 5-12 years Primary Skills: GCP, Big query, Python Notice period: Immediate to 30 days Overall 5+ years of experience in data eng Strong Experience with GCP services like Big Query Cloud Storage composer etc Hands on experience in Python Proficient in Pyspark databricks Strong understanding of data warehouse concepts ETL process and data modelling GCP certifications are added advantage",,,Python,
4226809161,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You‚Äôll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you‚Äôll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you‚Äôll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization. Strong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources. Proficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities. Hands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements Preferred Technical And Professional Experience Understanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling. Ability to implement robust data validation, cleansing, and governance frameworks within ETL processes. Proficiency in SQL and/or Shell scripting for custom transformations and automation tasks",,,"SQL, Machine Learning",
4244570677,Cloud Data Engineer,Uplers,"Ghaziabad, Uttar Pradesh, India (Remote)",Remote,‚Çπ1.8M/yr - ‚Çπ2.8M/yr,,"About the job Experience : 5.00 + years Salary : INR 1800000-2800000 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Intelebee LLC) (*Note: This is a requirement for one of Uplers' client - Intelebee LLC) What do you need for this opportunity? Must have skills required: Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory Intelebee LLC is Looking for: Data Engineer:We are seeking a skilled and hands-on Cloud Data Engineer with 5-8 years of experience to drive end-to-end data engineering solutions. The ideal candidate will have a deep understanding of dimensional modeling, data warehousing (DW), Lakehouse architecture, and the Medallion architecture. This role will focus on leveraging Azure's/AWS ecosystem to build scalable, efficient, and secure data solutions. You will work closely with customers to understand requirements, create technical specifications, and deliver solutions that scale across both on-premise and cloud environments. Key Responsibilities: End-to-End Data Engineering Lead the design and development of data pipelines for large-scale data processing, utilizing Azure/AWS tools such as Azure Data Factory, Azure Synapse, Azure functions, Logic Apps , Azure Databricks, and Data Lake Storage. Tools, AWS Lambda, AWS Glue Develop and implement dimensional modeling techniques and data warehousing solutions for effective data analysis and reporting. Build and maintain Lakehouse and Medallion architecture solutions for streamlined, high-performance data processing. Implement and manage Data Lakes on Azure/AWS, ensuring that data storage and processing is both scalable and secure. Handle large-scale databases (both on-prem and cloud) ensuring high availability, security, and performance. Design and enforce data governance policies for data security, privacy, and compliance within the Azure ecosystem. 5 How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory",,,Data Analysis,
4241897720,GCP Data Engineer,Vista Applied Solutions Group Inc,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Role: GCP Data Engineer Fulltime with Perficient India Pvt Ltd Location: Bengaluru or Chennai (Hybrid) Notice Period : Immediate - 2 weeks Job Description: * Experience as a Data Engineer with expertise in GCP technologies such as BigQuery, Data Flow, etc. * Strong understanding of SQL concepts including joins, aggregations, filtering, etc. * Proficiency in writing efficient Python code for data processing tasks.",Manager,,"Python, SQL",
4241539418,"Data Engineer II, Data & AI, Customer Engagement Technology",Amazon,"Hyderabad, Telangana, India",,Full-time,,"About the job Description As a Data Engineer on the Data and AI team, you will design and implement robust data pipelines and infrastructure that power our organization's data-driven decisions and AI capabilities. This role is critical in developing and maintaining our enterprise-scale data processing systems that handle high-volume transactions while ensuring data security, privacy compliance, and optimal performance. You'll be part of a dynamic team that designs and implements comprehensive data solutions, from real-time processing architectures to secure storage solutions and privacy-compliant data access layers. The role involves close collaboration with cross-functional teams, including software development engineers, product managers, and scientists, to create data products that power critical business capabilities. You'll have the opportunity to work with leading technologies in cloud computing, big data processing, and machine learning infrastructure, while contributing to the development of robust data governance frameworks. If you're passionate about solving complex technical challenges in high-scale environments, thrive in a collaborative team setting, and want to make a lasting impact on our organization's data infrastructure, this role offers an exciting opportunity to shape the future of our data and AI capabilities. Key job responsibilities Design and implement ETL/ELT frameworks that handle large-scale data operations, while building reusable components for data ingestion, transformation, and orchestration while ensuring data quality and reliability. Establish and maintain robust data governance standards by implementing comprehensive security controls, access management frameworks, and privacy-compliant architectures that safeguard sensitive information. Drive the implementation of data solutions, both real-time and batch, optimizing them for both analytical workloads and AI/ML applications. Lead technical design reviews and provide mentorship on data engineering best practices, identifying opportunities for architectural improvements and guiding the implementation of enhanced solutions. Build data quality frameworks with robust monitoring systems and validation processes to ensure data accuracy and reliability throughout the data lifecycle. Drive continuous improvement initiatives by evaluating and implementing new technologies and methodologies that enhance data infrastructure capabilities and operational efficiency. A day in the life The day often begins with a team stand-up to align priorities, followed by a review of data pipeline monitoring alarms to address any processing issues and ensure data quality standards are maintained across systems. Throughout the day, you'll find yourself immersed in various technical tasks, including developing and optimizing ETL/ELT processes, implementing data governance controls, and reviewing code for data processing systems. You'll work closely with software engineers, scientists, and product managers, participating in technical design discussions and sharing your expertise in data architecture and engineering best practices. Your responsibilities extend to communicating with non-technical stakeholders, explaining data-related projects and their business impact. You'll also mentor junior engineers and contribute to maintaining comprehensive technical documentation. You'll troubleshoot issues that arise in the data infrastructure, optimize the performance of data pipelines, and ensure data security and compliance with relevant regulations. Staying updated on the latest data engineering technologies and best practices is crucial, as you'll be expected to incorporate new learnings into your work. By the end of a typical day, you'll have advanced key data infrastructure initiatives, solved complex technical challenges, and improved the reliability, efficiency, and security of data systems. Whether it's implementing new data governance controls, optimizing data processing workflows, or enhancing data platforms to support new AI models, your work directly impacts the organization's ability to leverage data for critical business decisions and AI capabilities. If you are not sure that every qualification on the list above describes you exactly, we'd still love to hear from you! At Amazon, we value people with unique backgrounds, experiences, and skillsets. If you‚Äôre passionate about this role and want to make an impact on a global scale, please apply! About The Team The Data and Artificial Intelligence (AI) team is a new function within Customer Engagement Technology. We own the end-to-end process of defining, building, implementing, and monitoring a comprehensive data strategy. We also develop and apply Generative Artificial Intelligence (GenAI), Machine Learning (ML), Ontology, and Natural Language Processing (NLP) to customer and associate experiences. Basic Qualifications 3+ years of data engineering experience Bachelor‚Äôs degree in Computer Science, Engineering, or a related technical discipline Preferred Qualifications Experience with AWS data services (Redshift, S3, Glue, EMR, Kinesis, Lambda, RDS) and understanding of IAM security frameworks Proficiency in designing and implementing logical data models that drive physical designs Hands-on experience working with large language models, including understanding of data infrastructure requirements for AI model training Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - Amazon Dev Center India - Hyderabad Job ID: A2996966",manager,,Machine Learning,
4090279314,"Data Engineer, MIDAS Digital Intelligence",Amazon Music,"Chennai, Tamil Nadu, India",,Full-time,,"About the job Description Are you excited about the digital media revolution and passionate about designing and delivering advanced analytics that directly influence the product decisions of Amazon's digital businesses. Do you see yourself as a champion of innovating on behalf of the customer by turning data insights into action? The Amazon Digital Acceleration (DA) Analytics team is looking for an analytical and technically skilled individual to join our team. In this role, you will play a critical part in developing foundational data instrumentation components to seamlessly surface relevant digital content to Amazon customers. An ideal individual is someone who has deep data engineering skills around ETL, data modeling, database architecture and big data solutions. You should have strong business judgement, excellent written and verbal communication skills. Basic Qualifications 3+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - ADCI MAA 15 SEZ Job ID: A2844445",,,SQL,
3950872092,"Associate, Data Engineer",Bain & Company,"Gurugram, Haryana, India (Hybrid)",Hybrid,Full-time,,"About the job Who We Are Bain & Company is a global management consulting that helps the world‚Äôs most ambitious change makers define the future. Across 65 offices in 40 countries, we work alongside our clients as one team with a shared ambition to achieve extraordinary results, outperform the competition and redefine industries. Since our founding in 1973, we have measured our success by the success of our clients, and we proudly maintain the highest level of client advocacy in the industry. Who You‚Äôll work with BCN Labs is a Center of Excellence (CoE) functioning akin to a small R&D boutique startup within the Bain ecosystem, delivering end-to-end data driven client deployable solutions across a wide variety of sectors and industries. We work directly with other CoEs and Practices within Bain as part of the Expert Client Delivery system and interface with teams across the globe. We are first and foremost business thought partners working on intelligent ways of using analytical techniques and algorithms across the spectrum of disciplines that can enable building world-class solutions. Our goal is to build a disruptive high-impact business-enabled end-to-end analytical solutions delivery system across all verticals of Bain. What You Will Do We are seeking a strong candidate with experience in building applications around advanced analytics models, including front-end and back-end, and hosting said applications to fill an exciting Associate (AS) role within BCN Labs. The AS is expected to have a strong expertise in designing and building business-intuitive front-end interfaces and in setting up a robust back-end for our Machine Learning models and analytical solutions. The team has quite a few data scientists and the AS would have to interface with them on a day-to-day basis to enable the analytical solutions to be encapsulated within innovative, intuitive and seamless apps for end-clients and non-technical business leaders. An AS is expected to have the ability to: Collaborate with data scientists (who typically work with Python) to design and help build automated, deployable solutions using data orchestration tools (like AirFlow, etc.) Develop and maintain front-end interfaces for data science applications, enhancing user experience and interaction with complex data sets. Assist in the design and implementation of database solutions and data pipelines, supporting analytics and machine learning projects. Lead the integration of Python-based models with databases via RESTful APIs, facilitating real-time data processing and analytics. Employ Docker for application containerization and Git for version control, ensuring code quality and efficient team collaboration. Successfully deploy multiple applications on AWS, optimizing cloud resources for cost-efficiency and scalability. Contribute to the development of internal tools and platforms using technologies like Django and/or Streamlit to improve productivity and project delivery times. Engage in continuous learning to stay ahead of emerging technologies and methodologies in data engineering and software development. The AS is expected to have a knack for seeking out challenging problems and coming up with their own ideas which they will be encouraged to brainstorm with their peers and managers. They should be willing to learn new techniques and be open to solving problems with an interdisciplinary approach. They must have excellent coding skills and should demonstrate a willingness to be able to write modular, functional codes. About You A Masters or any other advanced degrees in a field linked to analytics such as Computer Science, Information Technology (IT), Operations Research, Engineering, etc. If you are a Bachelors in any of the fields mentioned above, then we would seek a stellar academic record with proof of ability in not just implementing applications but also working through the end-to-end delivery cycle of analytical solutions including hosting Excellent skills in HTML, CSS, JavaScript, SQL, Ajax, working with APIs ‚Äì RESTful or otherwise, a coding language such as C or JAVA and an intermediate exposure to Python. Having skills and exposure to Django and/or Streamlit would be given preference, although it is not a mandatory requirement. (Good-to-have, but not necessary) Exposure to either AWS or Azure with some experience of deploying Python based solutions on these cloud platforms, Airflow, Snowflake, PySpark, Git, Docker, etc. You will fit into our team-oriented structure with a college/hostel-style way of working, having the comfort of reaching out to anyone for support that can enable our clients better At least 1 or 2 years of demonstrated abilities in data engineering and software development if you are a Masters degree holder; and, at least 3 years of a stellar career track record if you are a Bachelors degree holder Demonstrated abilities to manage and deliver data-driven projects What makes us a great place to work We are proud to be consistently recognized as one of the world's best places to work, a champion of diversity and a model of social responsibility. We are currently ranked the #1 consulting firm on Glassdoor‚Äôs Best Places to Work list, and we have maintained a spot in the top four on Glassdoor's list for the last 12 years. We believe that diversity, inclusion and collaboration is key to building extraordinary teams. We hire people with exceptional talents, abilities and potential, then create an environment where you can become the best version of yourself and thrive both professionally and personally. We are publicly recognized by external parties such as Fortune, Vault, Mogul, Working Mother, Glassdoor and the Human Rights Campaign for being a great place to work for diversity and inclusion, women, LGBTQ and parents..",Associate,,"Python, SQL, R, Machine Learning",
4240895697,Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job To be responsible for data modelling, design, and development of the batch and real-time extraction, load, transform (ELT) processes, and the setup of the data integration framework, ensuring best practices are followed during the integration development. Bachelors degree in CS/IT or related field (minimum) Azure Data Engineer (ADF, ADSL, MS Fabric), Databricks Azure DevOps, Confluence Desired Skills and Experience Python",,,Python,
4217589656,Data Engineer - India GDC (Gurugram),ERM,"New Delhi, Delhi, India (On-site)",On-site,Full-time,,"About the job Who is ERM? ERM is a leading global sustainability consulting firm, committed for nearly 50 years to helping organizations navigate complex environmental, social, and governance (ESG) challenges. We bring together a diverse and inclusive community of experts across regions and disciplines, providing a truly multicultural environment that fosters collaboration, professional growth, and meaningful global exposure. As a people-first organization, ERM values well-being, career development, and the power of collective expertise to drive sustainable impact for our clients‚Äîand the planet. Introducing our new Global Delivery Centre (GDC) Our Global Delivery Centre (GDC) in India is a unified platform designed to deliver high-value services and solutions to ERM‚Äôs global clientele. By centralizing key business and consulting functions, we streamline operations, optimize service delivery, and enable our teams to focus on what matters most‚Äîadvising clients on sustainability challenges with agility and innovation. Through the GDC, you will collaborate with international teams, leverage emerging technologies, and further enhance ERM‚Äôs commitment to excellence‚Äîamplifying our shared mission to make a lasting, positive impact. Job Objective The objective of the Data Engineer role is to help our consultants upskill in the use of new technology and tools, especially AI tools. The Data Engineer will work with project teams to demonstrate and showcase capabilities using ERM's technology and AI tools, and impart that knowledge to the project teams so they can undertake these tasks themselves. Additionally, the role involves creating tools and resources for project teams to independently achieve their goals. The overall objective is to facilitate the growth of capabilities within our consulting ranks. Key Accountabilities & Responsibilities Work directly with project teams to solve their problems using available tools, ensuring that project teams can independently and autonomously solve these problems in the future. Identify generic forms of requests to build tools and applications that consultants can use off the shelf for similar purposes in the future. Understand requirements and own the execution of tasks while collaborating across the business. Demonstrate and showcase capabilities using ERM's technology and AI tools, such as Microsoft Fabric and Copilot. Create tools and resources to support project teams in achieving their goals independently. Influence And Decision Making Authority Influence: The Data Engineer will have significant influence over the adoption and implementation of new technologies and AI tools within project teams. They will guide and mentor consultants, helping them to upskill and become proficient in using these tools independently. Decision Making Authority: The Data Engineer will have the authority to make decisions regarding the design and development of tools and resources that will be used by project teams. They will also be responsible for identifying common problems and creating reusable solutions that can be applied across different projects. Additionally, they will collaborate with various departments to ensure that the tools and solutions developed align with the overall business strategy and objectives. Job Requirements & Capabilities Qualifications: Bachelors degree qualified in science, engineering or mathematics Job specific capabilities/skills: Skills in data engineering and working with databases. Experience in data science and coding in Python is favorable. Experience with Microsoft Fabric highly regarded. Experience with large language models (LLMs) or other natural language processing tools. Ability to work with non-technical specialists to upskill or train them. Strong communication skills and the ability to articulate complex scenarios effectively. Ability to work in a complex, global, dynamic organization and be effective within matrixed reporting environments and multi-partner contexts. Problem-solving skills and the ability to make decisions by assessing situations and selecting appropriate courses of action.",,,Python,
4248776008,Azure Data Engineer,Viraaj HR Solutions Private Limited,"Pune, Maharashtra, India (On-site)",On-site,‚Çπ1.2M/yr - ‚Çπ2M/yr,,"About the job Company Overview Viraaj HR Solutions is dedicated to connecting top talent with forward-thinking companies. Our mission is to provide exceptional talent acquisition services while fostering a culture of trust, integrity, and collaboration. We prioritize our clients' needs and work tirelessly to ensure the ideal candidate-job match. Join us in our commitment to excellence and become part of a dynamic team focused on driving success for individuals and organizations alike. Role Responsibilities Design, develop, and implement data pipelines using Azure Data Factory. Create and maintain data models for structured and unstructured data. Extract, transform, and load (ETL) data from various sources into data warehouses. Develop analytical solutions and dashboards using Azure Databricks. Perform data integration and migration tasks with Azure tools. Ensure optimal performance and scalability of data solutions. Collaborate with cross-functional teams to understand data requirements. Utilize SQL Server for database management and data queries. Implement data quality checks and ensure data integrity. Work on data governance and compliance initiatives. Monitor and troubleshoot data pipeline issues to ensure reliability. Document data processes and architecture for future reference. Stay current with industry trends and Azure advancements. Train and mentor junior data engineers and team members. Participate in design reviews and provide feedback for process improvements. Qualifications Bachelor's degree in Computer Science, Information Technology, or a related field. 3+ years of experience in a data engineering role. Strong expertise in Azure Data Factory and Azure Databricks. Proficient in SQL for data manipulation and querying. Experience with data warehousing concepts and practices. Familiarity with ETL tools and processes. Knowledge of Python or other programming languages for data processing. Ability to design scalable cloud architecture. Experience with data modeling and database design. Effective communication and collaboration skills. Strong analytical and problem-solving abilities. Familiarity with performance tuning and optimization techniques. Knowledge of data visualization tools is a plus. Experience with Agile methodologies. Ability to work independently and manage multiple tasks. Willingness to learn and adapt to new technologies. Skills: etl,azure databricks,sql server,azure,data governance,azure data factory,python,data warehousing,data engineer,data integration,performance tuning,python scripting,sql,data modeling,data migration,data visualization,analytical solutions,pyspark,agile methodologies,data quality checks",,,"Python, SQL",
4235048754,azure data engineer,Tata Consultancy Services,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Role- azure data engineer Experience- 8-10yrs Location- Kolkata Must-Have** ETL, Azure Data Factory, SSRS, MS Fabric, Python, PowerShell SN Responsibility of / Expectations from the Role 1 Azure Data Engineer 2 Develop full SDLC project plans to implement ETL solution and identify resource requirements, Good Knowledge of SQL server complex queries, joins, etc. 3 Rest API, ADF pipeline, MS Fabric 4 SSIS and Azure Data Factory based ETL architecture. 5 Good exposure in Client Communication and supporting requests from customer",,,"Python, SQL",
4251678214,Remote Python AI Engineer - 17852,Turing,"Pune, Maharashtra, India (Remote)",Save Remote Python AI Engineer - 17852¬†  at Turing,Contract,,"About the job Work on Real-World Problems with Global Tech Experts Join a leading U.S.-based technology company as a Python Developer / AI Engineer, where you‚Äôll tackle real-world challenges and build innovative solutions alongside top global experts. This is a fully remote, contract-based opportunity ideal for developers passionate about Python, data analysis, and AI-driven work. Key Responsibilities: Write efficient, production-grade Python code to solve complex problems. Analyze public datasets and extract meaningful insights using Python and SQL. Collaborate with researchers and global teams to iterate on data-driven ideas. Document all code and development decisions in Jupyter Notebooks or similar platforms. Maintain high-quality standards and contribute to technical excellence. Job Requirements: Open to all levels: junior, mid-level, or senior engineers. Degree in Computer Science, Engineering, or equivalent practical experience. Proficient in Python programming for scripting, automation, or backend development. Experience with SQL/NoSQL databases is a plus. Familiarity with cloud platforms (AWS, GCP, Azure) is advantageous. Must be able to work 5+ hours overlapping with Pacific Time (PST/PT). Strong communication and collaboration skills in a remote environment. Perks & Benefits: Work on cutting-edge AI and data projects impacting real-world use cases. Collaborate with top minds from Meta, Stanford, and Google. 100% remote ‚Äì work from anywhere. Contract role with flexibility and no traditional job constraints. Competitive compensation in USD, aligned with global tech standards. Selection Process: Shortlisted developers may be asked to complete an assessment. If you clear the assessment, you will be contacted for contract assignments with expected start dates, durations, and end dates. Some contract assignments require fixed weekly hours, averaging 20/30/40 hours per week for the duration of the contract assignment.",,,"Python, SQL, Data Analysis",
4247476752,Data Governance Engineer,Vialto Partners,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Company Description Vialto Partners is a market leader in global mobility services. Our purpose is to ‚ÄòConnect the world‚Äô. We are unique and the only stand-alone global mobility business. This presents a rare opportunity for our clients, stakeholders and colleagues. Our teams help companies streamline and effectively manage their global mobility programs in a cost-efficient and compliant manner. Our services focus on providing cross-border compliance and risk assessment for tax, immigration, business travel, rewards and compensation, and remote work. Working at Vialto Partners is about getting the chance to be part of a global and dynamic team. Globally, Vialto Partners has over 6,500 staff in over 50 countries around the world, and continues to grow. You will work with clients from a range of industries and different geographical locations. We believe in connecting the world and supporting our colleagues to do the same in their careers by undertaking assignments and opportunities globally that broaden their skills and ultimately benefit our clients. Vialto is unstoppable when we work together in a culture of belonging, where everyone can thrive. We encourage employees to bring their true selves and share their unique talents and expertise to positively impact the communities we serve. To learn more about what we do, tune in to our podcast On the Move to hear expert insights on issues affecting global mobility, and read about the latest news in the industry. You can also follow us on Linkedin and Instagram. Job Description Job Title: Data Governance Lead Department: AI & Data Governance Position Overview: We are seeking a detail-oriented and proactive Data Governance Lead to join our AI & Data Governance team. This individual will play a pivotal role in developing, maintaining, and supporting data management initiatives, AI-driven solutions, and governance frameworks. The role focuses on maximizing the value of enterprise data assets to deliver actionable insights that drive informed business decisions. The ideal candidate will be responsible for creating, maintaining, and enhancing data catalog platform & improve data quality, accessibility, and overall data maturity across the organization. Key Responsibilities: Creation, Maintenance, and Enhancement of Smart Data & Products Discovery: Create and maintain a comprehensive data catalog and data products dictionary to ensure data assets are well-documented and easily accessible. Develop and update a field-level business dictionary for data products to standardize terminology and enhance understanding across teams. Integrate and Implement AI-powered search and recommendation capabilities to improve discoverability of data products and provide intelligent responses to user queries. Creation and Maintenance of Client 360: Design and Implement Assignee and Client 360 views by integrating data from various sources, including Sales, Financial, and CRM systems, to provide a holistic view of Client engagements across Vialto and its interactions. AI-Driven MDM & DQ: Creating a Data Quality Platform and Master Data Management Framework using AI to automate data profiling, anomaly detection, entity resolution, and metadata enrichment ensuring accurate, consistent, and trusted data across the enterprise. Qualifications: Education: Bachelor‚Äôs or Master‚Äôs degree in Computer science , or a related field. Experience: 7+ years of experience in data management, AI-driven solutions, and data governance initiatives. Proven expertise in creating, maintaining, and enhancing data dictionaries, business glossaries, and supporting documentation. Strong familiarity with AI-driven platforms and their practical applications in data governance and discovery workflows. Demonstrated experience working with Enterprise data and KPIs, along with a solid understanding of enterprise processes and terminologies. Proficient in data integration techniques and tools, including Azure Data Factory / Pipelines. Knowledge of Data Quality (DQ) and Master Data Management (MDM) principles, including metadata management, entity resolution, and data stewardship. Familiar with data governance frameworks and data automation practices to improve efficiency and accuracy across data operations. Technical Skills: Strong proficiency in Python, Azure AI/ML, LLM and GenAI. Basic SQL skills for querying and extracting data from databases. Familiarity with ERP systems and financial software (e.g., SAP, Workday). A working knowledge of data modeling and Data Architecture is a plus. Soft Skills: Exceptional communication skills, with the ability to present complex enterprise data in a clear, concise manner. Strong problem-solving capabilities and acute attention to detail. Ability to work collaboratively across cross-functional teams and manage multiple projects simultaneously. Excellent organizational skills, with a keen ability to prioritize tasks and manage time effectively. Preferred Qualifications: Experience in leading data governance initiatives and projects. Knowledge of data integration techniques and tools. Additional Information Work Location: Hyderabad We are an equal opportunity employer that does not discriminate on the basis of any legally protected status. Please note, AI is used as part of the application process.",,,"Python, SQL",
4240091127,Data Engineer-Data Platforms,IBM,"Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities Experience with Scala object-oriented/object function Strong SQL background. Experience in Spark SQL, Hive, Data Engineer. SQL Experience with data pipelines & Data Lake Strong background in distributed comp Preferred Education Master's Degree Required Technical And Professional Expertise SQL Experience with data pipelines & Data Lake Strong background in distributed comp Experience with Scala object-oriented/object function Strong SQL background Preferred Technical And Professional Experience Core Scala Development Experience",,,SQL,
4235449363,Sr GCP Data Engineer,LTIMindtree,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Experience: 5-12 years Primary Skills: GCP, Big query, Python Notice period: Immediate to 30 days Overall 5+ years of experience in data eng Strong Experience with GCP services like Big Query Cloud Storage composer etc Hands on experience in Python Proficient in Pyspark databricks Strong understanding of data warehouse concepts ETL process and data modelling GCP certifications are added advantage",,,Python,
4244569726,Cloud Data Engineer,Uplers,"Dehradun, Uttarakhand, India (Remote)",Remote,‚Çπ1.8M/yr - ‚Çπ2.8M/yr,,"About the job Experience : 5.00 + years Salary : INR 1800000-2800000 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Intelebee LLC) (*Note: This is a requirement for one of Uplers' client - Intelebee LLC) What do you need for this opportunity? Must have skills required: Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory Intelebee LLC is Looking for: Data Engineer:We are seeking a skilled and hands-on Cloud Data Engineer with 5-8 years of experience to drive end-to-end data engineering solutions. The ideal candidate will have a deep understanding of dimensional modeling, data warehousing (DW), Lakehouse architecture, and the Medallion architecture. This role will focus on leveraging Azure's/AWS ecosystem to build scalable, efficient, and secure data solutions. You will work closely with customers to understand requirements, create technical specifications, and deliver solutions that scale across both on-premise and cloud environments. Key Responsibilities: End-to-End Data Engineering Lead the design and development of data pipelines for large-scale data processing, utilizing Azure/AWS tools such as Azure Data Factory, Azure Synapse, Azure functions, Logic Apps , Azure Databricks, and Data Lake Storage. Tools, AWS Lambda, AWS Glue Develop and implement dimensional modeling techniques and data warehousing solutions for effective data analysis and reporting. Build and maintain Lakehouse and Medallion architecture solutions for streamlined, high-performance data processing. Implement and manage Data Lakes on Azure/AWS, ensuring that data storage and processing is both scalable and secure. Handle large-scale databases (both on-prem and cloud) ensuring high availability, security, and performance. Design and enforce data governance policies for data security, privacy, and compliance within the Azure ecosystem. 5 How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory",,,Data Analysis,
4036640202,Azure Data Engineer,Medline India,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job About Medline: Title: Azure Data Engineer About Medline India: Medline India was setup in 2010 in Pune, primarily as an offshore Development centre and to augment resources for Medline Industries LP headquartered in Chicago, USA. We are a 1500+ strong and growing team of technology, finance & and business support professionals who support our businesses worldwide towards a mission to make healthcare run better. We are an organization with a conducive work environment, ample opportunities to learn, contribute and grow with a highly empowered & engaged team. We encourage our people to share their best ideas and create new opportunities for our customers and ourselves to work together to solve today‚Äôs toughest healthcare challenges. About Medline Industries, LP: Established in 1966, Medline Industries LP is a renowned global healthcare organization boasting 56 years of consecutive sales growth, exceeding $21 billion in annual sales. With a workforce of over 36,000 professionals spread across the globe, we operate in more than 125 countries and territories. As the largest privately held manufacturer and distributor of medical supplies in the United States, Medline is uniquely positioned to offer comprehensive products, education, and support across the continuum of care. At present, Medline Industries, LP holds the esteemed position as the #1 market leader, delivering an extensive portfolio of over 550,000 medical products and clinical solutions. Our clientele includes hospitals, extended care facilities, surgery centres, physician offices, home care agencies, providers, and retailers. We're proud to be recognized by Forbes as one of America‚Äôs Best Large Employers and Best Employers for Women. Additionally, the Chicago Tribune has consistently named us a Top Workplace for the past 12 years. Job Description: Medline India is seeking a highly skilled self-motivated Azure Data Engineer to join our business intelligence team. This individual will be responsible for building data pipelines, curation, integration and distribution of data across our on-prem and cloud landscape. We expect the candidate to have strong data engineering and data modeling knowledge. Required Skills: 8+ years of experience in Business Intelligence, Data Engineering, or related roles. Hands-on experience with Azure services. Design and implement data solutions using Azure services such as Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure Data Lake Drive best practices in data engineering, data modeling and data integration to ensure the reliability, scalability, and performance of data solutions. Good working knowledge on SAP systems and underling data Excellent oral and written communication skills with ability to independently engage all stakeholders at onsite and offshore Good data analytical skills to analyze data and understand business requirements. Strong prioritizing, interpersonal, problem-solving, task management (from conception to completion), & planning skills. Excellent knowledge of SQL for performing data analysis and performance tuning Ability to write highly optimized SQL queries for complex requirements Ability to test and document end-to-end processes Strong understanding of SDLC and best development practices. Experience in Agile/Scrum methodologies and project management tools such as Jira. Certifications in relevant BI and cloud technologies (preferably Azure) are a plus. Knowledge of Microsoft Fabric (preferred). Desired Skills (Good To Have): Data modeling (Native HANA), TDV, SQL Scripting Hands-on experience on Power BI Experience of ETL tools, preferably Talend Responsibilities: Talk to Business and work on requirement gathering Get the functional understanding for the requirements and create business requirement documents Responsible for designing relational and non-relational data stores on Azure. Responsible for designing and developing solutions in Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Azure Data Bricks, Synapse, SQL Server, HDInsight. Gather and process raw data at scale that meet functional / non-functional business requirements (including writing scripts, REST API calls, SQL Queries, etc.) Responsible for developing data set processes for data modeling, mining and production. Responsible for maintaining Raw and Curated Zones on ADLS and maintaining data temperatures across storage areas. Responsible for cost analysis and optimization of Azure resources used in BI Landscape and for designing governance solutions to make sure that the resources are used in an optimal and cost-effective manner. Responsible for supporting our Software Developers, Data Analysts and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Develop complex SQL queries in Azure SQL. Mentor and guide junior team members to get deliverables complete. Participate in defining best practices around Data models development and deliverables. End to end delivery of any project including Business communication, development, testing, migrations, support, user training and documentation. Responsible for query/reports performance tuning Identify, analyze, and interpret trends or patterns in complex data. Transform analyses into concrete, actionable recommendations to drive decision-making.",,,"SQL, Power BI, Data Analysis",
4246298175,Data Engineer,Droisys,India (Remote),Remote,Full-time,,"About the job About the job Droisys is an innovation technology company focused on helping companies accelerate their digital initiatives from strategy and planning through execution. We leverage deep technical expertise, Agile methodologies, and data-driven intelligence to modernize systems of engagement and simplify human/tech interaction. Amazing things happen when we work in environments where everyone feels a true sense of belonging and when candidates have the requisite skills and opportunities to succeed. At Droisys, we invest in our talent and support career growth, and we are always on the lookout for amazing talent who can contribute to our growth by delivering top results for our clients. Join us to challenge yourself and accomplish work that matters. Job Title: Data Engineer with Cortex ( or Similar AI experience) Location: India / Remote Key Responsibilities: Design and implement ETL/ELT workflows using Snowflake and Python. Build scalable and secure data pipelines using AWS services such as S3, Lambda, Glue, Redshift, and EMR. Develop, manage, and optimize data models and warehouse architecture in Snowflake. Leverage Cortex for deploying and managing machine learning workflows or real-time data processing (based on your use case). Utilize Docker and container orchestration tools (e.g., Kubernetes) for deploying data applications. Collaborate with cross-functional teams to understand data requirements and ensure high data quality and availability. Monitor and troubleshoot performance issues across data infrastructure. Droisys is an equal opportunity employer. We do not discriminate based on race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law. Droisys believes in diversity, inclusion, and belonging, and we are committed to fostering a diverse work environment.",,,"Python, Machine Learning",
4240853420,Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Skill: Data Engineer Role: T3, T2 Key Responsibility Data Engineer Must have 5+ years of experience in below mentioned skills. Must Have: Big Data Concepts , Python(Core Python- Able to write code), SQL, Shell Scripting, AWS S3 Good to Have: Event-driven/AWA SQS, Microservices, API Development, Kafka, Kubernetes, Argo, Amazon Redshift, Amazon Aurora Desired Skills and Experience Amazon Redshift",,,"Python, SQL",
4077197626,Data Engineer-II,TMRW House of Brands,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Responsibilities Create, implement and operate the strategy for robust and scalable data pipelines for business intelligence and machine learning. Develop and maintain core data framework and key infrastructures Create and support the ETL pipeline to get the data flowing correctly from the existing and new sources to our data warehouse. Data Warehouse design and data modeling for efficient and cost-effective reporting Collaborate with data analysts, data scientists, and other data consumers within the business to manage the data warehouse table structure and optimize it for reporting. Constantly striving to improve software development process and team productivity Define and implement Data Governance processes related to data discovery, lineage, access control and quality assurance Perform code reviews and QA data imported by various processes Qualifications 3-5 years of experience. At least 2+ years of experience in data engineering and data infrastructure space on any of the big data technologies: Hive, Spark, Pyspark(Batch and Streaming), Airflow, Redshift and Delta Lake. Experience in product-based companies or startups. Strong understanding of data warehousing concepts and the data ecosystem. Strong Design/Architecture experience architecting, developing, and maintaining solutions in AWS. Experience building data pipelines and managing the pipelines after they‚Äôre deployed. Experience with building data pipelines from business applications using APIs. Previous experience in Databricks is a big plus. Understanding of Dev Ops would be preferable though not a must Working knowledge of BI Tools like Metabase, and Power BI is a plus Experience of architecting systems for data access is a major plus.",,,"Power BI, Machine Learning",
4196665846,Business Intelligence Analyst / Data Engineer,Valtech,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job At Valtech, We‚Äôve Got Opportunities To Offer You ‚Äî For Growth, For Leadership, For Big, World-changing Impact And For, Dare We Say It, Fun. We Are a Global Workforce That Is Committed To Building a World That Works Better For Everyone. And That Starts With Our Kin. That‚Äôs Why We‚Äôre Proud Of The role Ideal candidates will have strong quantitative backgrounds and analytical discipline. While they will have some demonstrated ability to write code, they will not have learned programming languages for the sake of building their resume, but rather as a means to express their intellectual curiosity and analytical voice. Valtech will provide a platform and training to help them reach their full potential. Valtech is looking to hire a Business Intelligence Analyst / Data Engineer to join our growing capabilities team. If you are innovative, passionate about data and AI technologies, and look to continually learn and enjoy sharing expertise, read on! Role Responsibilities Analyze a collection of raw data sets to create meaningful impact to large enterprise clients while maintaining a high degree of scientific rigor and discipline. Engineer data pipelines and products to help stakeholders make and execute data driven decisions. Communicate analytical findings in an intuitive and visually compelling way. Creating highly visual and interactive dashboards via Tableau, PowerBI, or custom web applications Conducting deep dive analysis and designing KPIs to help guide business decisions and measure success Engineering data infrastructure, software libraries, and APIs supporting BI and ML data pipelines Architecting cloud data platform components enabling the above Building and tracking project timelines, dependences, and risks Gathering stakeholder requirements and conducting technical due diligence toward designing pragmatic data-driven business solutions Minimum Qualifications We want all new hires to succeed in their roles at Valtech. That's why we've outlined the job requirements below. To be considered for this role, it's important that you meet all Minimum Qualifications. If you do not meet all of the Preferred Qualifications, we still encourage you to apply. Proven industry experience executing data engineering, analytics, and/or data science projects or Bachelors/Masters degree in quantitative studies including Engineering, Mathematics, Statistics, Computer Science or computation-intensive Sciences and Humanities. Proficiency (can execute data ingestion to insight) in programmatic languages such as SQL, Python, and R. Preferred Qualifications Proficiency in visualization/reporting tools such as Tableau and PowerBI or programmatic visualization library such as R ggplot2, Python matplotlib/seaborn/bokeh, Javascript D3. Proficiency in big data environments and tools such as Spark, Hive, Impala, Pig, etc. Proficiency with cloud architecture components (AWS, Azure, Google) Proficiency with data pipeline software such as Airflow, Luigi, or Prefect Ability to turn raw data and ambiguous business questions into distilled findings and recommendations for action Experience with statistical and machine learning libraries along with the ability to apply them appropriately to business problems Experience leading and managing technical data/analytics/machine learning projects What We Offer You‚Äôll join an international network of data professionals within our organisation. We support continuous development through our dedicated Academy. If you're looking to push the boundaries of innovation and creativity in a culture that values freedom and responsibility, we encourage you to apply. At Valtech, we‚Äôre here to engineer experiences that work and reach every single person. To do this, we are proactive about creating workplaces that work for every person at Valtech. Our goal is to create an equitable workplace which gives people from all backgrounds the support they need to thrive, grow and meet their goals (whatever they may be). You can find out more about what we‚Äôre doing to create a Valtech for everyone here . Please do not worry if you do not meet all of the criteria or if you have some gaps in your CV. We‚Äôd love to hear from you and see if you‚Äôre our next member of the Valtech team!",,,"Python, SQL, Tableau, R, Machine Learning",
4154552016,Data Engineer,algoleap,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job We are seeking a highly skilled Data Engineer with extensive experience in Snowflake, Data Build Tool (dbt), Snaplogic, SQL Server, PostgreSQL, Azure Data Factory, and other ETL tools. The ideal candidate will have a strong ability to optimize SQL queries and a good working knowledge of Python. A positive attitude and excellent teamwork skills are essential. Key Responsibilities Data Pipeline Development: Design, develop, and maintain scalable data pipelines using Snowflake, dbt, Snaplogic and ETL tools. SQL Optimization: Write and optimize complex SQL queries to ensure high performance and efficiency. Data Integration: Integrate data from various sources, ensuring consistency, accuracy, and reliability. Database Management: Manage and maintain SQL Server and PostgreSQL databases. ETL Processes: Develop and manage ETL processes to support data warehousing and analytics. Collaboration: Work closely with data analysts, data scientists, and business stakeholders to understand data requirements and deliver solutions. Documentation: Maintain comprehensive documentation of data models, data flows, and ETL processes. Troubleshooting: Identify and resolve data-related issues and discrepancies. Python Scripting: Utilize Python for data manipulation, automation, and integration tasks. Qualifications Experience: Minimum of 9 years of experience in data engineering. Technical Skills Proficiency in Snowflake, dbt, Snaplogic, SQL Server, PostgreSQL, and Azure Data Factory. Strong SQL skills with the ability to write and optimize complex queries. Knowledge of Python for data manipulation and automation. Knowledge of data governance frameworks and best practices Soft Skills Excellent problem-solving and analytical skills. Strong Communication And Collaboration Skills. Positive attitude and ability to work well in a team environment. Certifications: Relevant certifications (e.g., Snowflake, Azure) are a plus.",,,"Python, SQL",
4229863293,Data Engineer - (Remote - India),Jobgether,India (Remote),Save Data Engineer - (Remote - India) at Jobgether,Full-time,,"About the job Description About Jobgether Jobgether is a Talent Matching Platform that partners with companies worldwide to efficiently connect top talent with the right opportunities through AI-driven job matching. One of our companies is currently looking for a Data Engineer in India . This is an opportunity to join a growing and dynamic team focused on building a modern, scalable, and efficient data infrastructure using Databricks. In this role, you‚Äôll design and maintain robust data pipelines and drive data integration from multiple business platforms into a centralized warehouse. You'll collaborate closely with cross-functional teams to create solutions that support analytical insights and strategic decisions across the organization. If you‚Äôre passionate about cloud data platforms, ETL processes, and driving business impact through data, we‚Äôd love to hear from you. Accountabilities Own and evolve data architecture to ensure high data quality, integration, and consistency across systems. Build and maintain efficient, scalable ETL/ELT pipelines in Databricks, handling data from platforms such as Salesforce, NetSuite, and Hubspot. Integrate structured and unstructured data into the data lakehouse while maintaining data integrity and security. Implement automated monitoring and alerting systems to ensure data pipeline reliability. Document processes, workflows, and architectural decisions, promoting maintainability and best practices in data engineering. Collaborate with cross-functional teams to understand data needs and develop tailored data solutions. Requirements Bachelor‚Äôs degree in Computer Science, Engineering, Statistics, or a related field. Minimum of 3 years‚Äô experience in data engineering, with at least 1 year of hands-on experience using Databricks or similar platforms. Strong SQL and Python programming skills for data manipulation and transformation. Proven experience with Apache Spark, Delta Lake, and Databricks environments. Solid understanding of cloud platforms such as AWS, Azure, or GCP, including storage and compute services. Experience in integrating CRM/ERP systems (e.g., Salesforce, NetSuite, Hubspot) into data platforms. Strong background in data warehousing, schema design, query optimization, and data governance. Excellent problem-solving and troubleshooting skills related to data quality and performance. Benefits Competitive salary and growth opportunities Flexible remote work environment Exposure to cutting-edge cloud and data technologies Opportunity to work in a global, fast-paced tech company Access to continuous learning and professional development Collaborative and innovative work culture Health and wellness benefits (varies by location) Jobgether hiring process disclaimer This job is posted on behalf of one of our partner companies. If you choose to apply, your application will go through our AI-powered 3-step screening process, where we automatically select the 5 best candidates. Our AI thoroughly analyzes every line of your CV and LinkedIn profile to assess your fit for the role, evaluating each experience in detail. When needed, our team may also conduct a manual review to ensure only the most relevant candidates are considered. Our process is fair, unbiased, and based solely on qualifications and relevance to the job. Only the best-matching candidates will be selected for the next round. If you are among the top 5 candidates, you will be notified within 7 days. If you do not receive feedback after 7 days, it means you were not selected. However, if you wish, we may consider your profile for other similar opportunities that better match your experience. Thank you for your interest!",,,"Python, SQL",
4247045246,Risk Data Engineer/ Leads,EY,"Chennai, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job At EY, you‚Äôll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we‚Äôre counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. RCE - Risk Data Engineer/Leads Job Description: Our Technology team builds innovative digital solutions rapidly and at scale to deliver the next generation of Financial and Non- Financial services across the globe. The Position is a senior technical, hands-on delivery role, requiring the knowledge of data engineering, cloud infrastructure and platform engineering, platform operations and production support using ground-breaking cloud and big data technologies. The ideal candidate with 3-6 years of relevant experience, will possess strong technical skills, an eagerness to learn, a keen interest on 3 keys pillars that our team support i.e. Financial Crime, Financial Risk and Compliance technology transformation, the ability to work collaboratively in fast-paced environment, and an aptitude for picking up new tools and techniques on the job, building on existing skillsets as a foundation. Senior Data & BI Analyst Job Summary: The Senior Data & BI Analyst is a key contributor to the Enterprise Analytics Center of Excellence (COE), responsible for promoting and supporting self-service analytics across the firm using tools such as ThoughtSpot and Tableau. This role requires a strong technical foundation in data analytics and BI tools, combined with strategic consulting skills to guide users in developing scalable, high-quality analytics. The ideal candidate has a passion for data, an understanding of financial services, and the ability to collaborate across teams to enable data-driven decision-making. Responsibilities: Contribute to the buildout and ongoing evolution of the Enterprise Analytics COE, shaping best practices and governance standards. Act as a subject matter expert (SME) in enterprise lakehouse data, guiding end users in identifying and leveraging key data sources. Work closely with business users to understand their analytics needs, providing consultation on BI tool selection and data strategies. Lead training sessions for ThoughtSpot (and Tableau) users to enhance adoption and proficiency in self-service analytics. Perform hands-on data analysis against enterprise lakehouse data to uncover insights and demonstrate tool capabilities. Develop reusable analytics templates, dashboards, and frameworks that drive consistency and efficiency across the organization. Support initial dashboard and analytics development efforts, assisting teams with prototyping and best practices implementation. Stay up to date on emerging BI trends, ThoughtSpot and Tableau advancements, and AI-driven analytics approaches to drive innovation. Required Qualifications: Experience working as a Technical Data Analyst, BI Analyst, or Business Analyst in an enterprise environment. Strong SQL skills and experience handling large datasets. Proficiency in data warehousing concepts, ETL processes, and BI reporting tools such as Tableau, Power BI, Qlik, ThoughtSpot. Excellent analytical and problem-solving skills with the ability to collaborate across cross-functional teams. Strong stakeholder management abilities, with the ability to effectively communicate technical insights to non-technical audiences. Preferred Qualifications: Hands-on experience with ThoughtSpot BI Platform. Financial services experience and understanding of financial data structures. Familiarity with cloud-based BI platforms (AWS, Azure, etc.). Knowledge of Natural Language Processing (NLP), AI-driven data aggregation, and automated reporting technologies. EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",,,"SQL, Tableau, Power BI, Data Analysis",
4241887601,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You‚Äôll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you‚Äôll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you‚Äôll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour‚Äôs. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization. Strong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources. Proficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities. Hands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements Preferred Technical And Professional Experience Understanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling. Ability to implement robust data validation, cleansing, and governance frameworks within ETL processes. Proficiency in SQL and/or Shell scripting for custom transformations and automation tasks",,,"SQL, Machine Learning",
4244569716,Cloud Data Engineer,Uplers,"Guwahati, Assam, India (Remote)",Remote,‚Çπ1.8M/yr - ‚Çπ2.8M/yr,,"About the job Experience : 5.00 + years Salary : INR 1800000-2800000 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Intelebee LLC) (*Note: This is a requirement for one of Uplers' client - Intelebee LLC) What do you need for this opportunity? Must have skills required: Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory Intelebee LLC is Looking for: Data Engineer:We are seeking a skilled and hands-on Cloud Data Engineer with 5-8 years of experience to drive end-to-end data engineering solutions. The ideal candidate will have a deep understanding of dimensional modeling, data warehousing (DW), Lakehouse architecture, and the Medallion architecture. This role will focus on leveraging Azure's/AWS ecosystem to build scalable, efficient, and secure data solutions. You will work closely with customers to understand requirements, create technical specifications, and deliver solutions that scale across both on-premise and cloud environments. Key Responsibilities: End-to-End Data Engineering Lead the design and development of data pipelines for large-scale data processing, utilizing Azure/AWS tools such as Azure Data Factory, Azure Synapse, Azure functions, Logic Apps , Azure Databricks, and Data Lake Storage. Tools, AWS Lambda, AWS Glue Develop and implement dimensional modeling techniques and data warehousing solutions for effective data analysis and reporting. Build and maintain Lakehouse and Medallion architecture solutions for streamlined, high-performance data processing. Implement and manage Data Lakes on Azure/AWS, ensuring that data storage and processing is both scalable and secure. Handle large-scale databases (both on-prem and cloud) ensuring high availability, security, and performance. Design and enforce data governance policies for data security, privacy, and compliance within the Azure ecosystem. 5 How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory",,,Data Analysis,
4219514472,AWS Data Engineer,Virtusa,"Chennai, Tamil Nadu, India (Hybrid)",Hybrid,Full-time,,"About the job P1-C3-STS Seeking a developer who has good Experience in Athena, Python code, Glue, Lambda, DMS , RDS, Redshift Cloud Formation and other AWS serverless resources. Can optimize data models for performance and efficiency. Able to write SQL queries to support data analysis and reporting Design, implement, and maintain the data architecture for all AWS data services. Work with stakeholders to identify business needs and requirements for data-related projects Design and implement ETL processes to load data into the data warehouse Good Experience in Athena, Python code, Glue, Lambda, DMS , RDS, Redshift, Cloud Formation and other AWS serverless resources Cloud Formation and other AWS serverless resources Desired Skills and Experience Amazon Redshift",,,"Python, SQL, Data Analysis",
4245290661,GCP Data Engineer,TELUS Digital,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job About Us : At TELUS Digital, we enable customer experience innovation through spirited teamwork, agile thinking, and a caring culture that puts customers first. TELUS Digital is the global arm of TELUS Corporation, one of the largest telecommunications service providers in Canada. We deliver contact center and business process outsourcing (BPO) solutions to some of the world's largest corporations in the consumer electronics, finance, telecommunications and utilities sectors. With global call center delivery capabilities, our multi-shore, multi-language programs offer safe, secure infrastructure, value-based pricing, skills-based resources and exceptional customer service - all backed by TELUS, our multi-billion dollar telecommunications parent. Required Skills : 5+ years of industry experience in data engineering, business intelligence, or a related field with experience in manipulating, processing, and extracting value from datasets. Expertise in architecting, designing, building, and deploying internal applications to support technology life cycle management, service delivery management, data, and business intelligence. Experience in developing modular code for versatile pipelines or complex ingestion frameworks aimed at loading data into Cloud SQL and managing data migration from multiple on-premises sources. Strong collaboration with analysts and business process owners to translate business requirements into technical solutions. Proficiency in coding with scripting languages (Shell scripting, Python, SQL). Deep understanding and hands-on experience with Google Cloud Platform (GCP) technologies, especially in data migration and warehousing, including Database Migration Service (DMS), Cloud SQL, BigQuery, Dataflow, Data Catalog, Cloud Composer, Google Cloud Storage (GCS), IAM, Compute Engine, Cloud Data Fusion, and optionally Dataproc. Adherence to best development practices including technical design, solution development, systems configuration, test documentation/execution, issue identification and resolution, and writing clean, modular, self-sustaining code. Familiarity with CI/CD processes using GitHub, Cloud Build, and Google Cloud SDK. Qualifications: Bachelor's degree in Computer Science or a related technical field, or equivalent practical experience. GCP Certified Data Engineer (preferred). Excellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists, engineering teams, and business audiences.",,,"Python, SQL",
4251650403,Data Engineer,HCLTech,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Primary & Secondary Skill Databricks ‚Äì Pyspark, Python and Collibra ( Primary) Unity catalog ETL AWS JD (Detailed) Design of data solutions on Databricks including delta lake, data warehouse, data marts and other data solutions to support the analytics needs of the organization. Proficiency in using Collibra Data Governance Center, Data Catalog, and Collibra Connect for data management and governance. Apply best practices during design in data modeling (logical, physical) and ETL pipelines (streaming and batch) using cloud-based services especially Python & Pyspark Design, develop and manage the pipelining (collection, storage, access), data engineering (data quality, ETL, Data Modelling) and understanding (documentation, exploration) of the data. Interact with stakeholders regarding data landscape understanding, conducting discovery exercises, developing proof of concepts, and demonstrating it to stakeholders. Implement data quality frameworks and standards using Collibra to ensure the integrity and accuracy of data Excellent collaboration skills to work effectively with cross-functional teams Strong verbal and written communication skills",,,Python,
4247342547,Data Engineer IRC262774,GlobalLogic,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job Description We are looking for a skilled Data Engineer to join our existing team. Candidates will be working on Big Data applications using cutting edge technologies including Google Cloud Platform. The position offers 1st hand exposure to build data pipelines that can process peta-byte scale of data solving complex business problems Requirements Mandatory Skills: 4-6 years of hands-on experience in Data Engineering. Experience in writing and optimizing SQL queries in HIVE/Spark. Excellent coding and/or scripting skills in Python. Good experience in deploying spark applications in Kubernetes cluster. Good experience in development, deployment & troubleshooting of Spark applications. Exposure to any cloud environment (AWS/GCP preferred) Job responsibilities Role Description: Candidate will be part of an agile team. Development/Migration of new data pipelines. Optimizing/Fine-tuning existing workflows. Deploying Spark tasks on K8 clusters. Bringing new ideas for performance enhancement of data pipelines running on K8s.. Mandatory Skills: 4-6 years of hands-on experience in Data Engineering. Experience in writing and optimizing SQL queries in HIVE/Spark. Excellent coding and/or scripting skills in Python. Good experience in deploying spark applications in Kubernetes cluster. Good experience in development, deployment & troubleshooting of Spark applications. Exposure to any cloud environment (AWS/GCP preferred) What we offer Culture of caring. At GlobalLogic, we prioritize a culture of caring. Across every region and department, at every level, we consistently put people first. From day one, you‚Äôll experience an inclusive culture of acceptance and belonging, where you‚Äôll have the chance to build meaningful connections with collaborative teammates, supportive managers, and compassionate leaders. Learning and development. We are committed to your continuous learning and development. You‚Äôll learn and grow daily in an environment with many opportunities to try new things, sharpen your skills, and advance your career at GlobalLogic. With our Career Navigator tool as just one example, GlobalLogic offers a rich array of programs, training curricula, and hands-on opportunities to grow personally and professionally. Interesting & meaningful work. GlobalLogic is known for engineering impact for and with clients around the world. As part of our team, you‚Äôll have the chance to work on projects that matter. Each is a unique opportunity to engage your curiosity and creative problem-solving skills as you help clients reimagine what‚Äôs possible and bring new solutions to market. In the process, you‚Äôll have the privilege of working on some of the most cutting-edge and impactful solutions shaping the world today. Balance and flexibility. We believe in the importance of balance and flexibility. With many functional career areas, roles, and work arrangements, you can explore ways of achieving the perfect balance between your work and life. Your life extends beyond the office, and we always do our best to help you integrate and balance the best of work and life, having fun along the way! High-trust organization. We are a high-trust organization where integrity is key. By joining GlobalLogic, you‚Äôre placing your trust in a safe, reliable, and ethical global company. Integrity and trust are a cornerstone of our value proposition to our employees and clients. You will find truthfulness, candor, and integrity in everything we do. About GlobalLogic GlobalLogic, a Hitachi Group Company, is a trusted digital engineering partner to the world‚Äôs largest and most forward-thinking companies. Since 2000, we‚Äôve been at the forefront of the digital revolution ‚Äì helping create some of the most innovative and widely used digital products and experiences. Today we continue to collaborate with clients in transforming businesses and redefining industries through intelligent products, platforms, and services.",manager,,"Python, SQL",
4241229022,GCP Data Engineer,Vista Applied Solutions Group Inc,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Job Summary: We are looking for GCP Data Engineer and this is Fulltime position. Hybrid in Bengaluru Experience: Experience as a Data Engineer with expertise in GCP technologies such as BigQuery, Data Flow, etc. Strong understanding of SQL concepts including joins, aggregations, filtering, etc. Proficiency in writing efficient Python code for data processing tasks.",,,"Python, SQL",
4245467507,Azure Data Engineer,The Blue Owls,India (Remote),Remote,Full-time,,"About the job Location: Remote Experience: 3+ Years Salary Range: 12-18 LPA Role Summary We‚Äôre seeking a seasoned Azure Data Engineer / Consultant to lead design, build, and operationalize cloud-native data solutions. You‚Äôll partner with external clients‚Äîfrom discovery through Go-Live‚Äîto ingest, model, transform, and serve high-volume data using Synapse, Fabric, PySpark, and SQL. You‚Äôll also establish best practices around CI/CD, security, and data quality within a medallion architecture. Technical Requirements: 3+ years of data engineering experience leading implementations of large-scale lakehouses on Databricks, Microsoft Fabric, or Synapse . Prior experience using PowerBI will be a plus Exposure to a broad range of Microsoft and Azure data services (Databricks, Synapse, Microsoft Fabric) and tooling. A solid understanding of functional architecture, modern data warehousing, big data analytics and CI/CD best practices. Extensive experience with Azure data services (Databricks, Synapse, Microsoft Fabric) and related azure infrastructure services like firewall, storage, key vault etc. is required Strong programming / scripting experience using SQL and python and Spark Knowledge of software configuration management environments and tools such as JIRA, Git, Jenkins, TFS, Shell, PowerShell, Bitbucket Experience with Agile development methods in data-oriented projects Preferred Skills & Certifications: DP-600, DP-700 or equivalent certification. Experience with Azure Purview, Data Catalogs, or metadata management tools. Familiarity with orchestration frameworks (ADF, Synapse Pipelines), Spark optimization (broadcast joins, partition pruning), and data-quality frameworks (Great Expectations, custom). Prior consulting or client-facing engagements in enterprise environments. Knowledge of BI tools (Power BI). What We Offer: Opportunity to own and shape cutting-edge data solutions for diverse industries. Collaborative, outcome-driven culture with career growth and skill-building opportunities. Flexible hours and remote-first work model. Competitive compensation and benefits package.",,,"Python, SQL, Power BI",
4243628151,AI-First Data Analyst/Data Engineer,GroMo,"Gurugram, Haryana, India (On-site)",On-site,‚Çπ700K/yr - ‚Çπ1.1M/yr,,"About the job We seek a forward-thinking, AI-first Data Analyst who leverages AI tools to transform data into actionable business insights. The ideal candidate will have a strong foundation in data analysis combined with an innovative mindset to solve complex problems using AI technologies. You'll replace traditional manual processes with AI-powered solutions, working with tools like Redshift, Zoho Analytics, SQL, and QuickSight to drive data-driven decision-making across the organization. Key Responsibilities AI-Powered Analysis: Utilize AI tools (Claude, Llama, Grok, Copilot, etc.) to analyse large datasets, generate insights, and automate repetitive analytical tasks Intelligent Dashboard Development: Create and maintain dynamic dashboards and reports using AI-assisted development, leveraging Redshift, QuickSight, and Zoho Analytics AI-Enhanced Problem Solving: Tackle complex analytical challenges by combining domain expertise with AI capabilities to solve problems previously considered unsolvable Automated Data Pipeline Management: Design and optimise data pipelines using AI tools for code generation, debugging, and performance enhancement Advanced Analytics: Conduct sophisticated cohort analysis, and predictive modeling using AI-powered statistical tools Intelligent Reporting: Generate comprehensive insights on customer behavior and business metrics using AI for pattern recognition and trend analysis Innovation Leadership: Continuously explore and implement new AI tools and methodologies to enhance analytical capabilities Key Requirements Bachelor's degree in Data Science, Statistics, Computer Science, Economics, or related field 2+ years of experience in data analysis with demonstrated strong AI tool proficiency Strong AI-first mindset - Must be proficient in leveraging AI APIs (ChatGPT, Claude, GitHub Copilot, etc.) for data analysis, code generation, and problem-solving Proven track record of using AI to solve analytical problems that couldn't be solved through traditional methods Advanced SQL skills with experience in Redshift, QuickSight, and Zoho Analytics Experience with AI-assisted coding and automation tools Strong analytical thinking combined with creative problem-solving using AI Excellent communication skills to present AI-generated insights to stakeholders Self-motivated learner who stays current with AI developments in data analytics Nice To Have Experience with machine learning platforms and AI/ML model deployment Knowledge of advanced prompt engineering and AI workflow optimization Experience in SaaS or product-driven companies Background in implementing AI solutions for business intelligence What Sets You Apart: You don't just use AI as a helper‚Äîyou think AI-first. You approach every analytical challenge by first considering how AI can enhance, automate, or completely revolutionize the solution. You're excited about pushing the boundaries of what's possible in data analysis through intelligent tool usage.",,,"SQL, Machine Learning, Data Analysis",
4248748860,"Consultant II - Data Engineer, Python",Hakk≈çda,"Jaipur, Rajasthan, India (On-site)",On-site,Full-time,,"About the job About Hakkoda Hakkoda, an IBM Company, is a modern data consultancy that empowers data driven organizations to realize the full value of the Snowflake Data Cloud. We provide consulting and managed services in data architecture, data engineering, analytics and data science. We are renowned for bringing our clients deep expertise, being easy to work with, and being an amazing place to work! We are looking for curious and creative individuals who want to be part of a fast-paced, dynamic environment, where everyone‚Äôs input and efforts are valued. We hire outstanding individuals and give them the opportunity to thrive in a collaborative atmosphere that values learning, growth, and hard work. Our team is distributed across North America, Latin America, India and Europe. If you have the desire to be a part of an exciting, challenging, and rapidly-growing Snowflake consulting services company, and if you are passionate about making a difference in this world, we would love to talk to you!. We are seeking a skilled and collaborative Sr. Data/Python Engineer with experience in the development of production Python-based applications (Such as Django, Flask, FastAPI on AWS) to support our data platform initiatives and application development. This role will initially focus on building and optimizing Streamlit application development frameworks, CI/CD Pipelines, ensuring code reliability through automated testing with Pytest , and enabling team members to deliver updates via CI/CD pipelines . Once the deployment framework is implemented, the Sr Engineer will own and drive data transformation pipelines in dbt and implement a data quality framework. Key Responsibilities Lead application testing and productionalization of applications built on top of Snowflake - This includes implementation and execution of unit testing and integration testing - Automated test suites include use of Pytest and Streamlit App Tests to ensure code quality, data accuracy, and system reliability. Development and Integration of CI/CD pipelines (e.g., GitHub Actions, Azure DevOps, or GitLab CI) for consistent deployments across dev, staging, and production environments. Development and testing of AWS-based pipelines - AWS Glue, Airflow (MWAA), S3. Design, develop, and optimize data models and transformation pipelines in Snowflake using SQL and Python. Build Streamlit-based applications to enable internal stakeholders to explore and interact with data and models. Collaborate with team members and application developers to align requirements and ensure secure, scalable solutions. Monitor data pipelines and application performance, optimizing for speed, cost, and user experience. Create end-user technical documentation and contribute to knowledge sharing across engineering and analytics teams. Work in CST hours and collaborate with onshore and offshore teams. Qualifications, Skills & Experience 5+ years of experience in Data Engineering or Python based application development on AWS (Flask, Django, FastAPI, Streamlit) - Experience building data data-intensive applications on python as well as data pipelines on AWS in a must. Bachelor‚Äôs degree in computer science, Information Systems, Data Engineering, or a related field (or equivalent experience). Proficient in SQL and Python for data manipulation and automation tasks. Experience with developing and productionalizing applications built on Python based Frameworks such as FastAPI, Django, Flask. Experience with application frameworks such as Streamlit, Angular, React etc for rapid data app deployment. Solid understanding of software testing principles and experience using Pytest or similar Python frameworks. Experience configuring and maintaining CI/CD pipelines for automated testing and deployment. Familiarity with version control systems such as Gitlab. Knowledge of data governance, security best practices, and role-based access control (RBAC) in Snowflake. Preferred Qualifications Experience with dbt (data build tool) for transformation modeling. Knowledge of Snowflake‚Äôs advanced features (e.g., masking policies, external functions, Snowpark). Exposure to cloud platforms (e.g., AWS, Azure, GCP). Strong communication and documentation skills. Benefits Health Insurance Paid leave Technical training and certifications Robust learning and development opportunities Incentive Toastmasters Food Program Fitness Program Referral Bonus Program Hakkoda is committed to fostering diversity, equity, and inclusion within our teams. A diverse workforce enhances our ability to serve clients and enriches our culture. We encourage candidates of all races, genders, sexual orientations, abilities, and experiences to apply, creating a workplace where everyone can succeed and thrive. Ready to take your career to the next level? üöÄ üíª Apply todayüëá and join a team that‚Äôs shaping the future!! Hakkoda is an IBM subsidiary which has been acquired by IBM and will be integrated in the IBM organization. Hakkoda will be the hiring entity. By Proceeding with this application, you understand that Hakkoda will share your personal information with other IBM subsidiaries involved in your recruitment process, wherever these are located. More information on how IBM protects your personal information, including the safeguards in case of cross-border data transfer, are available here.",,,"Python, SQL",
4243629935,Azure Data Engineer,Tata Consultancy Services,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job Azure Data Engineer Primary Skills: SQL(Azure), Data Vault, Azure Data Factory, Blob Storage, Azure Synapse, Pipelines, Delta Lake Secondary Skills: Python, Apache Spark, MS Fabric (Trained Knowledge or experience), Power BI Proficiency in SQL and Python. SQL being far more important. Experience with Apache Spark for data processing (Python version) Understanding of Delta files and Lakehouse architecture Data warehouse basic knowledge or experience Hands-on skills (even gotten only by training) are essential for effectively using Microsoft Fabric. Practical experience in setting up end-to-end analytics, managing Lakehouse and Medallion Architecture, using Apache Spark and Delta Lake tables, handling data ingestion with Dataflows Gen2, creating pipelines with Data Factory, and setting up data warehouses is crucial. Understand the capabilities of Microsoft Fabric for complete analytics solutions, including data ingestion, transformation, storage, and visualization. Familiarity with features such as Direct Lake access for Power BI reports, is important. Utilize Apache Spark for large-scale data processing and work with Delta Lake tables for advanced data analytics. Ingest data using Dataflows Gen2 and create pipelines with Data Factory capabilities for multi-step data ingestion and transformation tasks. Set up and query data warehouses in Microsoft Fabric, integrating them with other analytics components. Learn how to secure a Microsoft Fabric data warehouse and administer the platform effectively.",,,"Python, SQL, Power BI",
4245669427,Azure Data Engineer,LTIMindtree,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job We are seeking an experienced and strategic Data to design, build, and optimize scalable, secure, and high-performance data solutions. You will play a pivotal role in shaping our data infrastructure, working with technologies such as Databricks, Azure Data Factory, SQL and PySpark, Key Responsibilities: ‚Ä¢ Design and develop scalable data pipelines using Databricks and Medallion (Bronze, Silver, Gold layers). ‚Ä¢ Write efficient PySpark and SQL code for data transformation, cleansing, and enrichment. ‚Ä¢ Build and manage data workflows in Azure Data Factory (ADF) including triggers, linked services, and integration runtimes. ‚Ä¢ Optimize queries and data structures for performance and cost-efficiency . ‚Ä¢ Develop and maintain CI/CD pipelines using GitHub for automated deployment and version control. ‚Ä¢ Collaborate with cross-functional teams to define data strategies and drive data quality initiatives. ‚Ä¢ Implement best practices for DevOps, CI/CD , and infrastructure-as-code in data engineering. ‚Ä¢ Troubleshoot and resolve performance bottlenecks across Spark, ADF, and Databricks pipelines. Requirements: ‚Ä¢ Bachelor‚Äôs or master‚Äôs degree in computer science, Information Systems, or related field. ‚Ä¢ Proven experience as a Data Architect or Senior Data Engineer. ‚Ä¢ Strong knowledge of Databricks , Azure Data Factory , Spark (PySpark) , and SQL . ‚Ä¢ Hands-on experience with data governance , security frameworks , and catalog management . ‚Ä¢ Proficiency in cloud platforms (preferably Azure). ‚Ä¢ Experience with CI/CD tools and version control systems like GitHub. ‚Ä¢ Strong communication and collaboration skills.",,,SQL,
4235453001,Sr GCP Data Engineer,LTIMindtree,"Delhi, India (Hybrid)",Hybrid,Full-time,,"About the job Experience: 5-12 years Primary Skills: GCP, Big query, Python Notice period: Immediate to 30 days Overall 5+ years of experience in data eng Strong Experience with GCP services like Big Query Cloud Storage composer etc Hands on experience in Python Proficient in Pyspark databricks Strong understanding of data warehouse concepts ETL process and data modelling GCP certifications are added advantage",,,Python,
4233207369,Data Engineer-Data Platforms,IBM,"Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution. Your primary responsibilities include: Lead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements. Strive for continuous improvements by testing the build solution and working under an agile framework. Discover and implement the latest technologies trends to maximize and build creative solutions Preferred Education Master's Degree Required Technical And Professional Expertise Experience with Apache Spark (PySpark): In-depth knowledge of Spark‚Äôs architecture, core APIs, and PySpark for distributed data processing. Big Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts. Strong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy. SQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation. Cloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems Preferred Technical And Professional Experience Define, drive, and implement an architecture strategy and standards for end-to-end monitoring. Partner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering, Good to have detection and prevention tools for Company products and Platform and customer-facing",,,"Python, SQL, Data Analysis",
4239492402,Azure Data Engineer,Dexian India,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Shift time: UK Shift (2 PM ‚Äì 11 PM) Work Location: Chennai/Bangalore Position Responsibilities Azure Data Engineer: - Excellent understanding on data architecture system (source, target, transformations, processing, etc.,) and migration b/w DB platforms Hands-On Experience of 6+ years on Azure data analytics and Datawarehouse in Azure. Must have hands-on experience in the Azure services like Azure Data Explorer, Azure Databricks , Azure Data factory, Azure Synapse Analytics and Azure Fabric etc Must have strong hands-on experience with Python Strong hands-on experience in creating data pipeline monitors in the Azure environment Perform pre and post data validations checks to ensure completeness of data migrated Experience building and optimizing data pipelines, architectures and data sets Experience with the deployment datasets within the customers Cloud or On Premise environments Strong analytic skills related to working with unstructured datasets Experience supporting and working with cross-functional teams in a dynamic environment Assist with validating prototyped deployment options across various environments - Dev, QA, Test Assist in security hardening and implement role-based security as needed for Customer requirements Write and maintain documentation (e.g. run books, test plans, test results, etc.) for applications Configure and tune the platform in each environment based on best practices Provide general guidance, best practices, troubleshooting assistance as related to the Data Platform Strong analytical, debugging and problem-solving skills Quick learner, self-motivated and has the ability to work independently Strong verbal, written communication skills and a collaborative problem solving style Systems integration, including design and development of APIs, Adapters, and Connectors Healthcare domain experience is preferred Tools & Technology Experience preferred: Object-oriented /object function scripting languages: Python Data migration from on premise systems - RDBMS to Cloud Datawarehouse Relational SQL and NoSQL databases, including Snowflake and PostgreSQL Data pipeline using Azure stack Azure cloud services: Datafactory, Databricks, SQL Datawarehouse Qualification : Bachelor‚Äôs/Master‚Äôs Degree. Relevant Development Experience : 6+ Years Overall IT Experience : 6 to 10 Years",Executive,,"Python, SQL",
4250877889,Data Research Engineer,Aptita,India (Remote),Remote,Full-time,,"About the job Job Title - Data Research Engineer ‚Äì AI/ML Location - Remote (Hybrid for Chennai& Mumbai) Experience- 3+ Years Responsibilities: ‚óè Develop methods to leverage the potential of LLM and AI within the team. ‚óè Proactive at finding new solutions to engage the team with AI/LLM, and streamline processes in the team. ‚óè Be a visionary with AI/LLM tools and predict how the use of future technologies could be harnessed early on so that when these technologies come out, the team is ahead of the game regarding how it could be used. ‚óè Assist in acquiring and integrating data from various sources, including web crawling and API integration. ‚óè Stay updated with emerging technologies and industry trends. ‚óè Explore third-party technologies as alternatives to legacy approaches for efficient data pipelines. ‚óè Contribute to cross-functional teams in understanding data requirements. ‚óè Assume accountability for achieving development milestones. ‚óè Prioritize tasks to ensure timely delivery, in a fast-paced environment with rapidly changing priorities. ‚óè Collaborate with and assist fellow members of the Data Research Engineering Team as required. ‚óè Leverage online resources effectively like Stack Overflow, ChatGPT, Bard, etc., while considering their capabilities and limitations. Skills and Experience ‚óè Bachelor's degree in Computer Science, Data Science, or a related field. Higher qualifications is a plus. ‚óè Think proactively and creatively regarding the next AI/LLM technologies and how to use them to the team‚Äôs and company‚Äôs benefits. ‚óè ‚ÄúThink outside the box‚Äù mentality. ‚óè Experience prompting LLMs in a streamlined way, taking into account how the LLM can potentially ‚Äúhallucinate‚Äù and return wrong information. ‚óè Experience building agentic AI platforms with modular capabilities and autonomous task execution. (crewai, lagchain, etc.) ‚óè Proficient in implementing Retrieval-Augmented Generation (RAG) pipelines for dynamic knowledge integration. (chromadb, pinecone, etc) ‚óè Experience managing a team of AI/LLM experts is a plus: this includes setting up goals and objectives for the team and fine-tuning complex models. ‚óè Strong proficiency in Python programming ‚óè Proficiency in SQL and data querying is a plus. ‚óè Familiarity with web crawling techniques and API integration is a plus but not a must. ‚óè Experience in AI/ML engineering and data extraction ‚óè Experience with LLMs, NLP frameworks (spaCy, NLTK, Hugging Face, etc.) ‚óè Strong understanding of machine learning frameworks (TensorFlow, PyTorch) ‚óè Design and build AI models using LLMs ‚óè Integrate LLM solutions with existing systems via APIs ‚óè Collaborate with the team to implement and optimize AI solutions ‚óè Monitor and improve model performance and accuracy ‚óè Familiarity with Agile development methodologies is a plus. ‚óè Strong problem-solving and analytical skills with attention to detail. ‚óè Creative and critical thinking. ‚óè Ability to work collaboratively in a team environment. ‚óè Good and effective communication skills. ‚óè Experience with version control systems, such as Git, for collaborative development. ‚óè Ability to thrive in a fast-paced environment with rapidly changing priorities. ‚óè Comfortable with autonomy and ability to work independently. Notice period: Immediate to 30 days Email to: poniswarya.m@aptita.com",,,"Python, SQL, Machine Learning",
4244571524,Cloud Data Engineer,Uplers,"Jamshedpur, Jharkhand, India (Remote)",Remote,‚Çπ1.8M/yr - ‚Çπ2.8M/yr,,"About the job Experience : 5.00 + years Salary : INR 1800000-2800000 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Intelebee LLC) (*Note: This is a requirement for one of Uplers' client - Intelebee LLC) What do you need for this opportunity? Must have skills required: Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory Intelebee LLC is Looking for: Data Engineer:We are seeking a skilled and hands-on Cloud Data Engineer with 5-8 years of experience to drive end-to-end data engineering solutions. The ideal candidate will have a deep understanding of dimensional modeling, data warehousing (DW), Lakehouse architecture, and the Medallion architecture. This role will focus on leveraging Azure's/AWS ecosystem to build scalable, efficient, and secure data solutions. You will work closely with customers to understand requirements, create technical specifications, and deliver solutions that scale across both on-premise and cloud environments. Key Responsibilities: End-to-End Data Engineering Lead the design and development of data pipelines for large-scale data processing, utilizing Azure/AWS tools such as Azure Data Factory, Azure Synapse, Azure functions, Logic Apps , Azure Databricks, and Data Lake Storage. Tools, AWS Lambda, AWS Glue Develop and implement dimensional modeling techniques and data warehousing solutions for effective data analysis and reporting. Build and maintain Lakehouse and Medallion architecture solutions for streamlined, high-performance data processing. Implement and manage Data Lakes on Azure/AWS, ensuring that data storage and processing is both scalable and secure. Handle large-scale databases (both on-prem and cloud) ensuring high availability, security, and performance. Design and enforce data governance policies for data security, privacy, and compliance within the Azure ecosystem. 5 How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory",,,Data Analysis,
4173687324,Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Skill: Data Engineer Role: T2, T1 Key Responsibility Data Engineer Must have 9+ years of experience in below mentioned skills. Must Have: Big Data Concepts , Python(Core Python- Able to write code), SQL, Shell Scripting, AWS S3 Good to Have: Event-driven/AWA SQS, Microservices, API Development, Kafka, Kubernetes, Argo, Amazon Redshift, Amazon Aurora Desired Skills and Experience Amazon Redshift",,,"Python, SQL",
4230137412,Data Research - Database Engineer,Forbes Advisor,"Mumbai, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Responsibilities Job Description Design, develop, and maintain the database infrastructure to store and manage company data efficiently and securely. Work with databases of varying scales, including small-scale databases, and databases involving big data processing. Work on data security and compliance, by implementing access controls, encryption, and compliance standards. Collaborate with cross-functional teams to understand data requirements and support the design of the database architecture. Migrate data from spreadsheets or other sources to a relational database system (e.g., PostgreSQL, MySQL) or cloud-based solutions like Google BigQuery. Develop import workflows and scripts to automate the data import process and ensure data accuracy and consistency. Optimize database performance by analyzing query execution plans, implementing indexing strategies, and improving data retrieval and storage mechanisms. Work with the team to ensure data integrity and enforce data quality standards, including data validation rules, constraints, and referential integrity. Monitor database health and identify and resolve issues. Collaborate with the full-stack web developer in the team to support the implementation of efficient data access and retrieval mechanisms. Implement data security measures to protect sensitive information and comply with relevant regulations. Demonstrate creativity in problem-solving and contribute ideas for improving data engineering processes and workflows. Embrace a learning mindset, staying updated with emerging database technologies, tools, and best practices. Explore third-party technologies as alternatives to legacy approaches for efficient data pipelines. Familiarize yourself with tools and technologies used in the team's workflow, such as Knime for data integration and analysis. Use Python for tasks such as data manipulation, automation, and scripting. Collaborate with the Data Research Engineer to estimate development efforts and meet project deadlines. Assume accountability for achieving development milestones. Prioritize tasks to ensure timely delivery, in a fast-paced environment with rapidly changing priorities. Collaborate with and assist fellow members of the Data Research Engineering Team as required. Perform tasks with precision and build reliable systems. Leverage online resources effectively like StackOverflow, ChatGPT, Bard, etc., while considering their capabilities and limitations. Skills And Experience Bachelor's degree in Computer Science, Information Systems, or a related field is desirable but not essential. Experience with data warehousing concepts and tools (e.g., Snowflake, Redshift) to support advanced analytics and reporting, aligning with the team‚Äôs data presentation goals. Skills in working with APIs for data ingestion or connecting third-party systems, which could streamline data acquisition processes. Proficiency with tools like Prometheus, Grafana, or ELK Stack for real-time database monitoring and health checks beyond basic troubleshooting. Familiarity with continuous integration/continuous deployment (CI/CD) tools (e.g., Jenkins, GitHub Actions). Deeper expertise in cloud platforms (e.g., AWS Lambda, GCP Dataflow) for serverless data processing or orchestration. Knowledge of database development and administration concepts, especially with relational databases like PostgreSQL and MySQL. Knowledge of Python programming, including data manipulation, automation, and object-oriented programming (OOP), with experience in modules such as Pandas, SQLAlchemy, gspread, PyDrive, and PySpark. Knowledge of SQL and understanding of database design principles, normalization, and indexing. Knowledge of data migration, ETL (Extract, Transform, Load) processes, or integrating data from various sources. Knowledge of cloud-based databases, such as AWS RDS and Google BigQuery. Eagerness to develop import workflows and scripts to automate data import processes. Knowledge of data security best practices, including access controls, encryption, and compliance standards. Strong problem-solving and analytical skills with attention to detail. Creative and critical thinking. Strong willingness to learn and expand knowledge in data engineering. Familiarity with Agile development methodologies is a plus. Experience with version control systems, such as Git, for collaborative development. Ability to thrive in a fast-paced environment with rapidly changing priorities. Ability to work collaboratively in a team environment. Good and effective communication skills. Comfortable with autonomy and ability to work independently. Qualifications 5+ Years exp in Database Engineering. Additional Information Perks Day off on the 3rd Friday of every month (one long weekend each month) Monthly Wellness Reimbursement Program to promote health well-being Monthly Office Commutation Reimbursement Program Paid paternity and maternity leaves",,,"Python, SQL",
4236550647,Infra - Open source Data Base Engineer,Zoetis,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job POSITION SUMMARY Zoetis, Inc. is the world's largest producer of medicine and vaccinations for pets and livestock. The Zoetis Tech & Digital (ZTD) Global Infrastructure organization is as a key building block of ZTD, comprising of enterprise applications and systems platforms. Join us at Zoetis India Capability Center (ZICC) in Hyderabad, where innovation meets excellence. As part of the world's leading animal healthcare company, ZICC is at the forefront of driving transformative advancements and applying technology to solve the most complex problems. Our mission is to ensure sustainable growth and maintain a competitive edge for Zoetis globally by leveraging the exceptional talent in India. At ZICC, you'll be part of a dynamic team that partners with colleagues worldwide, embodying the true spirit of One Zoetis. Together, we ensure seamless integration and collaboration, fostering an environment where your contributions can make a real impact. Be a part of our journey to pioneer innovation and drive the future of animal healthcare. We are seeking a highly skilled and innovative Global Infrastructure Open-Source Database Engineer to join our team. This role focuses on designing, implementing, and managing open-source database solutions that support our global infrastructure. The ideal candidate will have deep expertise in open-source database technologies, such as PostgreSQL, MySQL, MariaDB, or similar platforms, and will play a key role in optimizing database performance, scalability, and reliability. This position requires collaboration with cross-functional teams to ensure the seamless integration of database systems into the organization's infrastructure while adhering to industry best practices. POSITION RESPONSIBILITIES Percent of Time Database Architecture and Design: * Design and implement scalable and secure open-source database solutions across global infrastructure systems. * Collaborate with application and infrastructure teams to ensure optimal database architecture that meets business requirements. * Create and maintain detailed compliance and technical documentation for database configurations, processes, and procedures. 30% Database Management and Optimization: * Manage the open-source database standard, configuration, and maintenance, and lifecycle solution to ensureing high availability and reliability. * Monitor database performance and design tuning strategies for MSP operations to optimize query execution and system efficiency. Lessons learned will be used to update future solutions and standards. 25% Automation and Scripting: * Develop scripts and automation tools to streamline database management tasks, including backups, monitoring, and recovery for operations use. * Implement Infrastructure-as-Code (IaC) practices for database deployment and management using tools like Terraform or Ansible. 25% Security and Compliance: * Ensure database systems comply with organizational security policies and industry regulations. * Implement encryption, access control, and auditing mechanisms to safeguard sensitive data residing on our platforms 10% Research and Innovation: * Stay updated on the latest developments in open-source database technologies and recommend innovative solutions. * Evaluate and implement new tools and frameworks to enhance database performance and scalability. 10% ORGANIZATIONAL RELATIONSHIPS * Application Teams * Security Team * Compliance Team * Global Infrastructure Teams * MSP Operations Teams EDUCATION AND EXPERIENCE Education: * * Bachelor's degree in Computer Science, Information Technology, or a related field. * * Relevant certifications such as PostgreSQL Certified Professional, MySQL Database Administrator, or equivalent are a plus. Experience: * Minimum of 5-7 years of experience in managing and engineering open-source database systems in a global environment. * Proven track record of designing and implementing scalable database solutions for high-performance applications. TECHNICAL SKILLS REQUIREMENTS * Expertise in open-source database technologies such as PostgreSQL, MySQL, MariaDB, or similar platforms. * Proficiency in database optimization techniques, indexing strategies, and query tuning. * Experience with cloud database services (AWS RDS & Azure Database for PostgreSQL/MySQL). * Familiarity withStrong knowledge of scripting languages (Python, Bash) and automation tools (Ansible, Terraform). * Familiarity with containerization and orchestration tools like Docker and Kubernetes. * Strong analytical and problem-solving skills with a focus on innovation. * Excellent communication and collaboration skills to work effectively in a global team environment. * Detail-oriented with a commitment to documentation and continuous improvement. PHYSICAL POSITION REQUIREMENTS This role requires working in a global environment, occasionally outside regular business hours to support infrastructure deployments or critical issues. 0-5% Travel may be required. Hybrid work - Tuesday to Tursday in offie. About Zoetis At Zoetis , our purpose is to nurture the world and humankind by advancing care for animals. As a Fortune 500 company and the world leader in animal health, we discover, develop, manufacture and commercialize vaccines, medicines, diagnostics and other technologies for companion animals and livestock. We know our people drive our success. Our award-winning culture, built around our Core Beliefs, focuses on our colleagues' careers, connection and support. We offer competitive healthcare and retirement savings benefits, along with an array of benefits, policies and programs to support employee well-being in every sense, from health and financial wellness to family and lifestyle resources. Global Job Applicant Privacy Notice",,,Python,
4187613495,"Data Engineer, Translation Services Data Analytics (TSDA)",Amazon,"Hyderabad, Telangana, India",,Full-time,,"About the job Description Translation Services Data and Analytics seeks a passionate Data Engineer to drive innovations in translation analytics space to create the data pipelines handling large volume data and help our customer's to analyze and understand Amazon Translation coverage across the languages.We support Translation Services in making data-driven decisions by providing easy access to data and self-serve analytics. We work closely with internal stakeholders and cross-functional teams to solve business problems through data by building data pipelines, develop automated reporting and dive deep into data to identify actionable root cause. Key job responsibilities Work closely with data scientists and business intelligence engineers to create robust data architectures and pipelines. Develop and manage scalable, automated, and fault-tolerant data solutions. Simplify and enhance the accessibility, clarity, and usability of large or complex datasets through the development of advanced ETL, BI dashboards and applications. Take ownership of the design, creation, and upkeep of metrics, reports, analyses, and dashboards to inform key business decisions. Navigate ambiguous environments by evaluating various options using both data-driven insights and business expertise. A day in the life Data Engineers focus on managing customer requests, maintaining operational excellence, and enhancing core data analytics infrastructure. You will be collaborating closely with both technical and non-technical teams to design and execute roadmaps for essential Translation Services metrics. If you are not sure that every qualification on the list above describes you exactly, we'd still love to hear from you! At Amazon, we value people with unique backgrounds, experiences, and skillsets. If you‚Äôre passionate about this role and want to make an impact on a global scale, please apply! Basic Qualifications 3+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with SQL Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Bachelor's degree Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - ADCI HYD 13 SEZ Job ID: A2929407",,,"Python, SQL",
4247044256,Risk Data Engineer/ Leads,EY,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job At EY, you‚Äôll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we‚Äôre counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. RCE - Risk Data Engineer/Leads Job Description: Our Technology team builds innovative digital solutions rapidly and at scale to deliver the next generation of Financial and Non- Financial services across the globe. The Position is a senior technical, hands-on delivery role, requiring the knowledge of data engineering, cloud infrastructure and platform engineering, platform operations and production support using ground-breaking cloud and big data technologies. The ideal candidate with 3-6 years of relevant experience, will possess strong technical skills, an eagerness to learn, a keen interest on 3 keys pillars that our team support i.e. Financial Crime, Financial Risk and Compliance technology transformation, the ability to work collaboratively in fast-paced environment, and an aptitude for picking up new tools and techniques on the job, building on existing skillsets as a foundation. Senior Data & BI Analyst Job Summary: The Senior Data & BI Analyst is a key contributor to the Enterprise Analytics Center of Excellence (COE), responsible for promoting and supporting self-service analytics across the firm using tools such as ThoughtSpot and Tableau. This role requires a strong technical foundation in data analytics and BI tools, combined with strategic consulting skills to guide users in developing scalable, high-quality analytics. The ideal candidate has a passion for data, an understanding of financial services, and the ability to collaborate across teams to enable data-driven decision-making. Responsibilities: Contribute to the buildout and ongoing evolution of the Enterprise Analytics COE, shaping best practices and governance standards. Act as a subject matter expert (SME) in enterprise lakehouse data, guiding end users in identifying and leveraging key data sources. Work closely with business users to understand their analytics needs, providing consultation on BI tool selection and data strategies. Lead training sessions for ThoughtSpot (and Tableau) users to enhance adoption and proficiency in self-service analytics. Perform hands-on data analysis against enterprise lakehouse data to uncover insights and demonstrate tool capabilities. Develop reusable analytics templates, dashboards, and frameworks that drive consistency and efficiency across the organization. Support initial dashboard and analytics development efforts, assisting teams with prototyping and best practices implementation. Stay up to date on emerging BI trends, ThoughtSpot and Tableau advancements, and AI-driven analytics approaches to drive innovation. Required Qualifications: Experience working as a Technical Data Analyst, BI Analyst, or Business Analyst in an enterprise environment. Strong SQL skills and experience handling large datasets. Proficiency in data warehousing concepts, ETL processes, and BI reporting tools such as Tableau, Power BI, Qlik, ThoughtSpot. Excellent analytical and problem-solving skills with the ability to collaborate across cross-functional teams. Strong stakeholder management abilities, with the ability to effectively communicate technical insights to non-technical audiences. Preferred Qualifications: Hands-on experience with ThoughtSpot BI Platform. Financial services experience and understanding of financial data structures. Familiarity with cloud-based BI platforms (AWS, Azure, etc.). Knowledge of Natural Language Processing (NLP), AI-driven data aggregation, and automated reporting technologies. EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",,,"SQL, Tableau, Power BI, Data Analysis",
4237424143,Data Engineer( AWS and Snowflake)-Associate-aaS - Operate,PwC Acceleration Centers in India,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job At PwC, our people in managed services focus on a variety of outsourced solutions and support clients across numerous functions. These individuals help organisations streamline their operations, reduce costs, and improve efficiency by managing key processes and functions on their behalf. They are skilled in project management, technology, and process optimization to deliver high-quality services to clients. Those in managed service management and strategy at PwC will focus on transitioning and running services, along with managing delivery teams, programmes, commercials, performance and delivery risk. Your work will involve the process of continuous improvement and optimising of the managed services process, tools and services. Driven by curiosity, you are a reliable, contributing member of a team. In our fast-paced environment, you are expected to adapt to working with a variety of clients and team members, each presenting varying challenges and scope. Every experience is an opportunity to learn and grow. You are expected to take ownership and consistently deliver quality work that drives value for our clients and success as a team. As you navigate through the Firm, you build a brand for yourself, opening doors to more opportunities. Skills Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to: Apply a learning mindset and take ownership for your own development. Appreciate diverse perspectives, needs, and feelings of others. Adopt habits to sustain high performance and develop your potential. Actively listen, ask questions to check understanding, and clearly express ideas. Seek, reflect, act on, and give feedback. Gather information from a range of sources to analyse facts and discern patterns. Commit to understanding how the business works and building commercial awareness. Learn and apply professional and technical standards (e.g. refer to specific PwC tax and audit guidance), uphold the Firm's code of conduct and independence requirements. Role: Associate Tower: Data, Analytics & Specialist Managed Service Experience: 2.0 - 5.5 years Key Skills: AWS Educational Qualification: BE / B Tech / ME / M Tech / MBA Work Location: India.;l Job Description As a Associate, you will work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to: Use feedback and reflection to develop self-awareness, personal strengths, and address development areas. Flexible to work in stretch opportunities/assignments. Demonstrate critical thinking and the ability to bring order to unstructured problems. Ticket Quality and deliverables review, Status Reporting for the project. Adherence to SLAs, experience in incident management, change management and problem management. Seek and embrace opportunities which give exposure to different situations, environments, and perspectives. Use straightforward communication, in a structured way, when influencing and connecting with others. Able to read situations and modify behavior to build quality relationships. Uphold the firm's code of ethics and business conduct. Demonstrate leadership capabilities by working, with clients directly and leading the engagement. Work in a team environment that includes client interactions, workstream management, and cross-team collaboration. Good team player, take up cross competency work and contribute to COE activities. Escalation/Risk management. Position Requirements Required Skills: AWS Cloud Engineer Job description: Candidate is expected to demonstrate extensive knowledge and/or a proven record of success in the following areas: Should have minimum 2 years hand on experience building advanced Data warehousing solutions on leading cloud platforms. Should have minimum 1-3 years of Operate/Managed Services/Production Support Experience Should have extensive experience in developing scalable, repeatable, and secure data structures and pipelines to ingest, store, collect, standardize, and integrate data that for downstream consumption like Business Intelligence systems, Analytics modelling, Data scientists etc. Designing and implementing data pipelines to extract, transform, and load (ETL) data from various sources into data storage systems, such as data warehouses or data lakes. Should have experience in building efficient, ETL/ELT processes using industry leading tools like AWS, AWS GLUE, AWS Lambda, AWS DMS, PySpark, SQL, Python, DBT, Prefect, Snoflake, etc. Design, implement, and maintain data pipelines for data ingestion, processing, and transformation in AWS. Work together with data scientists and analysts to understand the needs for data and create effective data workflows. Implementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data. Improve the scalability, efficiency, and cost-effectiveness of data pipelines. Monitoring and troubleshooting data pipelines and resolving issues related to data processing, transformation, or storage. Implementing and maintaining data security and privacy measures, including access controls and encryption, to protect sensitive data Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases Should have experience in Building and maintaining Data Governance solutions (Data Quality, Metadata management, Lineage, Master Data Management and Data security) using industry leading tools Scaling and optimizing schema and performance tuning SQL and ETL pipelines in data lake and data warehouse environments. Should have Hands-on experience with Data analytics tools like Informatica, Collibra, Hadoop, Spark, Snowflake etc. Should have Experience of ITIL processes like Incident management, Problem Management, Knowledge management, Release management, Data DevOps etc. Should have Strong communication, problem solving, quantitative and analytical abilities. Nice To Have AWS certification Managed Services- Data, Analytics & Insights Managed Service At PwC we relentlessly focus on working with our clients to bring the power of technology and humans together and create simple, yet powerful solutions. We imagine a day when our clients can simply focus on their business knowing that they have a trusted partner for their IT needs. Every day we are motivated and passionate about making our clients‚Äô better. Within our Managed Services platform, PwC delivers integrated services and solutions that are grounded in deep industry experience and powered by the talent that you would expect from the PwC brand. The PwC Managed Services platform delivers scalable solutions that add greater value to our client‚Äôs enterprise through technology and human-enabled experiences. Our team of highly skilled and trained global professionals, combined with the use of the latest advancements in technology and process, allows us to provide effective and efficient outcomes. With PwC‚Äôs Managed Services our clients are able to focus on accelerating their priorities, including optimizing operations and accelerating outcomes. PwC brings a consultative first approach to operations, leveraging our deep industry insights combined with world class talent and assets to enable transformational journeys that drive sustained client outcomes. Our clients need flexible access to world class business and technology capabilities that keep pace with today‚Äôs dynamic business environment. Within our global, Managed Services platform, we provide Data, Analytics & Insights where we focus more so on the evolution of our clients‚Äô Data and Analytics ecosystem. Our focus is to empower our clients to navigate and capture the value of their Data & Analytics portfolio while cost-effectively operating and protecting their solutions. We do this so that our clients can focus on what matters most to your business: accelerating growth that is dynamic, efficient and cost-effective. As a member of our Data, Analytics & Insights Managed Service team, we are looking for candidates who thrive working in a high-paced work environment capable of working on a mix of critical Data, Analytics & Insights offerings and engagement including help desk support, enhancement, and optimization work, as well as strategic roadmap and advisory level work. It will also be key to lend experience and effort in helping win and support customer engagements from not only a technical perspective, but also a relationship perspective.",Associate,,"Python, SQL",
4096051269,(Senior) Data Engineer,Aptiv,"Pune, Maharashtra, India",,Full-time,,"About the job Help shape the future of mobility. At Aptiv, we couldn‚Äôt solve mobility‚Äôs toughest challenges without our Corporate team. They ensure operations run smoothly by supporting more than 200,000 Aptiv employees and providing the direction and guidance needed as we strive to make the world safer, greener and more connected. IT Data Analytics is a diverse DevOps team of technology enthusiasts enabling our global business. Aptiv has embarked on a Data strategy that focuses on establishing a strong technology team, enterprise data management & cloud-based business solutions. Our team is charged with catalyzing value creation in the most critical areas of Aptiv‚Äôs value chain, touching our business by understanding customer demand, manufacturing implications, and our supply base. As a Data Engineer, you will design, develop and implement a cost-effective, scalable, reusable and secured Ingestion framework. You will take advantage of the opportunity to work with business leaders, various stakeholders, and source system SME‚Äôs to understand and define the business needs, translate to technical specifications, and ingest data into Google cloud platform, BigQuery. You will design and implement processes for data ingestion, transformation, storage, analysis, modelling, reporting, monitoring, availability, governance and security of high volumes of structured and unstructured data Want to join us? Your Role Pipeline Design & Implementation: Develop and deploy high-throughput data pipelines using the latest GCP technologies. Subject Matter Expertise: Serve as a specialist in data engineering and Google Cloud Platform (GCP) data technologies. Client Communication: Leverage your GCP data engineering experience to engage with clients, understand their requirements, and translate these into technical data solutions. Technical Translation: Analyze business requirements and convert them into technical specifications. Create source-to-target mappings, enhance ingestion frameworks to incorporate internal and external data sources, and transform data according to business rules. Data Cataloging: Develop capabilities to support enterprise-wide data cataloging. Security & Privacy: Design data solutions with a focus on security and privacy. Agile & DataOps: Utilize Agile and DataOps methodologies and implementation strategies in project delivery. Your Background Bachelor‚Äôs or Master‚Äôs degree in any one of the disciplines: Computer Science, Data & Analytics or similar relevant subjects. 4+ yrs years of hands-on IT experience in a similar role. Proven expertise in SQL ‚Äì subqueries, aggregations, functions, triggers, Indexes, DB optimization, creating/understanding relational data-based models. Deep experience working with Google Data Products (e.g. BigQuery, Dataproc, Dataplex, Looker, Cloud data fusion, Data Catalog, Dataflow, Cloud composer, Analytics Hub, Pub/Sub, Dataprep, Cloud Bigtable, Cloud SQL, Cloud IAM, Google Kubernetes engine, AutoML). Experience in Qlik replicate , Spark (Scala/Python/Java) and Kafka. Excellent written and verbal skills to communicate technical solutions to business teams. Understanding trends, new concepts, industry standards and new technologies in Data and Analytics space. Ability to work with globally distributed teams. Knowledge of Statistical methods and data modelling knowledge. Working knowledge in designing and creating Tableau/Qlik/Power BI dashboards, Alteryx and Informatica Data Quality. Why join us? You can grow at Aptiv. Aptiv provides an inclusive work environment where all individuals can grow and develop, regardless of gender, ethnicity or beliefs. You can have an impact. Safety is a core Aptiv value; we want a safer world for us and our children, one with: Zero fatalities, Zero injuries, Zero accidents. You have support. We ensure you have the resources and support you need to take care of your family and your physical and mental health with a competitive health insurance package. Your Benefits At Aptiv Benefits/Perks: Personal holidays, Healthcare, Pension, Tax saver scheme, Free Onsite Breakfast, Discounted Corporate Gym Membership. Multicultural environment Learning, professional growth and development in a world-recognized international environment. Access to internal & external training, coaching & certifications. Recognition for innovation and excellence. Access to transportation: Grand Canal Dock is well-connected to public transportation, including DART trains, buses, and bike-sharing services, making it easy to get to and from the area. # Privacy Notice - Active Candidates: https://www.aptiv.com/privacy-notice-active-candidates Aptiv is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, gender identity, sexual orientation, disability status, protected veteran status or any other characteristic protected by law.",,,"Python, SQL, Tableau, Power BI",
4235453005,Sr GCP Data Engineer,LTIMindtree,"Noida, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Experience: 5-12 years Primary Skills: GCP, Big query, Python Notice period: Immediate to 30 days Overall 5+ years of experience in data eng Strong Experience with GCP services like Big Query Cloud Storage composer etc Hands on experience in Python Proficient in Pyspark databricks Strong understanding of data warehouse concepts ETL process and data modelling GCP certifications are added advantage",,,Python,
4153229654,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you will work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. A career in IBM Consulting embraces long-term relationships and close collaboration with clients across the globe. You will collaborate with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio, including IBM Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you will be supported by mentors and coaches who will encourage you to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground-breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and learning opportunities in an environment that embraces your unique skills and experience. Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results Your Primary Responsibilities Include Develop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools. Liaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT. Work with data scientist and business analytics team to assist in data ingestion and data-related technical issues. Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter Knowledge of Cloud, Power BI, Data migration on cloud skills. Experience in Unix shell scripting and python Experience with relational SQL, Big Data etc Preferred Technical And Professional Experience Knowledge of MS-Azure Cloud Experience in Informatica PowerCenter Experience in Unix shell scripting and python",,,"Python, SQL, Power BI, Machine Learning",
4247662449,Data Engineer,Lenskart.com,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Job Title: Data Engineer (1‚Äì4 Years Experience) Location: Bangalore About the Role We‚Äôre looking for a hands-on Data Engineer to help us scale our data infrastructure and platforms. In this role, you‚Äôll work closely with engineering, analytics, and product teams to build reliable data pipelines and deliver high-quality datasets for analytics and reporting. If you're passionate about cloud data engineering, writing efficient code in Python, and working with technologies like BigQuery and GCP, this is the perfect role for you. Key Responsibilities ‚óè Build and maintain scalable ETL/ELT data pipelines using Python and cloud-native tools. ‚óè Design and optimize data models and queries on Google BigQuery for analytical workloads. ‚óè Develop, schedule, and monitor workflows using orchestration tools like Apache Airflow or Cloud Composer. ‚óè Ingest and integrate data from multiple structured and semi-structured sources, including MySQL, MongoDB, APIs, and cloud storage. ‚óè Ensure data integrity, security, and quality through validation, logging, and monitoring systems. ‚óè Collaborate with analysts and data consumers to understand requirements and deliver clean, usable datasets. ‚óè Implement data governance, lineage tracking, and documentation as part of platform hygiene. Must-Have Skills ‚óè 1‚Äì4 years of experience in data engineering or backend development. ‚óè Strong experience with Google BigQuery and GCP (Google Cloud Platform). ‚óè Proficiency in Python for scripting, automation, and data manipulation. ‚óè Solid understanding of SQL and experience with relational databases like MySQL. ‚óè Experience working with MongoDB and semi-structured data (e.g., JSON, nested formats). ‚óè Exposure to data warehousing, data modeling, and performance tuning. ‚óè Familiarity with Git-based version control and CI/CD practices.",,4 Years Experience,"Python, SQL",
4247662449,Data Engineer,Lenskart.com,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Job Title: Data Engineer (1‚Äì4 Years Experience) Location: Bangalore About the Role We‚Äôre looking for a hands-on Data Engineer to help us scale our data infrastructure and platforms. In this role, you‚Äôll work closely with engineering, analytics, and product teams to build reliable data pipelines and deliver high-quality datasets for analytics and reporting. If you're passionate about cloud data engineering, writing efficient code in Python, and working with technologies like BigQuery and GCP, this is the perfect role for you. Key Responsibilities ‚óè Build and maintain scalable ETL/ELT data pipelines using Python and cloud-native tools. ‚óè Design and optimize data models and queries on Google BigQuery for analytical workloads. ‚óè Develop, schedule, and monitor workflows using orchestration tools like Apache Airflow or Cloud Composer. ‚óè Ingest and integrate data from multiple structured and semi-structured sources, including MySQL, MongoDB, APIs, and cloud storage. ‚óè Ensure data integrity, security, and quality through validation, logging, and monitoring systems. ‚óè Collaborate with analysts and data consumers to understand requirements and deliver clean, usable datasets. ‚óè Implement data governance, lineage tracking, and documentation as part of platform hygiene. Must-Have Skills ‚óè 1‚Äì4 years of experience in data engineering or backend development. ‚óè Strong experience with Google BigQuery and GCP (Google Cloud Platform). ‚óè Proficiency in Python for scripting, automation, and data manipulation. ‚óè Solid understanding of SQL and experience with relational databases like MySQL. ‚óè Experience working with MongoDB and semi-structured data (e.g., JSON, nested formats). ‚óè Exposure to data warehousing, data modeling, and performance tuning. ‚óè Familiarity with Git-based version control and CI/CD practices.",,4 Years Experience,"Python, SQL",
4231104322,AWS Data Engineer,Tata Consultancy Services,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Greetings from TCS!! Job Title: AWS Data Engineer Job location: Bangalore Walk in Interview : 24-May-25 Experience Range: 4+Years Minimum Qualification: 15 years of full-time education Job Description: Strong hands-on experience in Python programming and PySpark Experience using AWS services (RedShift, Glue, EMR, S3 & Lambda) Experience working with Apache Spark and Hadoop ecosystem. Experience in writing and optimizing SQL for data manipulations. Good Exposure to scheduling tools. Airflow is preferable. Must ‚Äì Have Data Warehouse Experience with AWS Redshift or Hive Experience in implementing security measures for data protection. Expertise to build/test complex data pipelines for ETL processes (batch and near real time) Readable documentation of all the components being developed. Knowledge of Database technologies for OLTP and OLAP workloads",,,"Python, SQL",
4231104322,AWS Data Engineer,Tata Consultancy Services,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Greetings from TCS!! Job Title: AWS Data Engineer Job location: Bangalore Walk in Interview : 24-May-25 Experience Range: 4+Years Minimum Qualification: 15 years of full-time education Job Description: Strong hands-on experience in Python programming and PySpark Experience using AWS services (RedShift, Glue, EMR, S3 & Lambda) Experience working with Apache Spark and Hadoop ecosystem. Experience in writing and optimizing SQL for data manipulations. Good Exposure to scheduling tools. Airflow is preferable. Must ‚Äì Have Data Warehouse Experience with AWS Redshift or Hive Experience in implementing security measures for data protection. Expertise to build/test complex data pipelines for ETL processes (batch and near real time) Readable documentation of all the components being developed. Knowledge of Database technologies for OLTP and OLAP workloads",,,"Python, SQL",
4244675713,Data Platform Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job We are looking for a PySpark solutions developer and data engineer who can design and build solutions for one of our Fortune 500 Client programs, which aims towards building a data standardized and curation needs on Hadoop cluster. This is high visibility, fast-paced key initiative will integrate data across internal and external sources, provide analytical insights, and integrate with the customers critical systems. Key Responsibilities Ability to design, build and unit test applications on Spark framework on Python. Build PySpark based applications for both batch and streaming requirements, which will require in-depth knowledge on majority of Hadoop and NoSQL databases as well. Develop and execute data pipeline testing processes and validate business rules and policies. Optimize performance of the built Spark applications in Hadoop using configurations around Spark Context, Spark-SQL, Data Frame, and Pair RDD's. Optimize performance for data access requirements by choosing the appropriate native Hadoop file formats (Avro, Parquet, ORC etc) and compression codec respectively. Build integrated solutions leveraging Unix shell scripting, RDBMS, Hive, HDFS File System, HDFS File Types, HDFS compression codec. Build data tokenization libraries and integrate with Hive & Spark for column-level obfuscation. Experience in processing large amounts of structured and unstructured data, including integrating data from multiple sources. Create and maintain integration and regression testing framework on Jenkins integrated with Bit Bucket and/or GIT repositories. Participate in the agile development process, and document and communicate issues and bugs relative to data standards in scrum meetings. Work collaboratively with onsite and offshore team. Develop & review technical documentation for artifacts delivered. Ability to solve complex data-driven scenarios and triage towards defects and production issues. Ability to learn-unlearn-relearn concepts with an open and analytical mindset. Participate in code release and production deployment. Challenge and inspire team members to achieve business results in a fast paced and quickly changing environment. Preferred Qualifications BE/B.Tech/ B.Sc. in Computer Science/ Statistics from an accredited college or university. Minimum 3 years of extensive experience in design, build and deployment of PySpark-based applications. Expertise in handling complex large-scale Big Data environments preferably (20Tb+). Minimum 3 years of experience in the following: HIVE, YARN, HDFS. Hands-on experience writing complex SQL queries, exporting, and importing large amounts of data using utilities. Ability to build abstracted, modularized reusable code components. Prior experience on ETL tools preferably Informatica PowerCenter is advantageous. Able to quickly adapt and learn. Able to jump into an ambiguous situation and take the lead on resolution. Able to communicate and coordinate across various teams. Are comfortable tackling new challenges and new ways of working Are ready to move from traditional methods and adapt into agile ones Comfortable challenging your peers and leadership team. Can prove yourself quickly and decisively. Excellent communication skills and Good Customer Centricity. Strong Target & High Solution Orientation. Desired Skills and Experience Python",,,"Python, SQL",
4244675713,Data Platform Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job We are looking for a PySpark solutions developer and data engineer who can design and build solutions for one of our Fortune 500 Client programs, which aims towards building a data standardized and curation needs on Hadoop cluster. This is high visibility, fast-paced key initiative will integrate data across internal and external sources, provide analytical insights, and integrate with the customers critical systems. Key Responsibilities Ability to design, build and unit test applications on Spark framework on Python. Build PySpark based applications for both batch and streaming requirements, which will require in-depth knowledge on majority of Hadoop and NoSQL databases as well. Develop and execute data pipeline testing processes and validate business rules and policies. Optimize performance of the built Spark applications in Hadoop using configurations around Spark Context, Spark-SQL, Data Frame, and Pair RDD's. Optimize performance for data access requirements by choosing the appropriate native Hadoop file formats (Avro, Parquet, ORC etc) and compression codec respectively. Build integrated solutions leveraging Unix shell scripting, RDBMS, Hive, HDFS File System, HDFS File Types, HDFS compression codec. Build data tokenization libraries and integrate with Hive & Spark for column-level obfuscation. Experience in processing large amounts of structured and unstructured data, including integrating data from multiple sources. Create and maintain integration and regression testing framework on Jenkins integrated with Bit Bucket and/or GIT repositories. Participate in the agile development process, and document and communicate issues and bugs relative to data standards in scrum meetings. Work collaboratively with onsite and offshore team. Develop & review technical documentation for artifacts delivered. Ability to solve complex data-driven scenarios and triage towards defects and production issues. Ability to learn-unlearn-relearn concepts with an open and analytical mindset. Participate in code release and production deployment. Challenge and inspire team members to achieve business results in a fast paced and quickly changing environment. Preferred Qualifications BE/B.Tech/ B.Sc. in Computer Science/ Statistics from an accredited college or university. Minimum 3 years of extensive experience in design, build and deployment of PySpark-based applications. Expertise in handling complex large-scale Big Data environments preferably (20Tb+). Minimum 3 years of experience in the following: HIVE, YARN, HDFS. Hands-on experience writing complex SQL queries, exporting, and importing large amounts of data using utilities. Ability to build abstracted, modularized reusable code components. Prior experience on ETL tools preferably Informatica PowerCenter is advantageous. Able to quickly adapt and learn. Able to jump into an ambiguous situation and take the lead on resolution. Able to communicate and coordinate across various teams. Are comfortable tackling new challenges and new ways of working Are ready to move from traditional methods and adapt into agile ones Comfortable challenging your peers and leadership team. Can prove yourself quickly and decisively. Excellent communication skills and Good Customer Centricity. Strong Target & High Solution Orientation. Desired Skills and Experience Python",,,"Python, SQL",
4235369012,ML Engineer,Uplers,"Amritsar, Punjab, India (Remote)",Remote,‚Çπ1M/yr - ‚Çπ1.2M/yr,,"About the job Experience : Fresher Salary : INR 1000000-1200000 / year (based on experience) Expected Notice Period : 30 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Leegality) (*Note: This is a requirement for one of Uplers' client - Leegality) What do you need for this opportunity? Must have skills required: Start-up Experience, Python, Ml frameworks, Statistical analysis, CNNs, Machine Learning, Computer Vision, AI models Leegality is Looking for: What You'll Do As an ML Engineer, you will develop solutions to interesting technical problems, explore exciting growth opportunities, and have a real impact on our product. To ensure success as a machine learning engineer, you should demonstrate solid data science knowledge and experience in a related ML role. A first-class machine learning engineer will be someone whose expertise translates into the enhanced performance of predictive automation software. Responsibilities: Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models. Transforming data science prototypes and applying appropriate ML algorithms and tools. Solving complex problems with multi-layered data sets and optimizing existing machine learning libraries and frameworks. Developing ML algorithms to analyze huge volumes of historical data to make predictions. Running tests, performing statistical analysis, and interpreting test results. Exposure to generative AI (LLM) based ML architectures Experienced in working with CNN. Knows how to train and fine-tune CNN-based models. Documenting machine learning processes. Requirements: 1+ years of relevant experience in Machine Learning Engineering. Advanced proficiency with Python. Extensive knowledge of ML frameworks, libraries, data structures, data modeling, and software architecture. Superb analytical and problem-solving abilities. Great communication and collaboration skills. Preferences: Start-up experience is good to have. Willing to join immediately, ideally. Female candidates are preferred Interview Process - Technical Round 1 - ML Fundamentals, NLP models Assessment Technical Round 2 - Discussion on the assignment submitted & modify it according to the requirements Round 3 - with the Tech Lead - Tech Managerial - Past projects & in-depth knowledge of ML How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Start-up Experience, Python, Ml frameworks, Statistical analysis, CNNs, Machine Learning, Computer Vision, AI models",Manager,,"Python, Machine Learning",
4235369012,ML Engineer,Uplers,"Amritsar, Punjab, India (Remote)",Remote,‚Çπ1M/yr - ‚Çπ1.2M/yr,,"About the job Experience : Fresher Salary : INR 1000000-1200000 / year (based on experience) Expected Notice Period : 30 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Leegality) (*Note: This is a requirement for one of Uplers' client - Leegality) What do you need for this opportunity? Must have skills required: Start-up Experience, Python, Ml frameworks, Statistical analysis, CNNs, Machine Learning, Computer Vision, AI models Leegality is Looking for: What You'll Do As an ML Engineer, you will develop solutions to interesting technical problems, explore exciting growth opportunities, and have a real impact on our product. To ensure success as a machine learning engineer, you should demonstrate solid data science knowledge and experience in a related ML role. A first-class machine learning engineer will be someone whose expertise translates into the enhanced performance of predictive automation software. Responsibilities: Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models. Transforming data science prototypes and applying appropriate ML algorithms and tools. Solving complex problems with multi-layered data sets and optimizing existing machine learning libraries and frameworks. Developing ML algorithms to analyze huge volumes of historical data to make predictions. Running tests, performing statistical analysis, and interpreting test results. Exposure to generative AI (LLM) based ML architectures Experienced in working with CNN. Knows how to train and fine-tune CNN-based models. Documenting machine learning processes. Requirements: 1+ years of relevant experience in Machine Learning Engineering. Advanced proficiency with Python. Extensive knowledge of ML frameworks, libraries, data structures, data modeling, and software architecture. Superb analytical and problem-solving abilities. Great communication and collaboration skills. Preferences: Start-up experience is good to have. Willing to join immediately, ideally. Female candidates are preferred Interview Process - Technical Round 1 - ML Fundamentals, NLP models Assessment Technical Round 2 - Discussion on the assignment submitted & modify it according to the requirements Round 3 - with the Tech Lead - Tech Managerial - Past projects & in-depth knowledge of ML How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Start-up Experience, Python, Ml frameworks, Statistical analysis, CNNs, Machine Learning, Computer Vision, AI models",Manager,,"Python, Machine Learning",
4248133528,"Senior/Staff Data Engineer (Fintech team) (Bangkok based, relocation provided)",Agoda,"Lucknow, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job About Agoda Agoda is an online travel booking platform for accommodations, flights, and more. We build and deploy cutting-edge technology that connects travelers with a global network of 4.7M hotels and holiday properties worldwide, plus flights, activities, and more . Based in Asia and part of Booking Holdings, our 7,100+ employees representing 95+ nationalities in 27 markets foster a work environment rich in diversity, creativity, and collaboration. We innovate through a culture of experimentation and ownership,‚ÄØenhancing the ability for our customers to experience the world. Our Purpose ‚Äì Bridging the World Through Travel We believe travel allows people to enjoy, learn and experience more of the amazing world we live in. It brings individuals and cultures closer together, fostering empathy, understanding and happiness. We are a skillful, driven and diverse team from across the globe, united by a passion to make an impact. Harnessing our innovative technologies and strong partnerships, we aim to make travel easy and rewarding for everyone. About Agoda Agoda is an online travel booking platform for accommodation, flights, and more. We build and deploy cutting edge technology that connects travelers with more than 2.5 million accommodations globally. Based in Asia and part of Booking Holdings, our 4,000+ talents coming from 90+ different nationalities foster a work environment rich in diversity, creativity, and collaboration. We innovate through a culture of experimentation and ownership, enabling our customers to experience the world Get to Know Our Team Fintech is a rapidly expanding industry with endless opportunities for growth and innovation. At Agoda, we are proud to be at the forefront of this exciting field, working closely with our finance business team and product owners to reduce risk, increase efficiency, and seize new market opportunities. Our fintech projects are diverse and varied, ranging from traditional finance to cutting-edge customer-facing solutions. Whether using big data technologies for reconciliation, expanding and enhancing payment options for our customers, or developing lightning-fast tax calculation systems, we are constantly pushing the boundaries of what‚Äôs possible in fintech. With a talented team of data and backend engineers, we are well-equipped to tackle any challenge that comes our way. Agoda is the perfect place for you if you‚Äôre passionate about fintech and looking to make a real impact in this dynamic industry. The Opportunity We are seeking highly skilled engineers with a range of experience in fintech to join our team. Whether you are a seasoned expert or just starting out in the field, we welcome your application. We are looking for intelligent and agile engineers with strong attention to detail and the ability to work on both back-end and data engineering tasks. If you have a passion for fintech technology and are excited about the opportunity to build and innovate, we would love to hear from you. We value a wide range of experience and backgrounds and encourage all qualified candidates to apply. In this Role, you will get to Think and own the full life cycle of our products, not just a single piece of code ‚Äì from business requirements, technology selection, coding standards, agile development, unit and application testing, to CI/CD and proper monitoring Design, develop and maintain platforms and data pipelines across fintech Improve scalability, stability, and efficiency of our existing systems Write great code and help others write great code ‚Äì mentor people in your team and the wider organisation Collaborate with other teams and departments Help us hire extraordinary talent such as yourself! What You‚Äôll Need To Succeed Minimum 7 years of experience under your belt developing performance-critical applications that run in a production environment using Scala, Java, C# or Kotlin Experience with data tooling: Spark, Kafka, Workflow Orchestration Tools Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases Experience building and optimizing ‚Äòbig data‚Äô data pipelines, architectures and data sets. In depth knowledge of Model and Design of DB schemas for read and write performance Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Build processes supporting data transformation, data structures, metadata, dependency and workload management Experience supporting and working with Scrum in Agile Cross-functional teams Excellent verbal and written English communication skills It‚Äôs great if you have Deep experience with spark based distributed data pipelines. Able to deep dive and solve challenges in spark query plans and optimize code. Experience in Spark streaming is a plus Strong experience building finance stack applications including ledgers, revenue recognition, monetary transfers and reconciliations, financial accounting platforms etc Experience handling financial data risk management and data governance projects. Experience supporting internal and external financial audits Experience with data profiling, data lineage, data cataloging to enhance data governance and documentation Experience in leading projects, initiatives and/or teams, with full ownership of the systems involved Experience with building data pipelines to integrate 3rd parties is plus #telaviv #jerusalem #IT #ENG #4 #sanfrancisco #sanjose #losangeles #sandiego #oakland #denver #miami #orlando #atlanta #chicago #boston #detroit #newyork #portland #philadelphia #dallas #houston #austin #seattle #sydney #melbourne #perth #toronto #vancouver #montreal #shanghai #beijing #shenzhen #prague #Brno #Ostrava #cairo #alexandria #giza #estonia #paris #berlin #munich #hamburg #stuttgart #cologne #frankfurt #hongkong #budapest #jakarta #bali #dublin #telaviv #milan #rome #venice #florence #naples #turin #palermo #bologna #tokyo #osaka #kualalumpur #malta #amsterdam #oslo #manila #warsaw #krakow #doha #alrayyan #riyadh #jeddah #mecca #medina #singapore #seoul #barcelona #madrid #stockholm #zurich #taipei #tainan #taichung #kaohsiung #bangkok #Phuket #istanbul #london #manchester #edinburgh #hcmc #hanoi #lodz #wroclaw #poznan #katowice #rio #salvador #newdelhi #bangalore #bandung #yokohama #nagoya #okinawa #fukuoka #jerusalem #IT #4 #newdelhi #Pune #Hyderabad #Bangalore #Mumbai #Bengaluru #Chennai #Kolkata #Lucknow #sanfrancisco #sanjose #losangeles #sandiego #oakland #denver #miami #orlando #atlanta #chicago #boston #detroit #newyork #portland #philadelphia #dallas #houston #austin #seattle #sydney #melbourne #perth #toronto #vancouver #montreal #shanghai #beijing #shenzhen #prague #Brno #Ostrava #cairo #alexandria #giza #estonia #paris #berlin #munich #hamburg #stuttgart #cologne #frankfurt #hongkong #budapest #jakarta #bali #dublin #telaviv #milan #rome #tokyo #osaka #kualalumpur #amsterdam #oslo #manila #warsaw #krakow #bucharest #moscow #saintpetersburg #capetown #johannesburg #seoul #barcelona #madrid #stockholm #zurich #taipei #bangkok #Phuket #istanbul #london #manchester #edinburgh #kiev #hcmc #hanoi #wroclaw #poznan #katowice #rio #salvador #IT #4 #5 Equal Opportunity Employer At Agoda, we pride ourselves on being a company represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Agoda is based solely on a person‚Äôs merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details please read our privacy policy . Disclaimer We do not accept any terms or conditions, nor do we recognize any agency‚Äôs representation of a candidate, from unsolicited third-party or agency submissions. If we receive unsolicited or speculative CVs, we reserve the right to contact and hire the candidate directly without any obligation to pay a recruitment fee.",,,SQL,
4248133528,"Senior/Staff Data Engineer (Fintech team) (Bangkok based, relocation provided)",Agoda,"Lucknow, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job About Agoda Agoda is an online travel booking platform for accommodations, flights, and more. We build and deploy cutting-edge technology that connects travelers with a global network of 4.7M hotels and holiday properties worldwide, plus flights, activities, and more . Based in Asia and part of Booking Holdings, our 7,100+ employees representing 95+ nationalities in 27 markets foster a work environment rich in diversity, creativity, and collaboration. We innovate through a culture of experimentation and ownership,‚ÄØenhancing the ability for our customers to experience the world. Our Purpose ‚Äì Bridging the World Through Travel We believe travel allows people to enjoy, learn and experience more of the amazing world we live in. It brings individuals and cultures closer together, fostering empathy, understanding and happiness. We are a skillful, driven and diverse team from across the globe, united by a passion to make an impact. Harnessing our innovative technologies and strong partnerships, we aim to make travel easy and rewarding for everyone. About Agoda Agoda is an online travel booking platform for accommodation, flights, and more. We build and deploy cutting edge technology that connects travelers with more than 2.5 million accommodations globally. Based in Asia and part of Booking Holdings, our 4,000+ talents coming from 90+ different nationalities foster a work environment rich in diversity, creativity, and collaboration. We innovate through a culture of experimentation and ownership, enabling our customers to experience the world Get to Know Our Team Fintech is a rapidly expanding industry with endless opportunities for growth and innovation. At Agoda, we are proud to be at the forefront of this exciting field, working closely with our finance business team and product owners to reduce risk, increase efficiency, and seize new market opportunities. Our fintech projects are diverse and varied, ranging from traditional finance to cutting-edge customer-facing solutions. Whether using big data technologies for reconciliation, expanding and enhancing payment options for our customers, or developing lightning-fast tax calculation systems, we are constantly pushing the boundaries of what‚Äôs possible in fintech. With a talented team of data and backend engineers, we are well-equipped to tackle any challenge that comes our way. Agoda is the perfect place for you if you‚Äôre passionate about fintech and looking to make a real impact in this dynamic industry. The Opportunity We are seeking highly skilled engineers with a range of experience in fintech to join our team. Whether you are a seasoned expert or just starting out in the field, we welcome your application. We are looking for intelligent and agile engineers with strong attention to detail and the ability to work on both back-end and data engineering tasks. If you have a passion for fintech technology and are excited about the opportunity to build and innovate, we would love to hear from you. We value a wide range of experience and backgrounds and encourage all qualified candidates to apply. In this Role, you will get to Think and own the full life cycle of our products, not just a single piece of code ‚Äì from business requirements, technology selection, coding standards, agile development, unit and application testing, to CI/CD and proper monitoring Design, develop and maintain platforms and data pipelines across fintech Improve scalability, stability, and efficiency of our existing systems Write great code and help others write great code ‚Äì mentor people in your team and the wider organisation Collaborate with other teams and departments Help us hire extraordinary talent such as yourself! What You‚Äôll Need To Succeed Minimum 7 years of experience under your belt developing performance-critical applications that run in a production environment using Scala, Java, C# or Kotlin Experience with data tooling: Spark, Kafka, Workflow Orchestration Tools Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases Experience building and optimizing ‚Äòbig data‚Äô data pipelines, architectures and data sets. In depth knowledge of Model and Design of DB schemas for read and write performance Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Build processes supporting data transformation, data structures, metadata, dependency and workload management Experience supporting and working with Scrum in Agile Cross-functional teams Excellent verbal and written English communication skills It‚Äôs great if you have Deep experience with spark based distributed data pipelines. Able to deep dive and solve challenges in spark query plans and optimize code. Experience in Spark streaming is a plus Strong experience building finance stack applications including ledgers, revenue recognition, monetary transfers and reconciliations, financial accounting platforms etc Experience handling financial data risk management and data governance projects. Experience supporting internal and external financial audits Experience with data profiling, data lineage, data cataloging to enhance data governance and documentation Experience in leading projects, initiatives and/or teams, with full ownership of the systems involved Experience with building data pipelines to integrate 3rd parties is plus #telaviv #jerusalem #IT #ENG #4 #sanfrancisco #sanjose #losangeles #sandiego #oakland #denver #miami #orlando #atlanta #chicago #boston #detroit #newyork #portland #philadelphia #dallas #houston #austin #seattle #sydney #melbourne #perth #toronto #vancouver #montreal #shanghai #beijing #shenzhen #prague #Brno #Ostrava #cairo #alexandria #giza #estonia #paris #berlin #munich #hamburg #stuttgart #cologne #frankfurt #hongkong #budapest #jakarta #bali #dublin #telaviv #milan #rome #venice #florence #naples #turin #palermo #bologna #tokyo #osaka #kualalumpur #malta #amsterdam #oslo #manila #warsaw #krakow #doha #alrayyan #riyadh #jeddah #mecca #medina #singapore #seoul #barcelona #madrid #stockholm #zurich #taipei #tainan #taichung #kaohsiung #bangkok #Phuket #istanbul #london #manchester #edinburgh #hcmc #hanoi #lodz #wroclaw #poznan #katowice #rio #salvador #newdelhi #bangalore #bandung #yokohama #nagoya #okinawa #fukuoka #jerusalem #IT #4 #newdelhi #Pune #Hyderabad #Bangalore #Mumbai #Bengaluru #Chennai #Kolkata #Lucknow #sanfrancisco #sanjose #losangeles #sandiego #oakland #denver #miami #orlando #atlanta #chicago #boston #detroit #newyork #portland #philadelphia #dallas #houston #austin #seattle #sydney #melbourne #perth #toronto #vancouver #montreal #shanghai #beijing #shenzhen #prague #Brno #Ostrava #cairo #alexandria #giza #estonia #paris #berlin #munich #hamburg #stuttgart #cologne #frankfurt #hongkong #budapest #jakarta #bali #dublin #telaviv #milan #rome #tokyo #osaka #kualalumpur #amsterdam #oslo #manila #warsaw #krakow #bucharest #moscow #saintpetersburg #capetown #johannesburg #seoul #barcelona #madrid #stockholm #zurich #taipei #bangkok #Phuket #istanbul #london #manchester #edinburgh #kiev #hcmc #hanoi #wroclaw #poznan #katowice #rio #salvador #IT #4 #5 Equal Opportunity Employer At Agoda, we pride ourselves on being a company represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Agoda is based solely on a person‚Äôs merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details please read our privacy policy . Disclaimer We do not accept any terms or conditions, nor do we recognize any agency‚Äôs representation of a candidate, from unsolicited third-party or agency submissions. If we receive unsolicited or speculative CVs, we reserve the right to contact and hire the candidate directly without any obligation to pay a recruitment fee.",,,SQL,
4251671547,Remote Python AI Engineer - 17852,Turing,"Delhi, India (Remote)",Save Remote Python AI Engineer - 17852¬†  at Turing,Contract,,"About the job Work on Real-World Problems with Global Tech Experts Join a leading U.S.-based technology company as a Python Developer / AI Engineer, where you‚Äôll tackle real-world challenges and build innovative solutions alongside top global experts. This is a fully remote, contract-based opportunity ideal for developers passionate about Python, data analysis, and AI-driven work. Key Responsibilities: Write efficient, production-grade Python code to solve complex problems. Analyze public datasets and extract meaningful insights using Python and SQL. Collaborate with researchers and global teams to iterate on data-driven ideas. Document all code and development decisions in Jupyter Notebooks or similar platforms. Maintain high-quality standards and contribute to technical excellence. Job Requirements: Open to all levels: junior, mid-level, or senior engineers. Degree in Computer Science, Engineering, or equivalent practical experience. Proficient in Python programming for scripting, automation, or backend development. Experience with SQL/NoSQL databases is a plus. Familiarity with cloud platforms (AWS, GCP, Azure) is advantageous. Must be able to work 5+ hours overlapping with Pacific Time (PST/PT). Strong communication and collaboration skills in a remote environment. Perks & Benefits: Work on cutting-edge AI and data projects impacting real-world use cases. Collaborate with top minds from Meta, Stanford, and Google. 100% remote ‚Äì work from anywhere. Contract role with flexibility and no traditional job constraints. Competitive compensation in USD, aligned with global tech standards. Selection Process: Shortlisted developers may be asked to complete an assessment. If you clear the assessment, you will be contacted for contract assignments with expected start dates, durations, and end dates. Some contract assignments require fixed weekly hours, averaging 20/30/40 hours per week for the duration of the contract assignment.",,,"Python, SQL, Data Analysis",
4251671547,Remote Python AI Engineer - 17852,Turing,"Delhi, India (Remote)",Save Remote Python AI Engineer - 17852¬†  at Turing,Contract,,"About the job Work on Real-World Problems with Global Tech Experts Join a leading U.S.-based technology company as a Python Developer / AI Engineer, where you‚Äôll tackle real-world challenges and build innovative solutions alongside top global experts. This is a fully remote, contract-based opportunity ideal for developers passionate about Python, data analysis, and AI-driven work. Key Responsibilities: Write efficient, production-grade Python code to solve complex problems. Analyze public datasets and extract meaningful insights using Python and SQL. Collaborate with researchers and global teams to iterate on data-driven ideas. Document all code and development decisions in Jupyter Notebooks or similar platforms. Maintain high-quality standards and contribute to technical excellence. Job Requirements: Open to all levels: junior, mid-level, or senior engineers. Degree in Computer Science, Engineering, or equivalent practical experience. Proficient in Python programming for scripting, automation, or backend development. Experience with SQL/NoSQL databases is a plus. Familiarity with cloud platforms (AWS, GCP, Azure) is advantageous. Must be able to work 5+ hours overlapping with Pacific Time (PST/PT). Strong communication and collaboration skills in a remote environment. Perks & Benefits: Work on cutting-edge AI and data projects impacting real-world use cases. Collaborate with top minds from Meta, Stanford, and Google. 100% remote ‚Äì work from anywhere. Contract role with flexibility and no traditional job constraints. Competitive compensation in USD, aligned with global tech standards. Selection Process: Shortlisted developers may be asked to complete an assessment. If you clear the assessment, you will be contacted for contract assignments with expected start dates, durations, and end dates. Some contract assignments require fixed weekly hours, averaging 20/30/40 hours per week for the duration of the contract assignment.",,,"Python, SQL, Data Analysis",
4222431800,Lead Data Engineer,Khushi Baby,"Jaipur, Rajasthan, India (On-site)",On-site,Full-time,,"About the job Khushi Baby, a nonprofit organization in India, serves as a technical partner to health departments. Established in 2016 from a Yale University classroom, it has grown into a 90+ member team with offices in Jaipur, Udaipur, Delhi, and Bengaluru. Khushi Baby focuses on digital health solutions, health program strengthening, and R&D. Its flagship platform, the Community Health Integrated Platform (CHIP), supports over 70,000 community health workers across 40,000 villages, reaching 45 million beneficiaries. The platform has identified and monitored 5+ million high-risk individuals, with the Ministry of Health allocating ‚Çπ160 crore ($20M) for its scale-up. CHIP has enabled initiatives like Rajasthan's digital health census, TB case finding, vector-borne disease surveillance, labor room monitoring, and immunization drives, co-designed with extensive field input. In R&D, Khushi Baby advances community-level geospatial analysis and individual health diagnostics, including smartphone-based tools and low-literacy models. Programmatically, it focuses on maternal health, child malnutrition, and zero-dose children. Backed by donors like GAVI, Skoll Foundation, and CSR funding, Khushi Baby partners with IITs, AIIMS Jodhpur, JPAL South Asia, MIT, Microsoft Research, WHO, and multiple state governments. Khushi Baby seeks skilled, creative, and driven candidates eager to make a large-scale public health impact by joining its interdisciplinary team in policy, design, development, implementation, and data science. Job Overview We are looking for a Lead Data Engineer to design, build, and optimize scalable data systems for public health analytics. You will define data workflows, layer architecture, and pipelines, ensuring data quality, security, and efficiency while leading a team of engineers. Key Responsibilities Plan and define data architecture, modelling & workflows for efficient data processing. Build and optimize ETL/ELT pipelines for structured & unstructured health data. Ensure data quality, security, and compliance (FHIR, HL7, etc.). Develop real-time & batch processing systems (Kafka, Flink, RisingWave, etc.). Monitor & optimize performance, scalability, and cloud costs (AWS, GCP, Azure). Manage & mentor data engineers, fostering technical growth. Performance tuning of databases, queries, and pipelines. Handle large-scale data efficiently while ensuring cost-effectiveness. Implement access control, encryption, and compliance measures. Works closely with data scientists, analysts, and business teams to define data requirements. Strong documentation for maintaining pipeline workflows and architecture. Ability to translate technical concepts into business-friendly insights. Stays updated with the latest trends in data engineering and cloud technologies . Adapt and set up new tools, frameworks, and evolving business needs . Required Qualifications Master‚Äôs in Computer Science, Data Engineering, or related field. 7+ years in data engineering, 2+ years in leadership. Expertise in SQL, Python, Data Modeling, and pipeline orchestration (Mage AI, Airflow, etc.) Knowledge of partitioning, indexing, caching, and compression techniques, data lineage, cataloging, and metadata management is required. Experience with cloud data services Proficiency in big data processing (Iceberg/Delta). Knowledge of real-time streaming & CDC tools (Kafka, Debezium, Redpanda, etc.). Familiarity with public health standards (FHIR, HL7, ICD-10) is a plus. Strong problem-solving, logical thinking, communication, and leadership skills. Good to have Experience in public health data projects and key health metrics. Knowledge of data lakehouse architecture and federated learning. Remuneration The remuneration offered will be in the range of 15‚Äì25 LPA, depending on the candidate‚Äôs experience, skill set, and evaluation based on our internal parameters. Medical Insurance Paid sick leave, paid parental leave and menstrual leave Learning stipend policy A flexible, enabling environment workplace with the opportunity to grow into leadership roles. Opportunities to attend and actively participate in prestigious International conferences and workshops Note : The candidate will be on a probationary period for the first 90 days of the contract",,,"Python, SQL, R",
4222431800,Lead Data Engineer,Khushi Baby,"Jaipur, Rajasthan, India (On-site)",On-site,Full-time,,"About the job Khushi Baby, a nonprofit organization in India, serves as a technical partner to health departments. Established in 2016 from a Yale University classroom, it has grown into a 90+ member team with offices in Jaipur, Udaipur, Delhi, and Bengaluru. Khushi Baby focuses on digital health solutions, health program strengthening, and R&D. Its flagship platform, the Community Health Integrated Platform (CHIP), supports over 70,000 community health workers across 40,000 villages, reaching 45 million beneficiaries. The platform has identified and monitored 5+ million high-risk individuals, with the Ministry of Health allocating ‚Çπ160 crore ($20M) for its scale-up. CHIP has enabled initiatives like Rajasthan's digital health census, TB case finding, vector-borne disease surveillance, labor room monitoring, and immunization drives, co-designed with extensive field input. In R&D, Khushi Baby advances community-level geospatial analysis and individual health diagnostics, including smartphone-based tools and low-literacy models. Programmatically, it focuses on maternal health, child malnutrition, and zero-dose children. Backed by donors like GAVI, Skoll Foundation, and CSR funding, Khushi Baby partners with IITs, AIIMS Jodhpur, JPAL South Asia, MIT, Microsoft Research, WHO, and multiple state governments. Khushi Baby seeks skilled, creative, and driven candidates eager to make a large-scale public health impact by joining its interdisciplinary team in policy, design, development, implementation, and data science. Job Overview We are looking for a Lead Data Engineer to design, build, and optimize scalable data systems for public health analytics. You will define data workflows, layer architecture, and pipelines, ensuring data quality, security, and efficiency while leading a team of engineers. Key Responsibilities Plan and define data architecture, modelling & workflows for efficient data processing. Build and optimize ETL/ELT pipelines for structured & unstructured health data. Ensure data quality, security, and compliance (FHIR, HL7, etc.). Develop real-time & batch processing systems (Kafka, Flink, RisingWave, etc.). Monitor & optimize performance, scalability, and cloud costs (AWS, GCP, Azure). Manage & mentor data engineers, fostering technical growth. Performance tuning of databases, queries, and pipelines. Handle large-scale data efficiently while ensuring cost-effectiveness. Implement access control, encryption, and compliance measures. Works closely with data scientists, analysts, and business teams to define data requirements. Strong documentation for maintaining pipeline workflows and architecture. Ability to translate technical concepts into business-friendly insights. Stays updated with the latest trends in data engineering and cloud technologies . Adapt and set up new tools, frameworks, and evolving business needs . Required Qualifications Master‚Äôs in Computer Science, Data Engineering, or related field. 7+ years in data engineering, 2+ years in leadership. Expertise in SQL, Python, Data Modeling, and pipeline orchestration (Mage AI, Airflow, etc.) Knowledge of partitioning, indexing, caching, and compression techniques, data lineage, cataloging, and metadata management is required. Experience with cloud data services Proficiency in big data processing (Iceberg/Delta). Knowledge of real-time streaming & CDC tools (Kafka, Debezium, Redpanda, etc.). Familiarity with public health standards (FHIR, HL7, ICD-10) is a plus. Strong problem-solving, logical thinking, communication, and leadership skills. Good to have Experience in public health data projects and key health metrics. Knowledge of data lakehouse architecture and federated learning. Remuneration The remuneration offered will be in the range of 15‚Äì25 LPA, depending on the candidate‚Äôs experience, skill set, and evaluation based on our internal parameters. Medical Insurance Paid sick leave, paid parental leave and menstrual leave Learning stipend policy A flexible, enabling environment workplace with the opportunity to grow into leadership roles. Opportunities to attend and actively participate in prestigious International conferences and workshops Note : The candidate will be on a probationary period for the first 90 days of the contract",,,"Python, SQL, R",
4246298911,Data Engineer ‚Äì C11/Officer (India),Citi,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job The Role The Data Engineer is accountable for developing high quality data products to support the Bank‚Äôs regulatory requirements and data driven decision making. A Mantas Scenario Developer will serve as an example to other team members, work closely with customers, and remove or escalate roadblocks. By applying their knowledge of data architecture standards, data warehousing, data structures, and business intelligence they will contribute to business outcomes on an agile team. Responsibilities Developing and supporting scalable, extensible, and highly available data solutions Deliver on critical business priorities while ensuring alignment with the wider architectural vision Identify and help address potential risks in the data supply chain Follow and contribute to technical standards Design and develop analytical data models Required Qualifications & Work Experience First Class Degree in Engineering/Technology (4-year graduate course) 5 to 8 years‚Äô experience implementing data-intensive solutions using agile methodologies Experience of relational databases and using SQL for data querying, transformation and manipulation Experience of modelling data for analytical consumers Hands on Mantas ( Oracle FCCM ) expert throughout the full development life cycle, including: requirements analysis, functional design, technical design, programming, testing, documentation, implementation, and on-going technical support Translate business needs (BRD) into effective technical solutions and documents (FRD/TSD) Ability to automate and streamline the build, test and deployment of data pipelines Experience in cloud native technologies and patterns A passion for learning new technologies, and a desire for personal growth, through self-study, formal classes, or on-the-job training Excellent communication and problem-solving skills T echnical Skills (Must Have) ETL: Hands on experience of building data pipelines. Proficiency in two or more data integration platforms such as Ab Initio, Apache Spark, Talend and Informatica Mantas: Expert in Oracle Mantas/FCCM, Scenario Manager, Scenario Development, thorough knowledge and hands on experience in Mantas FSDM, DIS, Batch Scenario Manager Big Data: Experience of ‚Äòbig data‚Äô platforms such as Hadoop, Hive or Snowflake for data storage and processing Data Warehousing & Database Management: Understanding of Data Warehousing concepts, Relational (Oracle, MSSQL, MySQL) and NoSQL (MongoDB, DynamoDB) database design Data Modeling & Design: Good exposure to data modeling techniques; design, optimization and maintenance of data models and data structures Languages: Proficient in one or more programming languages commonly used in data engineering such as Python, Java or Scala DevOps: Exposure to concepts and enablers - CI/CD platforms, version control, automated quality control management Technical Skills (Valuable) Ab Initio: Experience developing Co>Op graphs; ability to tune for performance. Demonstrable knowledge across full suite of Ab Initio toolsets e.g., GDE, Express>IT, Data Profiler and Conduct>IT, Control>Center, Continuous>Flows Cloud: Good exposure to public cloud data platforms such as S3, Snowflake, Redshift, Databricks, BigQuery, etc. Demonstratable understanding of underlying architectures and trade-offs Data Quality & Controls: Exposure to data validation, cleansing, enrichment and data controls Containerization: Fair understanding of containerization platforms like Docker, Kubernetes File Formats: Exposure in working on Event/File/Table Formats such as Avro, Parquet, Protobuf, Iceberg, Delta Others: Basics of Job scheduler like Autosys. Basics of Entitlement management Certification on any of the above topics would be an advantage. ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Digital Software Engineering ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Most Relevant Skills Please see the requirements listed above. ------------------------------------------------------ Other Relevant Skills For complementary skills, please see above and/or contact the recruiter. ------------------------------------------------------ Citi is an equal opportunity employer, and qualified candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other characteristic protected by law. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi . View Citi‚Äôs EEO Policy Statement and the Know Your Rights poster.",Manager,,"Python, SQL",
4246298911,Data Engineer ‚Äì C11/Officer (India),Citi,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job The Role The Data Engineer is accountable for developing high quality data products to support the Bank‚Äôs regulatory requirements and data driven decision making. A Mantas Scenario Developer will serve as an example to other team members, work closely with customers, and remove or escalate roadblocks. By applying their knowledge of data architecture standards, data warehousing, data structures, and business intelligence they will contribute to business outcomes on an agile team. Responsibilities Developing and supporting scalable, extensible, and highly available data solutions Deliver on critical business priorities while ensuring alignment with the wider architectural vision Identify and help address potential risks in the data supply chain Follow and contribute to technical standards Design and develop analytical data models Required Qualifications & Work Experience First Class Degree in Engineering/Technology (4-year graduate course) 5 to 8 years‚Äô experience implementing data-intensive solutions using agile methodologies Experience of relational databases and using SQL for data querying, transformation and manipulation Experience of modelling data for analytical consumers Hands on Mantas ( Oracle FCCM ) expert throughout the full development life cycle, including: requirements analysis, functional design, technical design, programming, testing, documentation, implementation, and on-going technical support Translate business needs (BRD) into effective technical solutions and documents (FRD/TSD) Ability to automate and streamline the build, test and deployment of data pipelines Experience in cloud native technologies and patterns A passion for learning new technologies, and a desire for personal growth, through self-study, formal classes, or on-the-job training Excellent communication and problem-solving skills T echnical Skills (Must Have) ETL: Hands on experience of building data pipelines. Proficiency in two or more data integration platforms such as Ab Initio, Apache Spark, Talend and Informatica Mantas: Expert in Oracle Mantas/FCCM, Scenario Manager, Scenario Development, thorough knowledge and hands on experience in Mantas FSDM, DIS, Batch Scenario Manager Big Data: Experience of ‚Äòbig data‚Äô platforms such as Hadoop, Hive or Snowflake for data storage and processing Data Warehousing & Database Management: Understanding of Data Warehousing concepts, Relational (Oracle, MSSQL, MySQL) and NoSQL (MongoDB, DynamoDB) database design Data Modeling & Design: Good exposure to data modeling techniques; design, optimization and maintenance of data models and data structures Languages: Proficient in one or more programming languages commonly used in data engineering such as Python, Java or Scala DevOps: Exposure to concepts and enablers - CI/CD platforms, version control, automated quality control management Technical Skills (Valuable) Ab Initio: Experience developing Co>Op graphs; ability to tune for performance. Demonstrable knowledge across full suite of Ab Initio toolsets e.g., GDE, Express>IT, Data Profiler and Conduct>IT, Control>Center, Continuous>Flows Cloud: Good exposure to public cloud data platforms such as S3, Snowflake, Redshift, Databricks, BigQuery, etc. Demonstratable understanding of underlying architectures and trade-offs Data Quality & Controls: Exposure to data validation, cleansing, enrichment and data controls Containerization: Fair understanding of containerization platforms like Docker, Kubernetes File Formats: Exposure in working on Event/File/Table Formats such as Avro, Parquet, Protobuf, Iceberg, Delta Others: Basics of Job scheduler like Autosys. Basics of Entitlement management Certification on any of the above topics would be an advantage. ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Digital Software Engineering ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Most Relevant Skills Please see the requirements listed above. ------------------------------------------------------ Other Relevant Skills For complementary skills, please see above and/or contact the recruiter. ------------------------------------------------------ Citi is an equal opportunity employer, and qualified candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other characteristic protected by law. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi . View Citi‚Äôs EEO Policy Statement and the Know Your Rights poster.",Manager,,"Python, SQL",
4244844452,Lead Data Engineer,Atyeti Inc,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Job Description: Implement efficient and maintainable code using best process and coding standards. Design, develop, test, and deploy high-performance and scalable data solutions using Python. Collaborate with cross-functional teams to understand business requirements and translate them into technical specifications. Implement efficient and maintainable code using best practices and coding standards Utilize experiences in SQL to design, optimize, and maintain relational databases. Write complex SQL queries for data retrieval, manipulation, and analysis. Perform database performance tuning and optimization. Databricks Platform Work with Databricks platform for big data processing and analytics. Develop and maintain ETL processes using Databricks notebooks. Implement and optimize data pipelines for data transformation and integration",,,"Python, SQL",
4244844452,Lead Data Engineer,Atyeti Inc,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Job Description: Implement efficient and maintainable code using best process and coding standards. Design, develop, test, and deploy high-performance and scalable data solutions using Python. Collaborate with cross-functional teams to understand business requirements and translate them into technical specifications. Implement efficient and maintainable code using best practices and coding standards Utilize experiences in SQL to design, optimize, and maintain relational databases. Write complex SQL queries for data retrieval, manipulation, and analysis. Perform database performance tuning and optimization. Databricks Platform Work with Databricks platform for big data processing and analytics. Develop and maintain ETL processes using Databricks notebooks. Implement and optimize data pipelines for data transformation and integration",,,"Python, SQL",
4239070503,Senior Data Engineer,R Systems,"Noida, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Technical Requirements SQL (Advanced level): Strong command of complex SQL logic, including window functions, CTEs, pivot/unpivot, and be proficient in stored procedure/SQL script development. Experience writing maintainable SQL for transformations. Python for ETL : Ability to write modular and reusable ETL logic using Python. Familiarity with JSON manipulation and API consumption. Google BigQuery : Hands-on experience developing within the BigQuery environment. Understanding of partitioning, clustering, and performance tuning. ETL Pipeline Development : Experienced in developing ETL/ELT pipelines, data profiling, validation, quality/health check, error handling, logging and notifications, etc. Nice-to-Have Skills: Experiences with Google BigQuery platform. Knowledge of CI/CD practices for data workflows.",Manager,,"Python, SQL",
4239070503,Senior Data Engineer,R Systems,"Noida, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Technical Requirements SQL (Advanced level): Strong command of complex SQL logic, including window functions, CTEs, pivot/unpivot, and be proficient in stored procedure/SQL script development. Experience writing maintainable SQL for transformations. Python for ETL : Ability to write modular and reusable ETL logic using Python. Familiarity with JSON manipulation and API consumption. Google BigQuery : Hands-on experience developing within the BigQuery environment. Understanding of partitioning, clustering, and performance tuning. ETL Pipeline Development : Experienced in developing ETL/ELT pipelines, data profiling, validation, quality/health check, error handling, logging and notifications, etc. Nice-to-Have Skills: Experiences with Google BigQuery platform. Knowledge of CI/CD practices for data workflows.",Manager,,"Python, SQL",
4222670689,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You‚Äôll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you‚Äôll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you‚Äôll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization. Strong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources. Proficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities. Hands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements Preferred Technical And Professional Experience Understanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling. Ability to implement robust data validation, cleansing, and governance frameworks within ETL processes. Proficiency in SQL and/or Shell scripting for custom transformations and automation tasks",,,"SQL, Machine Learning",
4239193459,Data Engineer_Pune_CTC- 18-20 LPA,Andor Tech,"Pune, Maharashtra, India (On-site)",On-site,Contract,,"About the job Job Description : Able to enrich data by data transformation and joining with other datasets. Able to analyze data and derive statistical insights. Able to convey story through data visualization. Ability to build Data pipelines for diverse interfaces. Good understating of API workflow. Technical Skills : AWS Data Lake and AWS data hub and AWS cloud platform. Soft Skill : Good written and verbal communication skills. Good Team player. Strong analytical and logical reasoning skills. Self-motivated, Ability to complete assignments independently. Ability to make decisions. Ability to work effectively under stress. Flexible to work in global time zones. Ability to work with wider spectrum of stakeholders and senior managements #python#azure#nlp#dataextraction",Manager,,Python,
4247104962,Data Engineer,Trillionloans,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job Responsibilities will include Functional Expertise Collaborating closely with multiple teams to translate the requirements into technical specifications. Offering clear technical guidance and direction to ensure solutions meet user and tech requirements. Leading technical discussions, code reviews to maintain code quality, identify improvement opportunities, and ensure adherence to standards. Staying updated on the latest data engineering trends and applying them to solve complex challenges. Problem Solving & Communication : Strong analytical and problem-solving skills. Excellent communication and collaboration skills. Ability to work independently and as part of a team. Providing guidance and mentorship to other team members , fostering their professional growth and skill development. Experience working with Fintech institutions is a plus. Qualification & Experience (type & industry) B.Tech Degree Skills & know-how Experience level: 3-5 years Minimum 2 years relevant experience in AWS Cloud Data Warehousing experience - Cloud data warehouse - Redshift/SQL In-memory framework exp - Pyspark Data engineering pipeline use case experience : Ingestion of data from different sources to Cloud file system(S3 buckets) and then transforming/processing the data using AWS Glue and finally loading the same to cloud warehouse for Data analytics Big data use cases : Exposure to huge data volumes involving TBs of data for storage/migration/processing. Programming experience in Python Familiarity with Reports/Dashboards using cloud native applications Knowledge of data pipeline orchestration using Airflow - good to have. Understanding of API development",,,"Python, SQL",
4235450306,Sr GCP Data Engineer,LTIMindtree,"Chennai, Tamil Nadu, India (Hybrid)",Hybrid,Full-time,,"About the job Experience: 5-12 years Primary Skills: GCP, Big query, Python Notice period: Immediate to 30 days Overall 5+ years of experience in data eng Strong Experience with GCP services like Big Query Cloud Storage composer etc Hands on experience in Python Proficient in Pyspark databricks Strong understanding of data warehouse concepts ETL process and data modelling GCP certifications are added advantage",,,Python,
4235369204,ML Engineer,Uplers,"Jamshedpur, Jharkhand, India (Remote)",Remote,‚Çπ1M/yr - ‚Çπ1.2M/yr,,"About the job Experience : Fresher Salary : INR 1000000-1200000 / year (based on experience) Expected Notice Period : 30 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Leegality) (*Note: This is a requirement for one of Uplers' client - Leegality) What do you need for this opportunity? Must have skills required: Start-up Experience, Python, Ml frameworks, Statistical analysis, CNNs, Machine Learning, Computer Vision, AI models Leegality is Looking for: What You'll Do As an ML Engineer, you will develop solutions to interesting technical problems, explore exciting growth opportunities, and have a real impact on our product. To ensure success as a machine learning engineer, you should demonstrate solid data science knowledge and experience in a related ML role. A first-class machine learning engineer will be someone whose expertise translates into the enhanced performance of predictive automation software. Responsibilities: Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models. Transforming data science prototypes and applying appropriate ML algorithms and tools. Solving complex problems with multi-layered data sets and optimizing existing machine learning libraries and frameworks. Developing ML algorithms to analyze huge volumes of historical data to make predictions. Running tests, performing statistical analysis, and interpreting test results. Exposure to generative AI (LLM) based ML architectures Experienced in working with CNN. Knows how to train and fine-tune CNN-based models. Documenting machine learning processes. Requirements: 1+ years of relevant experience in Machine Learning Engineering. Advanced proficiency with Python. Extensive knowledge of ML frameworks, libraries, data structures, data modeling, and software architecture. Superb analytical and problem-solving abilities. Great communication and collaboration skills. Preferences: Start-up experience is good to have. Willing to join immediately, ideally. Female candidates are preferred Interview Process - Technical Round 1 - ML Fundamentals, NLP models Assessment Technical Round 2 - Discussion on the assignment submitted & modify it according to the requirements Round 3 - with the Tech Lead - Tech Managerial - Past projects & in-depth knowledge of ML How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Start-up Experience, Python, Ml frameworks, Statistical analysis, CNNs, Machine Learning, Computer Vision, AI models",Manager,,"Python, Machine Learning",
4235648600,Senior Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job 6+ years of experience with Java Spark. Strong understanding of distributed computing, big data principles, and batch/stream processing. Proficiency in working with AWS services such as S3, EMR, Glue, Lambda, and Athena. Experience with Data Lake architectures and handling large volumes of structured and unstructured data. Familiarity with various data formats. Strong problem-solving and analytical skills. Excellent communication and collaboration abilities. Design, develop, and optimize large-scale data processing pipelines using Java Spark Build scalable solutions to manage data ingestion, transformation, and storage in AWS-based Data Lake environments. Collaborate with data architects and analysts to implement data models and workflows aligned with business requirements. Ensure performance tuning, fault tolerance, and reliability of distributed data processing systems. Desired Skills and Experience DevOps, Java Spark, Terraform, EKS, AWS Serverless Services, Java, S3",,,,
4230811083,Data Engineer-MDM -Senior Associate,PwC Acceleration Centers in India,"Andhra Pradesh, India (On-site)",On-site,Full-time,,"About the job A career in our Advisory Acceleration Centre is the natural extension of PwC‚Äôs leading class global delivery capabilities. We provide premium, cost effective, high quality services that support process quality and delivery capability in support for client engagements. Years of Experience: Candidates with 4-8 years of hands-on experience Position Requirements Must Have: Experience in Master Data Management Projects using Reltio MDM Comfortable with various aspects of configuration - like data modelling, match rules, cleanse rules, metadata analysis etc. Good understanding of Data Stewardship and Governance process - Manual Merge, Hierarchy Management, Workflow Management, Unmerge, Curation, Survivorship etc. Refine MDM customer and reference-data data models as the business evolves Reltio Business Model Configuration - Survivorship Rules Configuration - Matching and Merging Configuration - Relationship, Graph, Roles and Tags Configuration Should have expertise in extracting data from different source systems like flat files, XML sources, Big data appliances, RDBMS, etc., Should have good understanding of Data Quality processes, methods and project lifecycle. Should have hands on experience of Profiling, Standardization, Matching, Survivorship and Consolidation Techniques Should be proficient in Fuzzy matching, Reporting section of tools Experience validating the ETL and writing SQL queries Strong knowledge in DWH concepts Perform design and code review Work proactively with team, provide the technical guidance to meet delivery timelines Should have clear understanding of DW Lifecycle and contributed in preparing technical design documents and test plans Good analytical & problem-solving skills Good communication and presentation skills Desired Knowledge / Skills Experience / Knowledge on other ETL tools like Informatica PC, Datastage, ODI Cloud data technologies, Data Lake, Big Data, Snowflake DB (Cloud) Worked in Offshore / Onsite Engagements Professional And Educational Background BE / B.Tech / MCA / M.Sc / M.E / M.Tech / MBA",Associate,,SQL,
4246299482,Data Engineer ‚ÄìC11/Officer Pune,Citi,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job The Role The Data Engineer is accountable for developing high quality data products to support the Bank‚Äôs regulatory requirements and data driven decision making. A Mantas Scenario Developer will serve as an example to other team members, work closely with customers, and remove or escalate roadblocks. By applying their knowledge of data architecture standards, data warehousing, data structures, and business intelligence they will contribute to business outcomes on an agile team. Responsibilities Developing and supporting scalable, extensible, and highly available data solutions Deliver on critical business priorities while ensuring alignment with the wider architectural vision Identify and help address potential risks in the data supply chain Follow and contribute to technical standards Design and develop analytical data models Required Qualifications & Work Experience First Class Degree in Engineering/Technology (4-year graduate course) 5 to 8 years‚Äô experience implementing data-intensive solutions using agile methodologies Experience of relational databases and using SQL for data querying, transformation and manipulation Experience of modelling data for analytical consumers Hands on Mantas ( Oracle FCCM ) expert throughout the full development life cycle, including: requirements analysis, functional design, technical design, programming, testing, documentation, implementation, and on-going technical support Translate business needs (BRD) into effective technical solutions and documents (FRD/TSD) Ability to automate and streamline the build, test and deployment of data pipelines Experience in cloud native technologies and patterns A passion for learning new technologies, and a desire for personal growth, through self-study, formal classes, or on-the-job training Excellent communication and problem-solving skills T echnical Skills (Must Have) ETL: Hands on experience of building data pipelines. Proficiency in two or more data integration platforms such as Ab Initio, Apache Spark, Talend and Informatica Mantas: Expert in Oracle Mantas/FCCM, Scenario Manager, Scenario Development, thorough knowledge and hands on experience in Mantas FSDM, DIS, Batch Scenario Manager Big Data: Experience of ‚Äòbig data‚Äô platforms such as Hadoop, Hive or Snowflake for data storage and processing Data Warehousing & Database Management: Understanding of Data Warehousing concepts, Relational (Oracle, MSSQL, MySQL) and NoSQL (MongoDB, DynamoDB) database design Data Modeling & Design: Good exposure to data modeling techniques; design, optimization and maintenance of data models and data structures Languages: Proficient in one or more programming languages commonly used in data engineering such as Python, Java or Scala DevOps: Exposure to concepts and enablers - CI/CD platforms, version control, automated quality control management Technical Skills (Valuable) Ab Initio: Experience developing Co>Op graphs; ability to tune for performance. Demonstrable knowledge across full suite of Ab Initio toolsets e.g., GDE, Express>IT, Data Profiler and Conduct>IT, Control>Center, Continuous>Flows Cloud: Good exposure to public cloud data platforms such as S3, Snowflake, Redshift, Databricks, BigQuery, etc. Demonstratable understanding of underlying architectures and trade-offs Data Quality & Controls: Exposure to data validation, cleansing, enrichment and data controls Containerization: Fair understanding of containerization platforms like Docker, Kubernetes File Formats: Exposure in working on Event/File/Table Formats such as Avro, Parquet, Protobuf, Iceberg, Delta Others: Basics of Job scheduler like Autosys. Basics of Entitlement management Certification on any of the above topics would be an advantage. ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Digital Software Engineering ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Most Relevant Skills Please see the requirements listed above. ------------------------------------------------------ Other Relevant Skills For complementary skills, please see above and/or contact the recruiter. ------------------------------------------------------ Citi is an equal opportunity employer, and qualified candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other characteristic protected by law. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi . View Citi‚Äôs EEO Policy Statement and the Know Your Rights poster.",Manager,,"Python, SQL",
4233009480,Lead Big Data Engineer,Qualys,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Come work at a place where innovation and teamwork come together to support the most exciting missions in the world! Description: We are seeking a talented Lead Big Data Engineer to deliver roadmap features of Enterprise TruRisk Platform which would help customers to Measure, Communicate and Eliminate Cyber Risks. Working with a team of engineers and architects, you will be responsible for prototyping, designing, developing and supporting a highly scalable, distributed SaaS based Security Risk Prioritization product. This is a fantastic opportunity to be an integral part of a team building Qualys next generation platform using Big Data & Micro-Services based technology to process over billions of transactions data per day, leverage open-source technologies, and work on challenging and business-impacting initiatives. Responsibilities: Be the thought leader in data platform and pipeline along with Risk Evaluation. Provide technical leadership to the engineering organization on data platform design, roll out and evolution. Liason to product teams, professional services and sales engineers on solution and trade-off reviews and represent engineering in such conversations. Drive technology explorations and roadmaps. Serve as a technical lead on our most demanding, cross-functional departments. Ensure the quality of architecture and design of systems. Functionally decompose complex problems into simple, straight-forward solutions. Fully and completely understand system interdependencies and limitations. Possess expert knowledge in performance, scalability, enterprise system architecture, and engineering best practices. Leverage knowledge of internal and industry prior art in design decisions. Effectively research and benchmark cloud technology against other competing systems in the industry. Able to document the details so it will be easy for developers to understand the requirements. Assisting developers with proper requirements and directions. Assist in the career development of others, actively mentoring individuals and the community on advanced technical issues and helping managers guide the career growth of their team members. Exert technical influence over multiple teams, increasing their productivity and effectiveness by sharing your deep knowledge and experience. Able to share knowledge and train others. Qualifications: Bachelor‚Äôs degree in computer science or equivalent 8+ years of total experience. 4+ years of relevant experience in design and architecture Big Data solutions using Spark 3+ years experience in working with engineering resources for innovation. 4+ years experience in understanding Big Data events flow pipeline. 3+ years experience in performance testing for large infrastructure. 3+ In depth experience in understanding various search solutions solr/elastic. 3+ years experience in Kafka In depth experience in Data lakes and related ecosystems. In depth experience of messing queue In depth experience in giving requirements to build a scalable architecture for Big data and Micro-services environments. In depth experience in understanding caching components or services Knowledge in Presto technology. Knowledge in Airflow. Hands-on experience in scripting and automation In depth understanding of RDBMS/NoSQL, Oracle , Cassandra , Kafka , Redis, Hadoop, lambda architecture, kappa , kappa ++ architectures with flink data streaming and rule engines Experience in working with ML models engineering and related deployment. Design and implement secure big data clusters to meet many compliances and regulatory requirements. Experience in leading the delivery of large-scale systems focused on managing the infrastructure layer of the technology stack. Strong experience in doing performance benchmarking testing for Big data technologies. Strong troubleshooting skills. Experience leading development life cycle process and best practices Experience in Big Data services administration would be added value. Experience with Agile Management (SCRUM, RUP, XP), OO Modeling, working on internet, UNIX, Middleware, and database related projects. Experience mentoring/training the engineering community on complex technical issue. Project management experience",manager,3+ years experience,,
4202145833,Staff I Data Engineer,BlackLine India,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Get to Know Us: It's fun to work in a company where people truly believe in what they're doing! At BlackLine, we're committed to bringing passion and customer focus to the business of enterprise applications. Since being founded in 2001, BlackLine has become a leading provider of cloud software that automates and controls the entire financial close process. Our vision is to modernize the finance and accounting function to enable greater operational effectiveness and agility, and we are committed to delivering innovative solutions and services to empower accounting and finance leaders around the world to achieve Modern Finance. Being a best-in-class SaaS Company, we understand that bringing in new ideas and innovative technology is mission critical. At BlackLine we are always working with new, cutting edge technology that encourages our teams to learn something new and expand their creativity and technical skillset that will accelerate their careers. Work, Play and Grow at BlackLine! Make Your Mark: As a member of the Data & BI Engineering team you will primarily focus on advancing our Enterprise Data Platform to allow the organization to make data-driven decisions. The successful candidate will work closely with cross-functional teams to identify business requirements, design, and develop data models, data warehouses, and data visualization solutions that help support the organization's strategic goals. The Staff Data Engineer will work in a dynamic environment and will be required to stay current with the latest trends and technologies in the business intelligence field. The ideal candidate will be able to pick up business domain and internal process knowledge and leverage that knowledge to think strategically, communicate effectively, and manage multiple projects simultaneously. The team is also responsible for administering tools and platforms around reporting, analytics, and data visualization while promoting best practices. The role requires a strong combination of technical expertise, leadership skills, and a deep understanding of data engineering principles and best practices. We are looking for a driven, detail-oriented, and passionate engineer to come to join our team. You'll Get To: Provide technical expertise and leadership in technology direction, road-mapping, architecture definition, design, development, and delivery of enterprise-class solutions while adhering to timelines, coding standards, requirements, and quality. Architect, design, develop, test, troubleshoot, debug, optimize, scale, perform the capacity planning, deploy, maintain, and improve software applications, driving the delivery of high-quality value and features to Blackline‚Äôs customers. Work collaboratively across the company to design, communicate and further assist with adoption of best practices in architecture and implementation. Deliver robust architectural solutions for complex design problems. Implement, refine, and enforce data engineering best practices to ensure that delivered features meet performance, security, and maintainability expectations. Research, test, benchmark, and evaluate new tools and technologies, and recommend ways to implement them in data platform. Identify and create solutions that are likely to contribute to the development of new company concepts while keeping in mind the business strategy, short- and long-term roadmap, and architectural considerations to support them in a highly scalable and easy extensible manner. Actively participate in research, development, support, management, and other company initiatives designing solutions to optimally address current and future business requirements and infrastructure plans. Inspire a forward-thinking team of developers, acting as an agent of change and evangelist for a quality-first culture within the organization. Mentor and coach key technical staff and guide them to solutions on complex design issues. Act as a conduit for questions and information flow when those outside of Engineering have ideas for new technology applications. Speak in terms relevant to audience, translating technical concepts into non-technical language and vice versa. Facilitate consensus building while striving for win/win scenarios and elicit value-add contributions from all team members in group settings. Maintain a strong sense of business value and return on investment in planning, design, and communication. Proactively identify issues, bottlenecks, gaps, or other areas of concern or opportunity and work to either directly affect change, or advocate for that change by working with peers and leadership to build consensus and act. Perform critical maintenance, deployment, and release support activities, including occasional off-hours support. What You'll Bring: Bachelor's or Master's degree in Computer Science, Data Science, or a related field.10+ years as a data engineer.10+ years of experience using RDBMS, SQL, NoSQL, Python, Java, or other programming languages is a plus. 10+ years of experience designing, developing, testing, and implementing Extract, Transform and Load (ELT/ETL) solutions using enterprise ELT/ETL tools and Open source. 5+ years working experience with SQL and familiarity with Snowflake data warehouse, strong working knowledge in stored procedures, CTEs, and UDFs, RBACKnowledge of data integration and data quality best practicesFamiliarity with data security and privacy regulations.Experience in working in a startup-type environment, good team player, and can work independently with minimal supervision Experience with cloud-native architecture and data solutions.Strong working knowledge in data modeling, data partitioning, and query optimization Demonstrated knowledge of development processes and agile methodologies.Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms.Proficient in managing large volumes of data. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams. Experience in providing technical support and troubleshooting for data-related issues. Expertise with at least one cloud environment and building cloud native data services. Prior experience driving data governance, quality, security initiatives. We‚Äôre Even More Excited If You Have: Experience with Google Cloud or similar cloud provider Significant experience with open source platforms and technologies. Experience with data science and machine learning tools and technologies is a plus. Thrive at BlackLine Because You Are Joining: A technology-based company with a sense of adventure and a vision for the future. Every door at BlackLine is open. Just bring your brains, your problem-solving skills, and be part of a winning team at the world's most trusted name in Finance Automation! A culture that is kind, open, and accepting. It's a place where people can embrace what makes them unique, and the mix of cultural backgrounds and varying interests cultivates diverse thought and perspectives. A culture where BlackLiner's continued growth and learning is empowered. BlackLine offers a wide variety of professional development seminars and inclusive affinity groups to celebrate and support our diversity. BlackLine is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity or expression, race, ethnicity, age, religious creed, national origin, physical or mental disability, ancestry, color, marital status, sexual orientation, military or veteran status, status as a victim of domestic violence, sexual assault or stalking, medical condition, genetic information, or any other protected class or category recognized by applicable equal employment opportunity or other similar laws. BlackLine recognizes that the ways we work and the workplace itself have shifted. We innovate in a workplace that optimizes a combination of virtual and in-person interactions to maximize collaboration and nurture our culture. Candidates who live within a reasonable commute to one of our offices will work in the office at least 2 days a week.",,,"Python, SQL, Machine Learning",
4247300730,Consultant Delivery ( Data Engineer),Worldline,"Ahmedabad, Gujarat, India (Hybrid)",Hybrid,Full-time,,"About the job Job Description Consultant Delivery ( Data Engineer) About Worldline At Worldline, we are pioneers in payments technology, committed to creating innovative solutions that make financial transactions secure, accessible, and seamless worldwide. Our diverse team of professionals collaborates across cultures and disciplines, driving progress that benefits society and businesses of all sizes. We believe that diverse perspectives fuel innovation and are dedicated to fostering an inclusive environment where all individuals can thrive. The Opportunity We are seeking a highly skilled and knowledgeable Data Engineer to join our Data Management team on a transformative Move to Cloud (M2C) project. This role offers a unique opportunity to contribute to a critical initiative, migrating our data infrastructure to the cloud and optimizing our data pipelines for performance and scalability. We welcome applicants from all backgrounds and experiences, believing that our strength lies in our diversity. Technical Skills & Qualifications Education: Bachelor's or Master's degree in Computer Science, Engineering, or a related field. Experience: Minimum of 5 years of experience as a Data Engineer, with a strong focus on cloud-based solutions, preferably within the Google Cloud Platform (GCP) ecosystem. Essential Skills: Strong knowledge of version control systems and CI/CD pipelines. Proficiency in GCP services, particularly DataProc, Dataflow, Cloud Functions, Workflows, Cloud Composer, and BigQuery. Extensive experience with ETL tools, specifically dbt Labs, and a deep understanding of ETL best practices. Proven ability to build and optimize data pipelines, architectures, and datasets from both structured and unstructured data sources. Proficiency in SQL and Python, with experience using Spark. Excellent analytical and problem-solving skills, with the ability to translate complex requirements into technical solutions. Desirable Skills: Relevant certifications in Google Cloud Platform or other data engineering credentials. Preferred Skills Experience migrating data from on-premises data warehouses (e.g., Oracle) to cloud-based solutions. Experience working with large-scale datasets and complex data transformations. Strong communication and interpersonal skills, with the ability to collaborate effectively within a team environment. Why Join Us? At Worldline, we believe that embracing diversity and promoting inclusion drives innovation and success. We foster a workplace where everyone feels valued and empowered to bring their authentic selves. We offer extensive training, mentorship, and development programs to support your growth and help you make a meaningful impact. Join a global team of passionate professionals shaping the future of payments technology‚Äîwhere your ideas, experiences, and perspectives are appreciated and celebrated. Learn more about life at Worldline at Jobs.worldline.com. We are an Equal Opportunity Employer. We do not discriminate based on race, ethnicity, religion, color, national origin, sex (including pregnancy and childbirth), sexual orientation, gender identity or expression, age, disability, or any other legally protected characteristic. We are committed to creating a diverse and inclusive environment for all employees",,,"Python, SQL",
4251670762,Remote Python AI Engineer - 17852,Turing,"Gurugram, Haryana, India (Remote)",Save Remote Python AI Engineer - 17852¬†  at Turing,Contract,,"About the job Work on Real-World Problems with Global Tech Experts Join a leading U.S.-based technology company as a Python Developer / AI Engineer, where you‚Äôll tackle real-world challenges and build innovative solutions alongside top global experts. This is a fully remote, contract-based opportunity ideal for developers passionate about Python, data analysis, and AI-driven work. Key Responsibilities: Write efficient, production-grade Python code to solve complex problems. Analyze public datasets and extract meaningful insights using Python and SQL. Collaborate with researchers and global teams to iterate on data-driven ideas. Document all code and development decisions in Jupyter Notebooks or similar platforms. Maintain high-quality standards and contribute to technical excellence. Job Requirements: Open to all levels: junior, mid-level, or senior engineers. Degree in Computer Science, Engineering, or equivalent practical experience. Proficient in Python programming for scripting, automation, or backend development. Experience with SQL/NoSQL databases is a plus. Familiarity with cloud platforms (AWS, GCP, Azure) is advantageous. Must be able to work 5+ hours overlapping with Pacific Time (PST/PT). Strong communication and collaboration skills in a remote environment. Perks & Benefits: Work on cutting-edge AI and data projects impacting real-world use cases. Collaborate with top minds from Meta, Stanford, and Google. 100% remote ‚Äì work from anywhere. Contract role with flexibility and no traditional job constraints. Competitive compensation in USD, aligned with global tech standards. Selection Process: Shortlisted developers may be asked to complete an assessment. If you clear the assessment, you will be contacted for contract assignments with expected start dates, durations, and end dates. Some contract assignments require fixed weekly hours, averaging 20/30/40 hours per week for the duration of the contract assignment.",,,"Python, SQL, Data Analysis",
3627613653,Web Scraping Data Engineer,TRANSFORM Solutions,"Surat, Gujarat, India (On-site)",On-site,‚Çπ300K/yr - ‚Çπ700K/yr,"(Outsourcing Leadership Award). ùó¢ùòÇùóø ùó∞ùóπùó∂ùó≤ùóªùòÅùòÄ Operating in a wide range of industries, we serve both global Fortune 500 companies (Hilton Worldwide, Oracle) and diverse small and medium-sized businesses (SMBs). Our clients benefit from our extensive experience with 2800+ processes as well as from our world-class infrastructure and industry certifications, ISO 9001 and ISO 27001. We look forward to learning more about your business and supporting your growth! ùóñùóºùóªùòÅùóÆùó∞ùòÅ ùòÇùòÄ: +1 (732) 829-6935 sales@transformsolution.com ‚Ä¶ show more Show more","About the job This job is sourced from a job board. Learn More Design, build web crawlers to scrape data and URLs. Integrate the data crawled and scraped into our databases Create more/better ways to crawl relevant information Strong knowledge of web technologies (HTML, CSS, Javascript, XPath, Regex) Understanding of data privacy policies (esp. GDPR) and personally identifiable information Develop automated and reusable routines for extracting information from various data sources Prepare requirement summary and re-confirm with Operation team Translate business requirements into specific solutions Ability to relay technical information to non-technical users Demonstrate Effective problem solving and analytical skill Ability to pay attention to detail, pro-active, critical thinking and accuracy is essential Ability to work to deadlines and give realistic estimates Skills & Expertise 2+ years of web scraping experience Experience with two or more of the following web scraping frameworks and tools: Selenium, Scrapy, Import.io, Webhose.io, ScrapingHub, ParseHub, Phantombuster, Octoparse, Puppeter, etc. Basic knowledge of data engineering (database ingestion, ETL, etc.) Solution orientation and ""can do"" attitude - with a desire to tackle complex problems. Skills:- Javascript, Python, Selenium, Web Scraping, HTML/CSS and XPath",,,Python,
4231580463,"Data Engineer, IN Data Engineering & Analytics",Amazon Business,"Bengaluru, Karnataka, India",,Full-time,,"About the job Description Amazon IN Platform Development team is looking to hire a rock star Data/BI Engineer to build for pan Amazon India businesses. Amazon India is at the core of hustle @ Amazon WW today and the team is charted with democratizing data access for the entire marketplace & add productivity. That translates to owning the processing of every Amazon India transaction, for which the team is organized to have dedicated business owners & processes for each focus area. The BI Engineer will play a key role in contributing to the success of each focus area, by partnering with respective business owners and leveraging data to identify areas of improvement & optimization. He / She will build deliverables like business process automation, payment behavior analysis, campaign analysis, fingertip metrics, failure prediction etc. that provide edge to business decision making AND can scale with growth. The role sits in the sweet spot between technology and business worlds AND provides opportunity for growth, high business impact and working with seasoned business leaders. An ideal candidate will be someone with sound technical background in data domain ‚Äì storage / processing / analytics, has solid business acumen and a strong automation / solution oriented thought process. Will be a self-starter who can start with a business problem and work backwards to conceive & devise best possible solution. Is a great communicator and at ease on partnering with business owners and other internal / external teams. Can explore newer technology options, if need be, and has a high sense of ownership over every deliverable by the team. Is constantly obsessed with customer delight & business impact / end result and ‚Äògets it done‚Äô in business time. Key job responsibilities Design, implement and support an data infrastructure for analytics needs of large organization Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies Be enthusiastic about building deep domain knowledge about Amazon‚Äôs business. Must possess strong verbal and written communication skills, be self-driven, and deliver high quality results in a fast-paced environment. Enjoy working closely with your peers in a group of very smart and talented engineers. Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency About The Team India Data Engineering and Analytics (IDEA) team is central data engineering team for Amazon India. Our vision is to simplify and accelerate data driven decision making for Amazon India by providing cost effective, easy & timely access to high quality data. We achieve this by building UDAI (Unified Data & Analytics Infrastructure for Amazon India) which serves as a central data platform and provides data engineering infrastructure, ready to use datasets and self-service reporting capabilities. Our core responsibilities towards India marketplace include a) providing systems(infrastructure) & workflows that allow ingestion, storage, processing and querying of data b) building ready-to-use datasets for easy and faster access to the data c) automating standard business analysis / reporting/ dashboarding d) empowering business with self-service tools for deep dives & insights seeking. Basic Qualifications 1+ years of data engineering experience Experience with SQL Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Preferred Qualifications Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Knowledge of AWS Infrastructure Knowledge of basics of designing and implementing a data schema like normalization, relational model vs dimensional model Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - ADCI - Karnataka Job ID: A2984267",,,"Python, SQL",
4219963395,Data Engineer,Atlassian,"Bengaluru, Karnataka, India (Remote)",Remote,Full-time,,"About the job Overview Working at Atlassian Atlassians can choose where they work ‚Äì whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company. Responsibilities As a data engineer, you will have the opportunity to apply your strong technical experience building highly reliable data products. You enjoy working in an agile environment. You are able to translate raw requirements into solid solutions. You are motivated by solving challenging problems, where creativity is as crucial as your ability to write code and test cases. On a typical day you will help our partner teams ingest data faster into our data lake, you‚Äôll find ways to make our data products more efficient, or come up with ideas to help build self-serve data engineering within the company. Then you will move on to building micro-services, architecting, designing, and promoting self serve capabilities at scale to help Atlassian grow. Qualifications On your first day, we'll expect you to have: At least 3+ years of professional experience as a software engineer or data engineer A BS in Computer Science or equivalent experience Strong programming skills (some combination of Python, Java, and Scala) Experience writing SQL, structuring data, and data storage practices Experience with data modeling Knowledge of data warehousing concepts Experienced building data pipelines and micro services Experience with Spark, Airflow and other streaming technologies to process incredible volumes of streaming data A willingness to accept failure, learn and try again An open mind to try solutions that may seem impossible at first Experience in working on Amazon Web Services (in particular using EMR, Kinesis, RDS, S3, SQS and the like), and databricks. It's Preferred, But Not Technically Required, That You Have Experience building self-service tooling and platforms Built and designed Kappa architecture platforms A passion for building and running continuous integration pipelines. Built pipelines using Databricks and well versed with their API‚Äôs Contributed to open source projects (Ex: Operators in Airflow) Our Perks & Benefits Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more. About Atlassian At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together. We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines. To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them. To learn more about our culture and hiring process, visit go.atlassian.com/crh .",,,"Python, SQL",
4250269101,Data Engineer,Adroit Innovative Solutions Inc,India (Remote),Remote,Contract,,"About the job We‚Äôre Hiring: Data Engineer About The Job Duration: 12 Months Location: PAN INDIA Timings: Full Time (As per company timings) Notice Period: within 15 days or immediate joiner Experience: 0- 2 years Responsibilities Job Description Design, develop and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views. Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality) components Support and troubleshoot the data environment (including periodically on call) Document technical artifacts for developed solutions Good interpersonal skills; comfort and competence in dealing with different teams within the organization. Requires an ability to interface with multiple constituent groups and build sustainable relationships Versatile, creative temperament, ability to think out-of-the box while defining sound and practical solutions. Ability to master new skills Proactive approach to problem solving with effective influencing skills Familiar with Agile practices and methodologies Education And Experience Requirements Four-year degree in Information Systems, Finance / Mathematics, Computer Science or similar 0-2 years of experience in Data Engineering REQUIRED KNOWLEDGE, SKILLS Or ABILITIES Advanced SQL queries, scripts, stored procedures, materialized views, and views Focus on ELT to load data into database and perform transformations in database Ability to use analytical SQL functions Snowflake experience a plus Cloud Data Warehouse solutions experience (Snowflake, Azure DW, or Redshift); data modelling, analysis, programming Experience with DevOps models utilizing a CI/CD tool Work in hands-on Cloud environment in Azure Cloud Platform (ADLS, Blob) Talend, Apache Airflow, Azure Data Factory, and BI tools like Tableau preferred Analyse data models We are looking for a Senior Data Engineer for the Enterprise Data Organization to build and manage data pipeline (Data ingest, data transformation, data distribution, quality rules, data storage etc.) for Azure cloud-based data platform. The candidate will require to possess strong technical, analytical, programming and critical thinking skills.",,,"SQL, Tableau",
4232304548,Data Engineer,Amazon,"Bengaluru, Karnataka, India",,Full-time,,"About the job Description We are seeking an experienced and highly skilled Data Engineer to join our team in India. As a Level 5 Data Engineer, you will play a crucial role in designing, implementing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to deliver scalable and efficient data solutions that drive business value. Key job responsibilities Lead the design and implementation of complex data pipelines and ETL processes to support our analytical and operational needs. Architect and develop scalable, high-performance data systems using cloud technologies and big data platforms. Collaborate with data scientists, analysts, and business stakeholders to understand data requirements and implement appropriate solutions. Optimize data storage and retrieval systems for improved performance and cost-effectiveness. Implement data quality checks and monitoring systems to ensure data integrity and reliability. Mentor junior engineers and provide technical leadership on data engineering best practices and methodologies. Evaluate and recommend new technologies and tools to enhance our data infrastructure capabilities. Contribute to the development of data governance policies and procedures. Troubleshoot and resolve complex data-related issues in production environments. About The Team We are a dynamic team within Amazon's Selling Partner Services organization, focused on building and scaling API authorization and customization systems that empower thousands of global selling partners. Our mission is to create flexible, reliable, and extensible API solutions that help businesses thrive on Amazon's platform. Our team combines the excitement of a startup with the resources and scale of Amazon. We offer an environment where you can make a significant impact while working with new technologies and talented professionals who are passionate about building world-class solutions. Basic Qualifications 3+ years of data engineering experience 4+ years of SQL experience Experience with data modeling, warehousing and building ETL pipelines Knowledge of distributed systems as it pertains to data storage and computing Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - ADCI - Karnataka Job ID: A2985253",,,SQL,
4236631734,Data Engineer,Ubique Systems,India (Remote),Remote,Full-time,,"About the job Ubique Systems is hiring. Location- Work From Home (Remote) Experience: 3-5 years Role: Data Engineer Type: Contract Job Description: Data Pipelines: Proven experience in building scalable and reliable data pipelines BigQuery: Expertise in writing complex SQL transformations; hands-on with indexing and performance optimization Ingestion: Skilled in data scraping and ingestion through RESTful APIs and file-based sources Orchestration: Familiarity with orchestration tools like Prefect, Apache Airflow (nice to have) Tech Stack: Proficient in Python, FastAPI, and PostgreSQL End-to-End Workflows: Capable of owning ingestion, transformation, and delivery processes Interested? Kindly share your CV with siddhi.divekar@ubique-systems.com",Manager,,"Python, SQL",
4247365428,Data Engineer,Jio,"Gurgaon, Haryana, India (On-site)",On-site,Full-time,,"About the job Skills: SQL, Spark, Spark sql, spark streaming, Azure, AWS, Kafka, NoSQL, Role : Data Engineer Location : Gurgaon/Bangalore/Mumbai Years of Experience : 5-12 Years Must Have: SQL, Spark, Spark sql, spark streaming, Scala/Python, Azure/AWS, Kafka Good to Have : NoSQL (Mongo/Cassandra), Neo4J, Prometheus, Flink, Iceberg Company Overview Jio stands at the forefront of India's telecom industry, boasting over 400 million customers. Beyond telecom, Jio powers a vast array of digital apps and services, offering solutions for both B2C and B2B markets. With cutting-edge technology, including 5G solutions, AI/ML platforms, cloud-native systems, and BSS solutions, Jio revolutionizes media and telecommunications. Headquartered in Navi Mumbai, Maharashtra, Jio operates with more than 10001 employees across India. For more information, visit Jio Platforms. Job Overview Jio is seeking a skilled Data Engineer for a Mid-Level position, based in Bangalore, Mumbai, or Gurgaon. This full-time role involves designing, building, and maintaining scalable data pipelines and systems. The ideal candidate will have a strong background in data technologies and platforms, contributing to advanced data solutions and AI-driven insights across Jios telecom and digital service frameworks. Qualifications And Skills Proficiency in SQL (Mandatory skill): Essential for querying, manipulating, and analyzing data within relational databases. Experience with spark streaming (Mandatory skill): Crucial for processing real-time data streams efficiently within big data solutions. Understanding of NoSQL databases (Mandatory skill): Vital for handling unstructured or semi-structured data formats in data-intensive applications. Experience in Spark: Key for large-scale data processing and analytical tasks within Jio's extensive data ecosystem. Proficiency in Spark SQL: Important for leveraging Spark's SQL interface to perform complex analytical queries effectively. Familiarity with Azure: Necessary for managing data solutions and storage within Microsofts cloud environment. Understanding of AWS: Essential for leveraging Amazons cloud services for scalable data storage and processing solutions. Experience with Kafka: Important for building real-time data pipelines and streaming applications. Roles And Responsibilities Design, develop, and maintain robust data pipelines for efficient data transformation and integration across platforms. Collaborate with cross-functional teams to gather and analyze data requirements, ensuring alignment with business goals. Implement real-time data streaming solutions using spark streaming, achieving high throughput and low latency data processing. Utilize SQL and NoSQL databases to manage, store, and retrieve large volumes of structured and unstructured data effectively. Optimize data workflows, improving performance and scalability of data-driven applications and reporting solutions. Ensure the security and integrity of data, implementing best practices for data protection and compliance. Participate in the deployment and monitoring of AI-driven analytics platforms, enhancing data-driven decision-making processes. Stay updated with emerging data technologies, recommending innovative solutions to foster continuous improvement. Desired Skills and Experience SQL, Spark, Spark sql, spark streaming, Azure, AWS, Kafka, NoSQL",,,"Python, SQL",
4247363579,Data Engineer,Jio,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Skills: SQL, Spark, Spark sql, spark streaming, Azure, AWS, Kafka, NoSQL, Role : Data Engineer Location : Gurgaon/Bangalore/Mumbai Years of Experience : 5-12 Years Must Have: SQL, Spark, Spark sql, spark streaming, Scala/Python, Azure/AWS, Kafka Good to Have : NoSQL (Mongo/Cassandra), Neo4J, Prometheus, Flink, Iceberg Company Overview Jio stands at the forefront of India's telecom industry, boasting over 400 million customers. Beyond telecom, Jio powers a vast array of digital apps and services, offering solutions for both B2C and B2B markets. With cutting-edge technology, including 5G solutions, AI/ML platforms, cloud-native systems, and BSS solutions, Jio revolutionizes media and telecommunications. Headquartered in Navi Mumbai, Maharashtra, Jio operates with more than 10001 employees across India. For more information, visit Jio Platforms. Job Overview Jio is seeking a skilled Data Engineer for a Mid-Level position, based in Bangalore, Mumbai, or Gurgaon. This full-time role involves designing, building, and maintaining scalable data pipelines and systems. The ideal candidate will have a strong background in data technologies and platforms, contributing to advanced data solutions and AI-driven insights across Jios telecom and digital service frameworks. Qualifications And Skills Proficiency in SQL (Mandatory skill): Essential for querying, manipulating, and analyzing data within relational databases. Experience with spark streaming (Mandatory skill): Crucial for processing real-time data streams efficiently within big data solutions. Understanding of NoSQL databases (Mandatory skill): Vital for handling unstructured or semi-structured data formats in data-intensive applications. Experience in Spark: Key for large-scale data processing and analytical tasks within Jio's extensive data ecosystem. Proficiency in Spark SQL: Important for leveraging Spark's SQL interface to perform complex analytical queries effectively. Familiarity with Azure: Necessary for managing data solutions and storage within Microsofts cloud environment. Understanding of AWS: Essential for leveraging Amazons cloud services for scalable data storage and processing solutions. Experience with Kafka: Important for building real-time data pipelines and streaming applications. Roles And Responsibilities Design, develop, and maintain robust data pipelines for efficient data transformation and integration across platforms. Collaborate with cross-functional teams to gather and analyze data requirements, ensuring alignment with business goals. Implement real-time data streaming solutions using spark streaming, achieving high throughput and low latency data processing. Utilize SQL and NoSQL databases to manage, store, and retrieve large volumes of structured and unstructured data effectively. Optimize data workflows, improving performance and scalability of data-driven applications and reporting solutions. Ensure the security and integrity of data, implementing best practices for data protection and compliance. Participate in the deployment and monitoring of AI-driven analytics platforms, enhancing data-driven decision-making processes. Stay updated with emerging data technologies, recommending innovative solutions to foster continuous improvement. Desired Skills and Experience SQL, Spark, Spark sql, spark streaming, Azure, AWS, Kafka, NoSQL",,,"Python, SQL",
4250586816,Data Engineer,Ericsson,"Noida, Uttar Pradesh, India",,Full-time,,"About the job Grow with us About this opportunity: Ericsson is a leading provider of telecommunications equipment and services to mobile and fixed network operators globally. We are seeking a highly skilled and experienced Data Engineer to join our dynamic team at Ericsson. As a Data Engineer, you will be responsible for leveraging advanced analytics and machine learning techniques to drive actionable insights and solutions for our telecom domain. This role requires a deep understanding of data science methodologies, strong programming skills, and proficiency in cloud-based environments. What you will do: Experience in analyzing complex problems and translate it into algorithms High customer focus with high accountability for delivering high-quality products Very strong skills in software development using Advanced Python Backend development in RestAPIs using Flask, FastAPI Deployment experience with CI/CD pipelines Working knowledge of handling data sets and data pre-processing through PySpark Have experience of GCP cloud Solid communication and presentation skills Furthermore, we believe you are curious, innovative, high own self-drive, and collaborative Writing queries to target Casandra, PostgreSQL database Design Principles in application development The skills you bring: Bachelor's degree in Computer Science, Statistics, Mathematics, or a related field. A Master's degree or PhD is preferred. 2-5 years of experience in Python and in advanced Python preferably within the telecommunications or related industry. Strong programming skills in Python and SQL. Excellent problem-solving skills and ability to work independently as well as part of a team. Strong communication and presentation skills, with the ability to explain complex analytical concepts to non-technical stakeholders. Why join Ericsson? At Ericsson, you¬¥ll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of what¬¥s possible. To build solutions never seen before to some of the world‚Äôs toughest problems. You¬¥ll be challenged, but you won‚Äôt be alone. You¬¥ll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next. What happens once you apply? Click Here to find all you need to know about what our typical hiring process looks like. Encouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more. Primary country and city: India (IN) || Noida Req ID: 765999",,,"Python, SQL, Machine Learning",
4235363570,ML Engineer,Uplers,"Patna, Bihar, India (Remote)",Remote,‚Çπ1M/yr - ‚Çπ1.2M/yr,,"About the job Experience : Fresher Salary : INR 1000000-1200000 / year (based on experience) Expected Notice Period : 30 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Leegality) (*Note: This is a requirement for one of Uplers' client - Leegality) What do you need for this opportunity? Must have skills required: Start-up Experience, Python, Ml frameworks, Statistical analysis, CNNs, Machine Learning, Computer Vision, AI models Leegality is Looking for: What You'll Do As an ML Engineer, you will develop solutions to interesting technical problems, explore exciting growth opportunities, and have a real impact on our product. To ensure success as a machine learning engineer, you should demonstrate solid data science knowledge and experience in a related ML role. A first-class machine learning engineer will be someone whose expertise translates into the enhanced performance of predictive automation software. Responsibilities: Designing machine learning systems and self-running artificial intelligence (AI) software to automate predictive models. Transforming data science prototypes and applying appropriate ML algorithms and tools. Solving complex problems with multi-layered data sets and optimizing existing machine learning libraries and frameworks. Developing ML algorithms to analyze huge volumes of historical data to make predictions. Running tests, performing statistical analysis, and interpreting test results. Exposure to generative AI (LLM) based ML architectures Experienced in working with CNN. Knows how to train and fine-tune CNN-based models. Documenting machine learning processes. Requirements: 1+ years of relevant experience in Machine Learning Engineering. Advanced proficiency with Python. Extensive knowledge of ML frameworks, libraries, data structures, data modeling, and software architecture. Superb analytical and problem-solving abilities. Great communication and collaboration skills. Preferences: Start-up experience is good to have. Willing to join immediately, ideally. Female candidates are preferred Interview Process - Technical Round 1 - ML Fundamentals, NLP models Assessment Technical Round 2 - Discussion on the assignment submitted & modify it according to the requirements Round 3 - with the Tech Lead - Tech Managerial - Past projects & in-depth knowledge of ML How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Start-up Experience, Python, Ml frameworks, Statistical analysis, CNNs, Machine Learning, Computer Vision, AI models",Manager,,"Python, Machine Learning",
4235862397,Data Engineer,Tata Consultancy Services,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Role- Data Engineer Skills- ADF, ETL, SQL, Python, PowerShell Location- Noida, Pune, BLR, Kolkata, Hyd Desired Competencies (Technical/Behavioral Competency) Must-Have** ETL, Azure Data Factory, SSRS, MS Fabric, Python, Powershell Good-to-Have SN Responsibility of / Expectations from the Role 1 Azure Data Engineer 2 Develop full SDLC project plans to implement ETL solution and identify resource requirements, Good Knowledge of SQL server complex queries, joins, etc. 3 Rest API, ADF pipeline, MS Fabric 4 SSIS and Azure Data Factory based ETL architecture. 5 Good exposure in Client Communication and supporting requests from customer",,,"Python, SQL",
4191525908,DATA Engineer,Lenovo,"Bangalore Urban, Karnataka, India",,Full-time,,"About the job We are Lenovo. We do what we say. We own what we do. We WOW our customers. Lenovo is a US$57 billion revenue global technology powerhouse, ranked #248 in the Fortune Global 500, and serving millions of customers every day in 180 markets. Focused on a bold vision to deliver Smarter Technology for All, Lenovo has built on its success as the world‚Äôs largest PC company with a full-stack portfolio of AI-enabled, AI-ready, and AI-optimized devices (PCs, workstations, smartphones, tablets), infrastructure (server, storage, edge, high performance computing and software defined infrastructure), software, solutions, and services. Lenovo‚Äôs continued investment in world-changing innovation is building a more equitable, trustworthy, and smarter future for everyone, everywhere. Lenovo is listed on the Hong Kong stock exchange under Lenovo Group Limited (HKSE: 992) (ADR: LNVGY). This transformation together with Lenovo‚Äôs world-changing innovation is building a more inclusive, trustworthy, and smarter future for everyone, everywhere. To find out more visit www.lenovo.com , and read about the latest news via our StoryHub . BS/BA in Computer Science, Mathematics, Statistics, MIS, or related At least 5 years' experience in the data warehouse space. At least 5 years' experience in custom Extract Transform and Load (ETL)/ Extract Load and Transform (ELT) design, implementation and maintenance. At least 5 years' experience in writing SQL statements. At least 3 years' experience with Cloud based data platform technologies such as Google Big Query, or Azure/Snowflake data platform equivalent. Ability in managing and communicating data warehouse plans to internal clients. We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, religion, sexual orientation, gender identity, national origin, status as a veteran, and basis of disability or any federal, state, or local protected class.",,,SQL,
4236816506,Data Engineer,LTIMindtree,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job We are seeking an experienced and strategic Data to design, build, and optimize scalable, secure, and high-performance data solutions. You will play a pivotal role in shaping our data infrastructure, working with technologies such as Databricks, Azure Data Factory, Unity Catalog , and Spark , while aligning with best practices in data governance, pipeline automation , and performance optimization . Key Responsibilities: ‚Ä¢ Design and develop scalable data pipelines using Databricks and Medallion (Bronze, Silver, Gold layers). ‚Ä¢ Architect and implement data governance frameworks using Unity Catalog and related tools. ‚Ä¢ Write efficient PySpark and SQL code for data transformation, cleansing, and enrichment. ‚Ä¢ Build and manage data workflows in Azure Data Factory (ADF) including triggers, linked services, and integration runtimes. ‚Ä¢ Optimize queries and data structures for performance and cost-efficiency . ‚Ä¢ Develop and maintain CI/CD pipelines using GitHub for automated deployment and version control. ‚Ä¢ Collaborate with cross-functional teams to define data strategies and drive data quality initiatives. ‚Ä¢ Implement best practices for DevOps, CI/CD , and infrastructure-as-code in data engineering. ‚Ä¢ Troubleshoot and resolve performance bottlenecks across Spark, ADF, and Databricks pipelines. ‚Ä¢ Maintain comprehensive documentation of architecture, processes, and workflows . Requirements: ‚Ä¢ Bachelor‚Äôs or master‚Äôs degree in computer science, Information Systems, or related field. ‚Ä¢ Proven experience as a Data Architect or Senior Data Engineer. ‚Ä¢ Strong knowledge of Databricks , Azure Data Factory , Spark (PySpark) , and SQL . ‚Ä¢ Hands-on experience with data governance , security frameworks , and catalog management . ‚Ä¢ Proficiency in cloud platforms (preferably Azure). ‚Ä¢ Experience with CI/CD tools and version control systems like GitHub. ‚Ä¢ Strong communication and collaboration skills.",,,SQL,
4160398989,Data Engineer | PAN India,Capgemini,Greater Kolkata Area (Remote),Remote,Full-time,,"About the job Capgemini Invent Capgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build what‚Äôs next for their businesses. Your role Develop and maintain data pipelines tailored to Azure environments, ensuring security and compliance with client data standards. Collaborate with cross-functional teams to gather data requirements, translate them into technical specifications, and develop data models. Leverage Python libraries for data handling, enhancing processing efficiency and robustness. Ensure SQL workflows meet client performance standards and handle large data volumes effectively. Build and maintain reliable ETL pipelines, supporting full and incremental loads and ensuring data integrity and scalability in ETL processes. Implement CI/CD pipelines for automated deployment and testing of data solutions. Optimize and tune data workflows and processes to ensure high performance and reliability. Monitor, troubleshoot, and optimize data processes for performance and reliability. Document data infrastructure, workflows, and maintain industry knowledge in data engineering and cloud tech. Your Profile Bachelor‚Äôs degree in computer science, Information Systems, or a related field 4+ years of data engineering experience with a strong focus on Azure data services for client-centric solutions. Extensive expertise in Azure Synapse, Data Lake Storage, Data Factory, Databricks, and Blob Storage, ensuring secure, compliant data handling for clients. Good interpersonal communication skills Skilled in designing and maintaining scalable data pipelines tailored to client needs in Azure environments. Proficient in SQL and PL/SQL for complex data processing and client-specific analytics. What You Will Love About Working Here We recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance. At the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities. Equip yourself with valuable certifications in the latest technologies such as Generative AI. About Capgemini Capgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of ‚Ç¨22.5 billion.",,,"Python, SQL",
4245420494,Data Engineer-Data Platforms,IBM,"Gurgaon, Haryana, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you will work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. A career in IBM Consulting embraces long-term relationships and close collaboration with clients across the globe. You will collaborate with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio, including IBM Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you will be supported by mentors and coaches who will encourage you to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground-breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and learning opportunities in an environment that embraces your unique skills and experience Your Role And Responsibilities A Data Engineer specializing in enterprise data platforms, experienced in building, managing, and optimizing data pipelines for large-scale environments. Having expertise in big data technologies, distributed computing, data ingestion, and transformation frameworks. Proficient in Apache Spark, PySpark, Kafka, and Iceberg tables, and understand how to design and implement scalable, high-performance data processing solutions.What you‚Äôll do: As a Data Engineer ‚Äì Data Platform Services, responsibilities include: Data Ingestion & Processing Designing and developing data pipelines to migrate workloads from IIAS to Cloudera Data Lake. Implementing streaming and batch data ingestion frameworks using Kafka, Apache Spark (PySpark). Working with IBM CDC and Universal Data Mover to manage data replication and movement. Big Data & Data Lakehouse Management Implementing Apache Iceberg tables for efficient data storage and retrieval. Managing distributed data processing with Cloudera Data Platform (CDP). Ensuring data lineage, cataloging, and governance for compliance with Bank/regulatory policies. Optimization & Performance Tuning Optimizing Spark and PySpark jobs for performance and scalability. Implementing data partitioning, indexing, and caching to enhance query performance. Monitoring and troubleshooting pipeline failures and performance bottlenecks. Security & Compliance Ensuring secure data access, encryption, and masking using Thales CipherTrust. Implementing role-based access controls (RBAC) and data governance policies. Supporting metadata management and data quality initiatives. Collaboration & Automation Working closely with Data Scientists, Analysts, and DevOps teams to integrate data solutions. Automating data workflows using Airflow and implementing CI/CD pipelines with GitLab and Sonatype Nexus. Supporting Denodo-based data virtualization for seamless data access Preferred Education Master's Degree Required Technical And Professional Expertise 4-7 years of experience in big data engineering, data integration, and distributed computing. Strong skills in Apache Spark, PySpark, Kafka, SQL, and Cloudera Data Platform (CDP). Proficiency in Python or Scala for data processing. Experience with data pipeline orchestration tools (Apache Airflow, Stonebranch UDM). Understanding of data security, encryption, and compliance frameworks Preferred Technical And Professional Experience Experience in banking or financial services data platforms. Exposure to Denodo for data virtualization and DGraph for graph-based insights. Familiarity with cloud data platforms (AWS, Azure, GCP). Certifications in Cloudera Data Engineering, IBM Data Engineering, or AWS Data Analytics",,,"Python, SQL",
4245306405,Data Engineer,Tata Consultancy Services,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Greetings from Tata Consultancy Services Join the Walk-in Drive on 14th June 2025 and Pave your path to value with TCS AI Cloud Team We are Hiring for Below Skills Walk In Drive Date: 14th June Registration Time: 09:30 AM ‚Äì 12:30 PM Venue:TCS Blore Azure DataBricks Engineer: https://lnkd.in/gTsJJu7w 3+ years ETL /Data Analysis experience with a reputed firm ¬∑ ¬∑ Expertise in Big Data Managed Platform Environment like Databricks using Python/ PySpark/ SparkSQL ¬∑ Experience in handling large data volumes and orchestrating automated ETL/ data pipelines using CI/CD and Cloud Technologies. ¬∑ Experience of deploying ETL / data pipelines and workflows in cloud technologies and architecture such as Azure and Amazon Web Services will be valued ¬∑ Experience in Data modelling (e.g., database structure, entity relationships, UID etc.) , data profiling, data quality validation. ¬∑ Experience adopting software development best practices (e.g., modularization, testing, refactoring, etc.) ¬∑ Conduct data assessment, perform data quality checks and transform data using SQL and ETL tools",,,"Python, SQL, Data Analysis",
4233217608,Data Engineer,HARMAN India,"Bengaluru, Karnataka, India",,Full-time,,"About the job Job Description Introduction: A Career at HARMAN Digital Transformation Solutions (DTS) We‚Äôre a global, multi-disciplinary team that‚Äôs putting the innovative power of technology to work and transforming tomorrow. At HARMAN DTS, you solve challenges by creating innovative solutions. Combine the physical and digital, making technology a more dynamic force to solve challenges and serve humanity‚Äôs needs Work at the convergence of cross channel UX, cloud, insightful data, IoT and mobility Empower companies to create new digital business models, enter new markets, and improve customer experiences What You Will Do As the Infor M3 ERP Senior Manufacturing Consultant, you will be responsible to implement the Infor M3 (V15.x ) ERP system and related business solutions for our global customers. Functional Responsibilities Develop and execute approaches to analyze variety of Healthcare and Life sciences datasets, primarily creating reports on exploratory data analysis and statistical reports Strong proficiency with manipulating data with PL-SQL ( ORACLE 11g) A high comfort level with data manipulation and extraction of meaningful insights from large data and prior experience of working with IMS Health data assets or Patient level data (APLD) data is a plus. Experience with SQL/SAS/WPS is acceptable. What You Need Minimum 3 to 5 years of experience of working with pharmaceutical databases, including patient level transactional data, Claim analytics is preferred. Being creative in identifying new techniques and processes to streamline and increase efficiency and effectiveness of current work-streams is a plus. High level of attention to detail and problem solving Exposure on various Healthcare data sources like SHS, IQVIA, DRG , Labcorp, Flatiron Experion ‚Ä¶etc Have strong experience claims data and worked on various patient level data analytics like Adherence Studites, Line of therapy and Treatment Path analysis. Experienced on HEOR studies. Preference will be given to applicants with a demonstrated ability to work independently, take initiative, and manage responsibilities on multiple projects simultaneously. Professional requirement Post graduate degree in Economics, MBA, Statistics, Mathematics, Operations Research, Quantitative Analysis, or related field Well-organized, capable of handling several projects at a time while meeting deadlines What Makes You Eligible 3-5 years of consolidated work experience in Analytic Industry. Strong statistical and quantitative analysis skills. Knowledge of Statistical Analysis for Advanced knowledge of PL/SQL and experience with statistical packages such as R/SAS Strong problem solving skills Excellent Data Interpretation Skills. Experience In Using Charting/reporting Skilled in MS-Office, specifically Excel and Powerpoint Should have strong knowledge of Healthcare domain OR any specific knowledge on Retail and FMCG/CPG industry will be an added advantage. Knowledge on Pharmaceutical Rx claims/ Medical claims will be an added advantage Intermediate to advanced proficiency in Excel and VBA Automate data analysis in support of the client office‚Äôs principals and consultants Utilize SAS/WPS/R to create and automate analytical and modeling processes Preference will be given to applicants with a demonstrated ability to work independently, take initiative, and manage responsibilities on multiple projects simultaneously What We Offer Access to employee discounts on world class HARMAN/Samsung products (JBL, Harman Kardon, AKG etc.) Professional development opportunities through HARMAN University‚Äôs business and leadership academies An inclusive and diverse work environment that fosters and encourages professional and personal development. ‚ÄúBe Brilliant‚Äù employee recognition and rewards program. You Belong Here HARMAN is committed to making every employee feel welcomed, valued, and empowered. No matter what role you play, we encourage you to share your ideas, voice your distinct perspective, and bring your whole self with you ‚Äì all within a support-minded culture that celebrates what makes each of us unique. We also recognize that learning is a lifelong pursuit and want you to flourish. We proudly offer added opportunities for training, development, and continuing education, further empowering you to live the career you want. About HARMAN: Where Innovation Unleashes Next-Level Technology Ever since the 1920s, we‚Äôve been amplifying the sense of sound. Today, that legacy endures, with integrated technology platforms that make the world smarter, safer, and more connected. Across automotive, lifestyle, and digital transformation solutions, we create innovative technologies that turn ordinary moments into extraordinary experiences. Our renowned automotive and lifestyle solutions can be found everywhere, from the music we play in our cars and homes to venues that feature today‚Äôs most sought-after performers, while our digital transformation solutions serve humanity by addressing the world‚Äôs ever-evolving needs and demands. Marketing our award-winning portfolio under 16 iconic brands, such as JBL, Mark Levinson, and Revel, we set ourselves apart by exceeding the highest engineering and design standards for our customers, our partners and each other. If you‚Äôre ready to innovate and do work that makes a lasting impact, join our talent community today !",,,"SQL, Excel, R, Data Analysis",
4250589249,Data Engineer,Ericsson,Greater Kolkata Area,,Full-time,,"About the job Grow with us About this opportunity: Ericsson is a leading provider of telecommunications equipment and services to mobile and fixed network operators globally. We are seeking a highly skilled and experienced Data Engineer to join our dynamic team at Ericsson. As a Data Engineer, you will be responsible for leveraging advanced analytics and machine learning techniques to drive actionable insights and solutions for our telecom domain. This role requires a deep understanding of data science methodologies, strong programming skills, and proficiency in cloud-based environments. What you will do: Experience in analyzing complex problems and translate it into algorithms High customer focus with high accountability for delivering high-quality products Very strong skills in software development using Advanced Python Backend development in RestAPIs using Flask, FastAPI Deployment experience with CI/CD pipelines Working knowledge of handling data sets and data pre-processing through PySpark Have experience of GCP cloud Solid communication and presentation skills Furthermore, we believe you are curious, innovative, high own self-drive, and collaborative Writing queries to target Casandra, PostgreSQL database Design Principles in application development The skills you bring: Bachelor's degree in Computer Science, Statistics, Mathematics, or a related field. A Master's degree or PhD is preferred. 2-5 years of experience in Python and in advanced Python preferably within the telecommunications or related industry. Strong programming skills in Python and SQL. Excellent problem-solving skills and ability to work independently as well as part of a team. Strong communication and presentation skills, with the ability to explain complex analytical concepts to non-technical stakeholders. Why join Ericsson? At Ericsson, you¬¥ll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of what¬¥s possible. To build solutions never seen before to some of the world‚Äôs toughest problems. You¬¥ll be challenged, but you won‚Äôt be alone. You¬¥ll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next. What happens once you apply? Click Here to find all you need to know about what our typical hiring process looks like. Encouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more. Primary country and city: India (IN) || Noida Req ID: 765999",,,"Python, SQL, Machine Learning",
4249251405,Data Engineer,ShyftLabs,"Noida, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Position Overview We are looking for an experienced Data Engineer to join our dynamic team. If you are passionate about building scalable software solutions, and work collaboratively with cross-functional teams to define requirements and deliver solutions we would love to hear from you. ShyftLabs is a growing data product company that was founded in early 2020 and works primarily with Fortune 500 companies. We deliver digital solutions built to help accelerate the growth of businesses in various industries, by focusing on creating value through innovation. Job Responsibilities: Develop and maintain data pipelines and ETL/ELT processes using Python Design and implement scalable, high-performance applications Work collaboratively with cross-functional teams to define requirements and deliver solutions Develop and manage near real-time data streaming solutions using Pub, Sub or Beam Contribute to code reviews, architecture discussions, and continuous improvement initiatives Monitor and troubleshoot production systems to ensure reliability and performance Basic Qualifications: 5+ years of professional software development experience with Python Strong understanding of software engineering best practices (testing, version control, CI/CD) Experience building and optimizing ETL/ELT processes and data pipelines Proficiency with SQL and database concepts Experience with data processing frameworks (e.g., Pandas) Understanding of software design patterns and architectural principles Ability to write clean, well-documented, and maintainable code Experience with unit testing and test automation Experience working with any cloud provider (GCP is preferred) Experience with CI/CD pipelines and Infrastructure as code Experience with Containerization technologies like Docker or Kubernetes Bachelor's degree in Computer Science, Engineering, or related field (or equivalent experience) Proven track record of delivering complex software projects Excellent problem-solving and analytical thinking skills Strong communication skills and ability to work in a collaborative environment Preferred Qualifications: Experience with GCP services, particularly Cloud Run and Dataflow Experience with stream processing technologies (Pub/Sub) Familiarity with big data technologies (Airflow) Experience with data visualization tools and libraries Knowledge of CI/CD pipelines with Gitlab and infrastructure as code with Terraform Familiarity with platforms like Snowflake, Bigquery or Databricks, GCP Data engineer certification We are proud to offer a competitive salary alongside a strong insurance package. We pride ourselves on the growth of our employees, offering extensive learning and development resources.",,,"Python, SQL",
4207928464,Senior Data Engineer II,MakeMyTrip,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Position: Senior Data Engineer II Experience: Must have 4+ years of experience About Role: We are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences. About us: Nurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India‚Äôs online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010. Post-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018. GO-MMT is the corporate entity of three giants in the Online Travel Industry‚ÄîGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry. About the team: MakeMyTrip as India‚Äôs leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs. Data Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement. There are two key responsibilities of Data Engineering team: One to develop the platform for data capture, storage, processing, serving and querying. Second is to develop data products starting from; o personalization & recommendation platform o customer segmentation & intelligence o data insights engine for persuasions and o the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users We developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences. Technology experience : Extensive experience working with large data sets with hands-on technology skills to design and build robust data architecture Extensive experience in data modeling and database design At least 4+ years of hands-on experience in PySpark/BigData Tech stack Stream processing engines ‚Äì Spark Structured Streaming Analytical processing on Big Data using Spark At least 4+ years of experience in Python/Scala Hands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems At least 4+ years of cloud deployment experience ‚Äì AWS | Azure | Google Cloud Platform At least 4+ product deployments of big data technologies ‚Äì Business Data Lake, NoSQL databases etc Awareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies Should have experience in architecting and implementing domain centric big data solutions Ability to frame architectural decisions and provide technology leadership & direction Excellent problem solving, hands-on engineering, and communication skills",,,"Python, SQL, Machine Learning",
4159976331,Data Engineer with data science,Adobe,"Bengaluru, Karnataka, India",,Full-time,,"About the job Our Company Changing the world through digital experiences is what Adobe‚Äôs all about. We give everyone‚Äîfrom emerging artists to global brands‚Äîeverything they need to design and deliver exceptional digital experiences! We‚Äôre passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen. We‚Äôre on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours! Position Summary A data scientist cum data engineer with 5+ years of experience with a deep understanding of data analysis, Big Data & Cloud technologies, AI & machine learning, GenAI and NLP techniques. You will be responsible for developing & building low level design as per approved Tech specs, extracting valuable insights from large datasets, building advanced AIML models and driving data-driven decision-making within the organization or for the enterprise customers. This role involves a combination of data analysis, model development, and strategic thinking to solve complex business problems. You will interact with key stakeholders and apply your technical proficiency in AIML, Python, R and algorithms. You will work across different stages of the development project using data science & technologies to provide solutions and interface directly with enterprise customers for the Adobe Experience Platform. What You‚Äôll Do Interface with Adobe customers to gather requirements, map solutions & make recommendations. Document projects with clear business objectives, provides data gathering & data preparation, final algorithm, detailed set of results Experience in Natural Language Processing, GenAI and Image processing Support the execution of Data Science solutions to business problems Innovate on new ideas to solve customer needs & assist to create GTM strategies for new solutions Experience in AIML modeling like propensity models, clusterings, regression, etc. Developing ETL pipelines involving big data. Developing data processing\analytics applications primarily using PySpark. Experience of developing applications on cloud(AWS/Azure/GCP) mostly using services related to storage, compute, ETL, DWH, Analytics and streaming. Clear understanding and ability to implement distributed storage, processing and scalable applications. Experience in working with SQL and NoSQL database. Ability to write and analyze SQL, HQL and other query languages for NoSQL databases. Proficiency in writing disitributed & scalable data processing code using PySpark, Python and related libraries. Experience in developing applications that consume the services exposed as ReST APIs. Understand and clean datasets, interpret outputs and make them comprehensible to understand for other teams. Strong collaboration with consultants onshore & offshore Create reusable statistical models & modify existing algorithms Report on customer trends and deployment performance and identify areas that we can use target using ML/Data science solutions. Requirements 2+ years of experience & knowledge with Web Analytics or Digital Marketing. 5+ years of experience in AIML role, with a focus on building data pipelines for conducting data intensive analysis 5+ years of experience with Machine Learning and alogrithims for classification, clustering, prediction, recommendations and NLP 5+ years of experience with common Data Science Toolkits (i.e. R, Jupyter Notebooks, PySpark etc.) 5+ years of enterprise development using Python Strong understanding of GenAI and LLM Ability to enhance Standard Algorithms is required Knowledge & experience using Amazon Sagemaker, Microsoft Azure ML or Google AI technologies 5+ years of complex SQL experience 5+ years of Data Modeling experience Demonstrate exceptional organizational skills and ability to multi-task simultaneous different customer projects Strong verbal & written communication skills to lead customers to a successful outcome and explain complex technical concepts to non-technical stakeholders. Excellent problem-solving and critical-thinking skills. Experience with big data technologies and cloud platforms (e.g., AWS, Azure, Google Cloud) Must be self-managed, proactive and customer focused Degree in Computer Science, Information Systems or related field Should be able to work in teams Special Consideration given for Experience & knowledge with Adobe Experience Cloud solutions Experience & knowledge with Web Analytics or Digital Marketing Adobe is proud to be an Equal Employment Opportunity employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more about our vision here. Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.",,,"Python, SQL, R, Machine Learning, Data Analysis",
4233432557,Data Engineer,Barclays,"Pune, Maharashtra, India",,Full-time,,"About the job Join us as a Data Engineer at Barclays, where you will be responsible for supporting the successful delivery of location strategy projects to plan, budget, agreed quality and governance standards. You'll spearhead the evolution of our digital landscape, driving innovation and excellence. You will harness cutting-edge technology to revolutionise our digital offerings, ensuring unparalleled customer experiences. To be successful as a Data Engineer you should have experience with: Ab>Initio SQL UNIX BigData Hadoop/Hive Some Other Highly Valued Skills May Include Python AWS/Cloud Platform You may be assessed on the key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen strategic thinking and digital and technology, as well as job-specific technical skills. This role is for Pune Location. Purpose of the role To build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure. Accountabilities Build and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data. Design and implementation of data warehoused and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures. Development of processing and analysis algorithms fit for the intended data complexity and volumes. Collaboration with data scientist to build and deploy machine learning models. Analyst Expectations To perform prescribed activities in a timely manner and to a high standard consistently driving continuous improvement. Requires in-depth technical knowledge and experience in their assigned area of expertise Thorough understanding of the underlying principles and concepts within the area of expertise They lead and supervise a team, guiding and supporting professional development, allocating work requirements and coordinating team resources. If the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L ‚Äì Listen and be authentic, E ‚Äì Energise and inspire, A ‚Äì Align across the enterprise, D ‚Äì Develop others. OR for an individual contributor, they develop technical expertise in work area, acting as an advisor where appropriate. Will have an impact on the work of related teams within the area. Partner with other functions and business areas. Takes responsibility for end results of a team‚Äôs operational processing and activities. Escalate breaches of policies / procedure appropriately. Take responsibility for embedding new policies/ procedures adopted due to risk mitigation. Advise and influence decision making within own area of expertise. Take ownership for managing risk and strengthening controls in relation to the work you own or contribute to. Deliver your work and areas of responsibility in line with relevant rules, regulation and codes of conduct. Maintain and continually build an understanding of how own sub-function integrates with function, alongside knowledge of the organisations products, services and processes within the function. Demonstrate understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function. Make evaluative judgements based on the analysis of factual information, paying attention to detail. Resolve problems by identifying and selecting solutions through the application of acquired technical experience and will be guided by precedents. Guide and persuade team members and communicate complex / sensitive information. Act as contact point for stakeholders outside of the immediate function, while building a network of contacts outside team and external to the organisation. All colleagues will be expected to demonstrate the Barclays Values of Respect, Integrity, Service, Excellence and Stewardship ‚Äì our moral compass, helping us do what we believe is right. They will also be expected to demonstrate the Barclays Mindset ‚Äì to Empower, Challenge and Drive ‚Äì the operating manual for how we behave.",,,"Python, SQL, Machine Learning",
4211918900,Data Engineer,HP,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Job Summary We are looking for a Senior Analytics Engineer to drive data excellence and innovation in our organization. As a thought leader in data engineering and analytics principles , you will be responsible for designing, building, and optimizing our data infrastructure while ensuring cost efficiency, security, and scalability . You will play a crucial role in managing Databricks and AWS usage , ensuring budget adherence, and taking proactive measures to optimize costs. This role also requires expertise in ETL processes, large-scale data processing, analytics, and data-driven decision-making , along with strong analytical and leadership skills. Responsibilities Act as a thought leader in data engineering and analytics, driving best practices and standards. Oversee cost management of Databricks and AWS, ensuring resource usage stays within allocated budgets and taking corrective actions when necessary. Design, implement, and optimize ETL pipelines for incremental data loading, ensuring seamless data ingestion, transformation, and performance tuning. Lead migration activities, ensuring smooth transitions while maintaining data integrity and availability. Handle massive data loads efficiently, optimizing storage, compute usage, and query performance. Adhere to Git principles for version control, ensuring best practices for collaboration and deployment. Implement and manage DSR (Airflow) workflows to automate and schedule data pipelines efficiently. Ensure data security and compliance, especially when handling PII data, aligning with regulations like GDPR and HIPAA. Optimize query performance and data storage strategies to improve cost efficiency and speed of analytics. Collaborate with data analysts and business stakeholders to enhance analytics capabilities, enabling data-driven decision-making. Develop and maintain dashboards, reports, and analytical models to provide actionable insights for business and engineering teams. Required Skills & Qualifications Four-year or Graduate Degree in Computer Science, Information Systems, or any other related discipline or commensurate work experience or demonstrated competence. 6-11 years of experience in Data Engineering, Analytics, Big Data, or related domains. Strong expertise in Databricks, AWS (S3, EC2, Lambda, RDS, Redshift, Glue, etc.), and cost optimization strategies. Hands-on experience with ETL pipelines, incremental data loads, and large-scale data processing. Proven experience in analyzing large datasets, deriving insights, and optimizing data workflows. Strong knowledge of SQL, Python, PySpark, and other data engineering and analytics tools. Strong problem-solving, analytical, and leadership skills. Experience with BI tools like Tableau, Looker, or Power BI for data visualization and reporting. Preferred Certifications Certified Software Systems Engineer (CSSE) Certified Systems Engineering Professional (CSEP) Cross-Org Skills Effective Communication Results Orientation Learning Agility Digital Fluency Customer Centricity Impact & Scope Impacts function and leads and/or provides expertise to functional project teams and may participate in cross-functional initiatives. Complexity Works on complex problems where analysis of situations or data requires an in-depth evaluation of multiple factors. Disclaimer This job description describes the general nature and level of work performed in this role. It is not intended to be an exhaustive list of all duties, skills, responsibilities, knowledge, etc. These may be subject to change and additional functions may be assigned as needed by management.",,,"Python, SQL, Tableau, Power BI",
4239603548,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You‚Äôll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you‚Äôll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you‚Äôll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization. Strong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources. Proficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities. Hands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements Preferred Technical And Professional Experience Understanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling. Ability to implement robust data validation, cleansing, and governance frameworks within ETL processes. Proficiency in SQL and/or Shell scripting for custom transformations and automation tasks",,,"SQL, Machine Learning",
4250000173,"Data Engineer, Associate",BlackRock,"Gurgaon, Haryana, India (Hybrid)",Hybrid,Full-time,,"About the job About This Role About this role The incumbent will be responsible for building and maintaining highly scalable data management systems and data pipelines to acquire data from various sources that can be used to support Data Scientists and Analytic systems for generating actionable insights. This includes tasks like collaborating with data vendors and partners to build and support the integration of data feeds, data transformation, and data warehousing to ensure data is easily accessible and analytics work smoothly. May work independently with counterparts to integrate new data feeds, resolving and problem-solving to ensure high system availability. Ability to analyze business requirements and translate them into technical solutions. Understand the broader environment, including relevant systems, processes, and databases. The overall goal is optimizing the performance and efficiency of the firm's client data platform ecosystem. Requirements Bachelor‚Äôs degree in computer engineering or equivalent experience. Hand-on experience as a Data Engineer (+4 years) Good knowledge of Hadoop, Spark, and SQL database. In-depth knowledge of programming languages including SQL, Python, and Java is a plus. Familiarity with cloud technologies including Snowflake and Azure Good project management skills, Good communication skills. Great Teammate. Ability to solve complex data, and software issues. Self-starter and eager to learn. Our Benefits To help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about. Our hybrid work model BlackRock‚Äôs hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person ‚Äì aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock. About BlackRock At BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children‚Äôs educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress. This mission would not be possible without our smartest investment ‚Äì the one we make in our employees. It‚Äôs why we‚Äôre dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive. For additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock BlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.",Associate,,"Python, SQL",
4232492834,Data Engineer,Accenture in India,"Kochi, Kerala, India (On-site)",On-site,Full-time,,"About the job Job Title - Data Engineer Sr.Analyst ACS SONG Management Level: Level 10 Sr. Analyst Location: Kochi, Coimbatore, Trivandrum Must have skills: Python/Scala, Pyspark/Pytorch Good to have skills: Redshift Job Summary You‚Äôll capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include: Roles And Responsibilities Designing, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals Solving complex data problems to deliver insights that helps our business to achieve their goals. Source data (structured‚Üí unstructured) from various touchpoints, format and organize them into an analyzable format. Creating data products for analytics team members to improve productivity Calling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline. Fostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions Preparing data to create a unified database and build tracking solutions ensuring data quality Create Production grade analytical assets deployed using the guiding principles of CI/CD. Professional And Technical Skills Expert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least) Extensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies. Experience in one of the many BI tools such as Tableau, Power BI, Looker. Good working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs. Worked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3),‚ÄØ Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse. Additional Information Experience working in cloud Data warehouses like Redshift or Synapse Certification in any one of the following or equivalent AWS- AWS certified data Analytics- Speciality Azure- Microsoft certified Azure Data Scientist Associate Snowflake- Snowpro core- Data Engineer Databricks Data Engineering About Our Company | Accenture (do not remove the hyperlink) Experience: 3.5 -5 years of experience is required Educational Qualification: Graduation (Accurate educational details should capture)",Associate,,"Python, SQL, Tableau, Power BI, Data Analysis",
4217274178,Machine Learning Engineer (Remote),Uplers,"Patna, Bihar, India (Remote)",Save Machine Learning Engineer (Remote) at Uplers,‚Çπ5M/yr,,"About the job Experience : 5.00 + years Salary : INR 5000000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Precanto) (*Note: This is a requirement for one of Uplers' client - A fast-growing, VC-backed B2B SaaS platform revolutionizing financial planning and analysis for modern finance teams.) What do you need for this opportunity? Must have skills required: async workflows, MLOps, Ray Tune, Data Engineering, MLFlow, Supervised Learning, Time-Series Forecasting, Docker, machine_learning, NLP, Python, SQL A fast-growing, VC-backed B2B SaaS platform revolutionizing financial planning and analysis for modern finance teams. is Looking for: We are a fast-moving startup building AI-driven solutions to the financial planning workflow. We‚Äôre looking for a versatile Machine Learning Engineer to join our team and take ownership of building, deploying, and scaling intelligent systems that power our core product. Job Description- Full-time Team: Data & ML Engineering We‚Äôre looking for 5+ years of experience as a Machine Learning or Data Engineer (startup experience is a plus) What You Will Do- Build and optimize machine learning models ‚Äî from regression to time-series forecasting Work with data pipelines and orchestrate training/inference jobs using Ray, Airflow, and Docker Train, tune, and evaluate models using tools like Ray Tune, MLflow, and scikit-learn Design and deploy LLM-powered features and workflows Collaborate closely with product managers to turn ideas into experiments and production-ready solutions Partner with Software and DevOps engineers to build robust ML pipelines and integrate them with the broader platform Basic Skills Proven ability to work creatively and analytically in a problem-solving environment Excellent communication (written and oral) and interpersonal skills Strong understanding of supervised learning and time-series modeling Experience deploying ML models and building automated training/inference pipelines Ability to work cross-functionally in a collaborative and fast-paced environment Comfortable wearing many hats and owning projects end-to-end Write clean, tested, and scalable Python and SQL code Leverage async workflows and cloud-native infrastructure (S3, Docker, etc.) for high-throughput data processing. Advanced Skills Familiarity with MLOps best practices Prior experience with LLM-based features or production-level NLP Experience with LLMs, vector stores, or prompt engineering Contributions to open-source ML or data tools TECH STACK Languages: Python, SQL Frameworks & Tools: scikit-learn, Prophet, pyts, MLflow, Ray, Ray Tune, Jupyter Infra: Docker, Airflow, S3, asyncio, Pydantic How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience async workflows, MLOps, Ray Tune, Data Engineering, MLFlow, Supervised Learning, Time-Series Forecasting, Docker, machine_learning, NLP, Python, SQL",manager,,"Python, SQL, Machine Learning",
4236006410,Data Engineer Tech Lead,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Must have 5+ years of experience in data engineer role Strong background in Relational Databases ( Microsoft SQL) and strong ETL (Microsoft SSIS) experience. Strong hand on T-SQL programming language Ability to develop reports using Microsoft Reporting Services (SSRS) Familiarity with C# is preferred Strong Analytical and Logical Reasoning skills Should be able to build processes that support data transformation, workload management, data structures, dependency and metadata Should be able to develop data models to answer questions for the business users Should be good at performing root cause analysis on internal/external data and processes to answer specific business data questions. Excellent communication skills to work with business users independently. Desired Skills and Experience C# (C Sharp), MS SQL Server, SSIS",,,SQL,
4244680221,Data Engineer-Data Platforms,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe. You'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio Your Role And Responsibilities As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution. Your primary responsibilities include: Lead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements. Strive for continuous improvements by testing the build solution and working under an agile framework. Discover and implement the latest technologies trends to maximize and build creative solutions Preferred Education Master's Degree Required Technical And Professional Expertise Experience with Apache Spark (PySpark): In-depth knowledge of Spark‚Äôs architecture, core APIs, and PySpark for distributed data processing. Big Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts. Strong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy. SQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation. Cloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems Preferred Technical And Professional Experience Define, drive, and implement an architecture strategy and standards for end-to-end monitoring. Partner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering, Good to have detection and prevention tools for Company products and Platform and customer-facing",,,"Python, SQL, Data Analysis",
4235095508,GCP Data Engineer/Architect,Tata Consultancy Services,Greater Bengaluru Area (On-site),On-site,Full-time,,"About the job Greetings from TCS! Job Title: GCP Data Engineer/Architect Required Skillset: Data Architect/Engineer Location: Pan India Experience Range: 5+ years Job Description Must-Have** Python SQL Responsibility of / Expectations from the Role: Develop Mappings and Workflows based on the project requirements. Unit testing and support during Production migration. Root cause analysis of incidents and implementing fix for the same. Thanks & Regards, Ria Aarthi A.",,,"Python, SQL",
4251118526,Python Developer Intern,Unified Mentor Private Limited,India (Remote),Remote,Internship,,"About the job Job Title: Python Developer Intern Company: Unified Mentor Location: Remote Duration: 3 months Opportunity: Full-time based on performance, with a Certificate of Internship Application Deadline: 18th June 2025 About Unified Mentor Unified Mentor provides students and graduates with hands-on experience, professional training, and career-building opportunities in software development , helping you enhance your coding skills and prepare for a successful career. Role Overview As a Python Developer Intern , you will work on real-world projects , enhance your coding skills, and gain practical experience in software development . Responsibilities ‚úÖ Develop, test, and debug Python applications. ‚úÖ Collaborate on software projects and API integrations. ‚úÖ Learn and apply Python frameworks like Django/Flask . Qualifications üéì Enrolled in or completed a Computer Science or related program. üêç Proficient in Python programming . üåê Familiarity with web frameworks (Django/Flask). üß© Strong problem-solving and time-management skills . Perks üí∞ Stipend: ‚Çπ7,500 - ‚Çπ15,000 (Performance-Based) (Paid) ‚úî Certificate of Internship & Letter of Recommendation . ‚úî Hands-on experience with real-world projects for your portfolio . How to Apply üì© Submit your application with the subject: ""Python Developer Intern Application"" . Equal Opportunity Unified Mentor is an equal opportunity employer , welcoming applicants from all backgrounds.",,,Python,"‚Çπ7,500 - ‚Çπ15,000"
4244169425,AGM-Data Engineer,Vodafone Idea Limited,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Role Overview We are seeking a highly skilled and motivated Senior Data Scientist with deep expertise in Generative AI , Machine Learning , Deep Learning , and advanced Data Analytics . The ideal candidate will have hands-on experience in building, deploying, and maintaining end-to-end ML solutions at scale, preferably within the Telecom domain. You will be part of our AI & Data Science team, working on high-impact projects ranging from customer analytics, network intelligence, churn prediction, to generative AI applications in telco automation and customer experience. Key Responsibilities Design, develop, and deploy advanced machine learning and deep learning models for Telco use cases such as: Network optimization Customer churn prediction Usage pattern modeling Fraud detection GenAI applications (e.g., personalized recommendations, customer service automation) Lead the design and implementation of Generative AI solutions (LLMs, transformers, text-to-text/image models) using tools like OpenAI, Hugging Face, LangChain, etc. Collaborate with cross-functional teams including network, marketing, IT, and business to define AI-driven solutions. Perform exploratory data analysis, feature engineering, model selection, and evaluation using real-world telecom datasets (structured and unstructured). Drive end-to-end ML solution deployment into production (CI/CD pipelines, model monitoring, scalability). Optimize model performance and latency in production, especially for real-time and edge applications. Evaluate and integrate new tools, platforms, and AI frameworks to advance Vi‚Äôs data science capabilities. Provide technical mentorship to junior data scientists and data engineers. Required Qualifications & Skills 8+ years of industry experience in Machine Learning, Deep Learning, and Advanced Analytics. Strong hands-on experience with GenAI models and frameworks (e.g., GPT, BERT, Llama, LangChain, RAG pipelines). Proficiency in Python, and libraries such as scikit-learn, TensorFlow, PyTorch, Hugging Face Transformers, etc. Experience in end-to-end model lifecycle management, from data preprocessing to production deployment (MLOps). Familiarity with cloud platforms like AWS, GCP, or Azure; and ML deployment tools (Docker, Kubernetes, MLflow, FastAPI, etc.). Strong understanding of SQL, big data tools (Spark, Hive), and data pipelines. Excellent problem-solving skills with a strong analytical mindset and business acumen. Prior experience working on Telecom datasets or use cases is a strong plus. Preferred Skills Experience with vector databases, embeddings, and retrieval-augmented generation (RAG) pipelines. Exposure to real-time ML inference and streaming data platforms (Kafka, Flink). Knowledge of network analytics, geo-spatial modeling, or customer behavior modeling in a Telco environment. Experience mentoring teams or leading small AI/ML projects.",,,"Python, SQL, Machine Learning, Data Analysis",
4206236022,Lead Data Engineer,"Verticalmove, Inc","Pune, Maharashtra, India (Hybrid)",Hybrid,Starting at $75K/yr + Bonus,,"About the job PLEASE READ BEFORE APPLY: WE WILL ONLY CONSIDER CANDIDATES COMING FROM B2B SAAS OR CONSUMER INTERNET COMPANIES (THINK SIMILAR TO SALESFORCE, WORKDAY, INTUIT, ATLASSIAN, WALMART, AMAZON) WE WILL NOT CONSIDER ANY EXPERIENCE FROM IT OR DIGITAL TRANSFORMATION CONSULTANCIES (EXAMPLES OF THE WRONG COMPANIES WOULD BE TATA, INFOSYS, COGNIZANT, IBM GLOBAL SERVICES) THIS IS A HYBRID ROLE 3-DAYS A WEEK, NO REMOTE AT THIS TIME We are disrupting the supply chain planning industry with our AI-driven demand planning and inventory replenishment software. The total addressable market (TAM) for supply chain management (SCM) software is substantial and growing rapidly. In 2023, the global SCM software market was valued at approximately $28.9 billion and is projected to reach $45.2 billion by 2027, reflecting a compound annual growth rate (CAGR) of around 9.4% during the forecast period. We specifically target the rapidly growing segment of the supply chain management (SCM) industry: small and medium-sized businesses (SMBs). This focus has become even more pertinent in light of the supply chain disruptions caused by COVID-19. SMBs now have access to technological advantages previously exclusive to large enterprise companies like Walmart, Amazon, Lowe's, and Home Depot, who have invested hundreds of millions into these technologies. In this role, you will be responsible for designing, implementing, and deploying the next-generation platform tightly integrated with Amazon AWS services such as EMR, Athena, Glue, and Spark. You will develop an ultra-real-time, AI-driven demand planning engine to help our client serve industries that manage perishable items, including food manufacturing, restaurants, grocery stores, pharmaceuticals, and more. If you are passionate about leveraging machine learning and AI technologies to solve complex supply chain challenges, this is the perfect opportunity for you. Fast Facts About Our Company: ‚Ä¢ Employee Count: 500+, 58 in Engineering (6 in the US and 52 in Pune India) ‚Ä¢ Customer Count: 200+ ‚Ä¢ Revenue: $25.0M in ARR (and growing fast) ‚Ä¢ Profitable: Yes Job Description: As a Lead Data Engineer, you will play a crucial role in building a scalable data platform that powers advanced supply chain solutions and next-generation AI applications. The role involves working on both legacy systems and modern cloud-based platforms to help scale the data infrastructure. You will collaborate with a team of data engineers and work closely with data scientists, machine learning engineers, and software developers to optimize data performance and build a platform that supports the adoption of Generative AI (GenAI) technologies. You will be responsible for building and optimizing distributed computing frameworks, involving functional business logic, and focusing on distributing jobs to the lowest unit possible to achieve infinite scale. Job Responsibilities: Design and build a scalable, fault-tolerant data platform optimized for distributed computing and large-scale data processing using AWS, Databricks, and Apache Spark. Implement data pipelines and ETL/ELT processes to efficiently ingest, transform, and load massive datasets from various sources. Leverage cloud data platforms to enable seamless data sharing, near-zero maintenance, and fast analytics on both structured and semi-structured data. Build and Optimize distributed computing jobs and queries to ensure maximum performance, cost efficiency, and scalability by distributing tasks to the lowest level possible. Collaborate with data scientists, machine learning engineers, and software developers to build solutions that power GenAI applications. Provide guidance on distributed computing architecture and mentor junior data engineers. Implement data governance, security, and compliance best practices. Drive innovation by partnering with global teams to enhance supply chain technology. Develop solutions that incorporate functional business logic into distributed computing frameworks, ensuring the scalability and efficiency of the data platform. Experience: Experience with reinforcement learning and optimization techniques for supply chain use cases. Familiarity with Generative AI (GenAI) and its application to predictive analytics and decision support. Experience with big data technologies such as Apache Spark and data orchestration tools like Apache Airflow. AWS, GCP, or Azure certifications (e.g., AWS Certified Machine Learning - Specialty). Required Experience: 5+ years of experience as a Data Engineer, with strong expertise in big data technologies. 7+ years of experience with cloud architecture, with extensive expertise in AWS. Strong proficiency in SQL, Python, and data modeling techniques. Deep knowledge of distributed computing principles and frameworks (e.g., Apache Spark, Apache Airflow), including experience with data streaming (Kafka). Hands-on experience with developing and deploying distributed computing applications using cloud-based platforms (e.g., AWS EMR, Azure HDInsight). Experience with cloud data platform architectures and best practices for ETL/ELT, data sharing, and query optimization. Experience in building and optimizing distributed computing frameworks to handle functional business logic and achieve scalability. Nice to have .NET code, project structure, and typical application development process/technologies. Excellent problem-solving and communication skills. AWS, GCP, or Azure certifications are highly valued (e.g., AWS Certified Solutions Architect, AWS Certified Big Data - Specialty). Required Education: Bachelor's or Master's degree in Data Science, Computer Science, Operations Research, Statistics, or a related field.",Executive,,"Python, SQL, Machine Learning",
4235447859,Sr GCP Data Engineer,LTIMindtree,"Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Experience: 5-12 years Primary Skills: GCP, Big query, Python Notice period: Immediate to 30 days Overall 5+ years of experience in data eng Strong Experience with GCP services like Big Query Cloud Storage composer etc Hands on experience in Python Proficient in Pyspark databricks Strong understanding of data warehouse concepts ETL process and data modelling GCP certifications are added advantage",,,Python,
4192932437,Data Engineer,Roadzen,"Delhi, Delhi, India (Remote)",Remote,Full-time,,"About the job This job is sourced from a job board. Learn More A Data Engineer, or Data system engineer, converts the raw data into usable information for the company. They are responsible for developing and maintaining data processing software like databases. For this, they create unique data infrastructure, run tests, and update the systems. Responsibilities Data engineers create and maintain an optimal data infrastructure. They combine raw information to create machine-readable formats. These are the major duties and responsibilities of a data engineer: Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. Work with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. This job was posted by Arpita Rai from Roadzen. Desired Skills and Experience Big Data,Python,SQL",Executive,,"Python, SQL",
4222692150,Sr. Data Engineer,Anblicks,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Primary Duties Designing Data Models: Utilize Kimball data modeling techniques and Data Vault 2.0 methodologies to create and maintain robust data models. Data Integration: Use Fivetran for seamless data extraction and loading, and Azure Data Factory (ADF) for orchestrating data workflows. Data Warehousing: Manage and optimize data storage in Snowflake, ensuring efficient data retrieval and processing. Data Transformation: Develop and maintain transformation scripts using DBT (Data Build Tool) to convert raw data into actionable insights. Maintaining Data Documentation: Ensure all data processes and models are well-documented and easily understandable. Communicating Results: Present data insights to stakeholders through visual representations and reports. Collaborating: Work closely with data analysts, data engineers, and business executives to align data strategies with business goals. Skills Required Technical Skills: Proficiency in SQL, Python, and data visualization tools like Tableau or Power BI. Analytical Skills: Ability to interpret complex data and provide actionable insights. Communication Skills: Strong ability to explain technical concepts to non-technical stakeholders. Problem-Solving: Aptitude for identifying issues within data and developing solutions.",executive,,"Python, SQL, Tableau, Power BI",
4244772099,Snowflake Data Engineer,Viraaj HR Solutions Private Limited,"Bhubaneswar, Odisha, India (On-site)",On-site,‚Çπ1M/yr - ‚Çπ2M/yr,,"About the job Company Overview Viraaj HR Solutions is a dynamic recruitment agency focused on connecting top talent with leading employers across various industries in India. Our mission is to enhance the hiring process through innovative solutions and personalized services, ensuring both candidates and clients find their perfect match. As we continue to grow, we foster a culture of excellence, collaboration, and integrity, putting the needs of our clients and candidates at the forefront of everything we do. Job Title: Snowflake Data Engineer Location: India (On-Site) Role Responsibilities Design, develop and maintain scalable data models using Snowflake. Implement ETL processes to integrate data from multiple sources. Optimize Snowflake performance to ensure efficient data retrieval. Conduct data analysis and validation to ensure data quality. Collaborate with cross-functional teams to gather data requirements. Build data pipelines to automate data flow and processing. Ensure data governance and compliance with data standards. Monitor and troubleshoot data workflows for smooth operations. Write complex SQL queries for data extraction and reporting. Participate in architecture discussions and design strategies. Implement security measures for sensitive data handling. Document technical specifications and maintain proper data dictionaries. Provide support and training to other team members on Snowflake tools. Stay updated with the latest trends and technologies in data engineering. Engage in continuous improvement of data processes and methodologies. Qualifications Bachelor's degree in Computer Science, Information Technology, or a related field. Proven experience as a Data Engineer, specifically with Snowflake. Strong knowledge of data warehousing concepts and best practices. Experience with cloud-based data solutions (AWS, Azure). Proficient in SQL and data manipulation languages. Familiarity with ETL tools and data integration techniques. Understanding of data modeling and schema design. Ability to write Python or similar scripting languages for data processing. Strong analytical and problem-solving skills. Excellent communication and teamwork abilities. Experience with data governance frameworks. Knowledge of data visualization tools is a plus. Ability to work independently and manage multiple tasks concurrently. Willingness to adapt to a fast-paced environment. Certifications related to Snowflake or equivalent data platforms are desirable. Skills: data integration,problem solving,performance tuning,python,cloud-based data solutions,snowflake,data modeling,data engineer,etl processes,data security,data visualization,data analysis,data warehousing,data governance,sql",,,"Python, SQL, Data Analysis",
4251675191,Remote Python AI Engineer - 17852,Turing,"Delhi, India (Remote)",Save Remote Python AI Engineer - 17852¬†  at Turing,Contract,,"About the job Work on Real-World Problems with Global Tech Experts Join a leading U.S.-based technology company as a Python Developer / AI Engineer, where you‚Äôll tackle real-world challenges and build innovative solutions alongside top global experts. This is a fully remote, contract-based opportunity ideal for developers passionate about Python, data analysis, and AI-driven work. Key Responsibilities: Write efficient, production-grade Python code to solve complex problems. Analyze public datasets and extract meaningful insights using Python and SQL. Collaborate with researchers and global teams to iterate on data-driven ideas. Document all code and development decisions in Jupyter Notebooks or similar platforms. Maintain high-quality standards and contribute to technical excellence. Job Requirements: Open to all levels: junior, mid-level, or senior engineers. Degree in Computer Science, Engineering, or equivalent practical experience. Proficient in Python programming for scripting, automation, or backend development. Experience with SQL/NoSQL databases is a plus. Familiarity with cloud platforms (AWS, GCP, Azure) is advantageous. Must be able to work 5+ hours overlapping with Pacific Time (PST/PT). Strong communication and collaboration skills in a remote environment. Perks & Benefits: Work on cutting-edge AI and data projects impacting real-world use cases. Collaborate with top minds from Meta, Stanford, and Google. 100% remote ‚Äì work from anywhere. Contract role with flexibility and no traditional job constraints. Competitive compensation in USD, aligned with global tech standards. Selection Process: Shortlisted developers may be asked to complete an assessment. If you clear the assessment, you will be contacted for contract assignments with expected start dates, durations, and end dates. Some contract assignments require fixed weekly hours, averaging 20/30/40 hours per week for the duration of the contract assignment.",,,"Python, SQL, Data Analysis",
4242448270,Data Engineer-Data Warehouse,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As an Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. In this role, your responsibilities may include: Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Design and implement efficient database schemas and data models using Teradata. Optimize SQL queries and stored procedures for performance. Perform database administration tasks including installation, configuration, and maintenance of Teradata systems Preferred Technical And Professional Experience You thrive on teamwork and have excellent verbal and written communication skills. Ability to communicate with internal and external clients to understand and define business needs, providing analytical solutions Ability to communicate results to technical and non-technical audiences",,,"SQL, Machine Learning",
4217273195,Machine Learning Engineer (Remote),Uplers,"Ranchi, Jharkhand, India (Remote)",Save Machine Learning Engineer (Remote) at Uplers,‚Çπ5M/yr,,"About the job Experience : 5.00 + years Salary : INR 5000000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Precanto) (*Note: This is a requirement for one of Uplers' client - A fast-growing, VC-backed B2B SaaS platform revolutionizing financial planning and analysis for modern finance teams.) What do you need for this opportunity? Must have skills required: async workflows, MLOps, Ray Tune, Data Engineering, MLFlow, Supervised Learning, Time-Series Forecasting, Docker, machine_learning, NLP, Python, SQL A fast-growing, VC-backed B2B SaaS platform revolutionizing financial planning and analysis for modern finance teams. is Looking for: We are a fast-moving startup building AI-driven solutions to the financial planning workflow. We‚Äôre looking for a versatile Machine Learning Engineer to join our team and take ownership of building, deploying, and scaling intelligent systems that power our core product. Job Description- Full-time Team: Data & ML Engineering We‚Äôre looking for 5+ years of experience as a Machine Learning or Data Engineer (startup experience is a plus) What You Will Do- Build and optimize machine learning models ‚Äî from regression to time-series forecasting Work with data pipelines and orchestrate training/inference jobs using Ray, Airflow, and Docker Train, tune, and evaluate models using tools like Ray Tune, MLflow, and scikit-learn Design and deploy LLM-powered features and workflows Collaborate closely with product managers to turn ideas into experiments and production-ready solutions Partner with Software and DevOps engineers to build robust ML pipelines and integrate them with the broader platform Basic Skills Proven ability to work creatively and analytically in a problem-solving environment Excellent communication (written and oral) and interpersonal skills Strong understanding of supervised learning and time-series modeling Experience deploying ML models and building automated training/inference pipelines Ability to work cross-functionally in a collaborative and fast-paced environment Comfortable wearing many hats and owning projects end-to-end Write clean, tested, and scalable Python and SQL code Leverage async workflows and cloud-native infrastructure (S3, Docker, etc.) for high-throughput data processing. Advanced Skills Familiarity with MLOps best practices Prior experience with LLM-based features or production-level NLP Experience with LLMs, vector stores, or prompt engineering Contributions to open-source ML or data tools TECH STACK Languages: Python, SQL Frameworks & Tools: scikit-learn, Prophet, pyts, MLflow, Ray, Ray Tune, Jupyter Infra: Docker, Airflow, S3, asyncio, Pydantic How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience async workflows, MLOps, Ray Tune, Data Engineering, MLFlow, Supervised Learning, Time-Series Forecasting, Docker, machine_learning, NLP, Python, SQL",manager,,"Python, SQL, Machine Learning",
4242956103,Lead Data Engineer,Hitachi Digital,"Itanagar, Arunachal Pradesh, India",,Full-time,,"About the job Our Company We‚Äôre Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We‚Äôre crucial to the company‚Äôs strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market. Our group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole. Imagine the sheer breadth of talent it takes to unleash a digital future. We don‚Äôt expect you to ‚Äòfit‚Äô every requirement ‚Äì your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us. Preferred job location: Bengaluru, Hyderabad, Pune, New Delhi or Remote The team Hitachi Digital is a leader in digital transformation, leveraging advanced AI and data technologies to drive innovation and efficiency across various operational companies (OpCos) and departments. We are seeking a highly experienced Lead Data Engineer to join our dynamic team and contribute to the development of robust data solutions and applications. The role Lead the design, development, and implementation of data engineering solutions with a focus on Google BigQuery. Develop and optimize complex SQL queries and data pipelines in BigQuery. Implement and integrate VectorAI and Agent Workspace for Google Gemini into data solutions. Lead the development of high-performance data ingestion processes using modern ETL/ELT practices. Collaborate with engineers to establish best practices for data system creation, ensuring data quality, integrity, and proper documentation. Continuously improve reporting and analysis by automating processes and streamlining workflows. Conduct research and stay updated on the latest advancements in data engineering and technologies. Troubleshoot and resolve complex issues related to data systems and applications. Document development processes, methodologies, and best practices. Mentor junior developers and participate in code reviews, providing constructive feedback to team members. Provide strategic direction and leadership in data engineering and technology adoption. What You‚Äôll Bring Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field. 10+ years of experience in data technologies. 5+ years of extensive experience in migrating data workloads to BigQuery on GCP. Strong programming skills in languages such as Python, Java, or SQL. Technical proficiency in BigQuery and other related tools on GCP. GCP Certifications in the data space. Knowledge of cloud platforms, particularly Google Cloud Platform (GCP). Experience with VectorAI and Agent Workspace for Google Gemini. Excellent problem-solving skills and the ability to work independently and as part of a team. Strong communication skills and the ability to convey complex technical concepts to non-technical stakeholders. Proven leadership skills and experience in guiding development projects from conception to deployment. Preferred Qualifications: Familiarity with data engineering tools and techniques. Previous experience in a similar role within a tech-driven company. About Us We‚Äôre a global, 1000-strong diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We‚Äôre curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you‚Äôre not just another employee; you‚Äôre part of a tradition of excellence and a community working towards creating a digital future. Championing diversity, equity, and inclusion Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team. How We Look After You We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We‚Äôre also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We‚Äôre always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you‚Äôll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with. We‚Äôre proud to say we‚Äôre an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",,,"Python, SQL",
4171869101,BI & Data Warehouse Data Engineer,Astellas Pharma,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job As part of the Astellas commitment to delivering value for our patients, our organization is currently undergoing transformation to achieve this critical goal. This is an opportunity to work on digital transformation and make a real impact within a company dedicated to improving lives. DigitalX our new information technology function is spearheading this value driven transformation across Astellas. We are looking for people who excel in embracing change, manage technical challenges and have exceptional communication skills. This position is based in Bengaluru and will require some on-site work. Purpose And Scope As a Junior Data Engineer, you will play a crucial role in assisting in the design, build, and maintenance of our data infrastructure focusing on BI and DWH capabilities. Working with the Senior Data Engineer, your foundational expertise in BI, Databricks, PySpark, SQL, Talend and other related technologies, will be instrumental in driving data-driven decision-making across the organization. You will play a pivotal role in building maintaining and enhancing our systems across the organization. This is a fantastic global opportunity to use your proven agile delivery skills across a diverse range of initiatives, utilize your development skills, and contribute to the continuous improvement/delivery of critical IT solutions. Essential Job Responsibilities Collaborate with FoundationX Engineers to design and maintain scalable data systems. Assist in building robust infrastructure using technologies like PowerBI, Qlik or alternative, Databricks, PySpark, and SQL. Contribute to ensuring system reliability by incorporating accurate business-driving data. Gain experience in BI engineering through hands-on projects. Data Modelling and Integration: Collaborate with cross-functional teams to analyse requirements and create technical designs, data models, and migration strategies. Design, build, and maintain physical databases, dimensional data models, and ETL processes specific to pharmaceutical data. Cloud Expertise: Evaluate and influence the selection of cloud-based technologies such as Azure, AWS, or Google Cloud. Implement data warehousing solutions in a cloud environment, ensuring scalability and security. BI Expertise: Leverage and create PowerBI, Qlik or equivalent technology for data visualization, dashboards, and self-service analytics. Data Pipeline Development: Design, build, and optimize data pipelines using Databricks and PySpark. Ensure data quality, reliability, and scalability. Application Transition: Support the migration of internal applications to Databricks (or equivalent) based solutions. Collaborate with application teams to ensure a seamless transition. Mentorship and Leadership: Lead and mentor junior data engineers. Share best practices, provide technical guidance, and foster a culture of continuous learning. Data Strategy Contribution: Contribute to the organization‚Äôs data strategy by identifying opportunities for data-driven insights and improvements. Participate in smaller focused mission teams to deliver value driven solutions aligned to our global and bold move priority initiatives and beyond. Design, develop and implement robust and scalable data analytics using modern technologies. Collaborate with cross functional teams and practises across the organisation including Commercial, Manufacturing, Medical, DataX, GrowthX and support other X (transformation) Hubs and Practices as appropriate, to understand user needs and translate them into technical solutions. Provide Technical Support to internal users troubleshooting complex issues and ensuring system uptime as soon as possible. Champion continuous improvement initiatives identifying opportunities to optimise performance security and maintainability of existing data and platform architecture and other technology investments. Participate in the continuous delivery pipeline. Adhering to DevOps best practises for version control automation and deployment. Ensuring effective management of the FoundationX backlog. Leverage your knowledge of data engineering principles to integrate with existing data pipelines and explore new possibilities for data utilization. Stay-up to date on the latest trends and technologies in data engineering and cloud platforms. Qualifications Required Bachelor's degree in computer science, Information Technology, or related field (master‚Äôs preferred) or equivalent experience 1-3+ years of experience in data engineering with a strong understanding of BI technologies, PySpark and SQL, building data pipelines and optimization. 1-3 +years + experience in data engineering and integration tools (e.g., Databricks, Change Data Capture) 1-3+ years + experience of utilizing cloud platforms (AWS, Azure, GCP). A deeper understanding/certification of AWS and Azure is considered a plus. Experience with relational and non-relational databases. Any relevant cloud-based integration certification at foundational level or above. (Any QLIK or BI certification, AWS certified DevOps engineer, AWS Certified Developer, Any Microsoft Certified Azure qualification, Proficient in RESTful APIs, AWS, CDMP, MDM, DBA, SQL, SAP, TOGAF, API, CISSP, VCP or any relevant certification) Experience in MuleSoft (Anypoint platform, its components, Designing and managing API-led connectivity solutions). Experience in AWS (environment, services and tools), developing code in at least one high level programming language. Experience with continuous integration and continuous delivery (CI/CD) methodologies and tools Experience with Azure services related to computing, networking, storage, and security Understanding of cloud integration patterns and Azure integration services such as Logic Apps, Service Bus, and API Management Preferred Subject Matter Expertise: possess a strong understanding of data architecture/ engineering/operations/ reporting within Life Sciences/ Pharma industry across Commercial, Manufacturing and Medical domains. Other complex and highly regulated industry experience will be considered across diverse areas like Commercial, Manufacturing and Medical. Data Analysis and Automation Skills: Proficient in identifying, standardizing, and automating critical reporting metrics and modelling tools Analytical Thinking: Demonstrated ability to lead ad hoc analyses, identify performance gaps, and foster a culture of continuous improvement. Technical Proficiency: Strong coding skills in SQL, R, and/or Python, coupled with expertise in machine learning techniques, statistical analysis, and data visualization. Agile Champion: Adherence to DevOps principles and a proven track record with CI/CD pipelines for continuous delivery. Working Environment At Astellas we recognize the importance of work/life balance, and we are proud to offer a hybrid working solution allowing time to connect with colleagues at the office with the flexibility to also work from home. We believe this will optimize the most productive work environment for all employees to succeed and deliver. Hybrid work from certain locations may be permitted in accordance with Astellas‚Äô Responsible Flexibility Guidelines. \ Category FoundationX Astellas is committed to equality of opportunity in all aspects of employment. EOE including Disability/Protected Veterans",,,"Python, SQL, Excel, R, Machine Learning, Data Analysis",
4235645664,Senior Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job 6+ years of experience with Java Spark. Strong understanding of distributed computing, big data principles, and batch/stream processing. Proficiency in working with AWS services such as S3, EMR, Glue, Lambda, and Athena. Experience with Data Lake architectures and handling large volumes of structured and unstructured data. Familiarity with various data formats. Strong problem-solving and analytical skills. Excellent communication and collaboration abilities. Design, develop, and optimize large-scale data processing pipelines using Java Spark Build scalable solutions to manage data ingestion, transformation, and storage in AWS-based Data Lake environments. Collaborate with data architects and analysts to implement data models and workflows aligned with business requirements. Ensure performance tuning, fault tolerance, and reliability of distributed data processing systems. Desired Skills and Experience AWS Serverless Services, Java Spark, Terraform, EKS, DevOps, Java, S3",,,,
4246262034,Data Engineer,Meant To Be Inc,"Gurgaon, Haryana, India (On-site)",On-site,Full-time,,"About the job We are looking for a Data Engineer to help design, build, and scale the data platform and infrastructure that powers our core recommendation systems and personalisation engines. You‚Äôll be a founding data engineer helping us lay the pipelines and infrastructure that feed our recommendation systems, onboarding flows, engagement metrics, dashboards, and ML models. You'll work hand-in-hand with our ML team to build data adapters and interfaces for model training, serving, and experimentation. Our Philosophy We believe: Great data + Good models = Great recommendations Good data + Great models = Average recommendations That‚Äôs why we‚Äôre investing in data infrastructure from our inception and foundation. What You'll Do Build and scale ETL from raw app events to ML-ready features Build real-time and batch pipelines for user actions and events Contribute to the feature store , embedding pipelines, data validation, and freshness monitoring systems Design data layers for use in A/B tests , product metrics , and ML training Partner with ML engineers to deliver data adapters for model inputs, embedding generation, and training loops Help evolve the core event schema and data model with privacy, observability, and performance in mind Ideal Profile You‚Äôre a systems thinker who starts with data and designs for scale. You‚Äôve likely exposure at early-stage or high-scale consumer platforms ‚Äî social, gaming, transactions, or media. 3‚Äì6 years of building scalable data systems in fast-moving environments Experience supporting RecSys, ML, or content feeds in social or consumer platforms Understands how features feed into models, or has worked closely with ML teams before Understands how data impacts user experience , not just analytics Kafka, Spark, Flink, Airflow, dbt, Redis, BigQuery, Feast, Terraform, Pick your best tool for the job Nice to Haves Experience with personalisation systems , recommender pipelines, or ranking models Exposure to A/B testing platforms or online experimentation infrastructure Passion for consumer social or dating products",,,,
4089384280,Lead Data Engineer - Data Engineering,Bristlecone,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Job Description Fair knowledge of programming languages like Python /SQLand advanced SQL Having good business knowledge like sales cycle , Inventory , supply application Fair knowledge of Snowflake and GItHUB , DBT Sigma / DOMO , Airflow , HVR , SAP Ability to identify, analyze, and resolve problems logically Ability to troubleshoot and identify root cause Responsibilities Maintain production systems reliability through correct utilization of IT standards and governance processes Collaborate with business / functional team, develop detailed plans and accurate estimates for completion of build, system testing and implementation of project Review program codes and suggest correction in case of any errors Conduct performance tuning to improve system performance over multiple business processes Monitor and setup the jobs in test/ sandbox systems for testing Making sure the correct test data is arranged for checking on reports Working with business to check on the testing and sign off of the reports Qualifications Snowflake : SQL DBT Airflow Github SAP and HVR : sap integration with HVR Sigma & DOMO for reporting Proficiency in Python. Understanding of Airflow architecture and concepts. Experience with SQL and database design. About Us ABOUT US Bristlecone is the leading provider of AI-powered application transformation services for the connected supply chain. We empower our customers with speed, visibility, automation, and resiliency ‚Äì to thrive on change. Our transformative solutions in Digital Logistics, Cognitive Manufacturing, Autonomous Planning, Smart Procurement and Digitalization are positioned around key industry pillars and delivered through a comprehensive portfolio of services spanning digital strategy, design and build, and implementation across a range of technology platforms. Bristlecone is ranked among the top ten leaders in supply chain services by Gartner. We are headquartered in San Jose, California, with locations across North America, Europe and Asia, and over 2,500 consultants. Bristlecone is part of the $19.4 billion Mahindra Group. Equal Opportunity Employer Bristlecone is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status . Information Security Responsibilities Understand and adhere to Information Security policies, guidelines and procedure, practice them for protection of organizational data and Information System. Take part in information security training and act while handling information. Report all suspected security and policy breach to InfoSec team or appropriate authority (CISO). Understand and adhere to the additional information security responsibilities as part of the assigned job role.",,,"Python, SQL",
4242616514,Data Engineer,Toyota Connected India,"Chennai, Tamil Nadu, India (Hybrid)",Hybrid,Full-time,,"About the job About Toyota Connected If you want to change the way the world works, transform the automotive industry and positively impact others on a global scale, then Toyota Connected is the right place for you! Within our collaborative, fast-paced environment we focus on continual improvement and work in a highly iterative way to deliver exceptional value in the form of connected products and services that wow and delight our customers and the world around us. About the Team Toyota Connected India is looking for an experienced Data Engineer to build and optimize data pipelines for a real-time Digital Twin platform powering mobility simulation, complex event processing, and multi-agent learning . You‚Äôll design the backbone for scalable, low-latency ingestion and processing of high-volume sensor, vehicle, and infrastructure data to feed prediction models and simulations. What you will do ¬∑ Design and implement streaming data pipelines from IoT sensors, camera, vehicle telemetry, and infrastructure systems. ¬∑ Build scalable infrastructure using Kinesis, Apache Flink / Spark for real-time and batch workloads. ¬∑ Enable t ime-series feature stores and sliding window processing for mobility patterns. ¬∑ Integrate simulation outputs and model predictions into data lakes in AWS . ¬∑ Maintain data validation, schema versioning, and high-throughput ingestion. ¬∑ Collaborate with Data Scientists and Simulation Engineers to optimize data formats (e.g., Parquet, Protobuf, Delta Lake). ¬∑ Deploy and monitor pipelines on AWS cloud and/or edge infrastructure. You are a successful candidate if you have ¬∑ 3+ years of experience in data engineering , preferably with real-time systems. ¬∑ Proficient with Python, SQL, and distributed data systems (Kinesis, Spark, Flink, etc.). ¬∑ Strong understanding of event-driven architectures, data lakes, and message serialization . ¬∑ Experience with sensor data processing, telemetry ingestion, or mobility data is a plus. ¬∑ Familiarity with Docker, CI/CD, Kubernetes , and cloud-native architectures. ¬∑ Familiarity with building data pipelines & its workflows (eg: Airflow). Preferred Qualifications: ¬∑ Exposure to smart city platforms, V2X ecosystems or other timeseries paradigms . ¬∑ Experience integrating data from Camera and other sensors. What is in it for you? ¬∑ Top of the line compensation! ¬∑ You'll be treated like the professional we know you are and left to manage your own time and workload. ¬∑ Yearly gym membership reimbursement & Free catered lunches. ¬∑ No dress code! We trust you are responsible enough to choose what‚Äôs appropriate to wear for the day. ¬∑ Opportunity to build products that improves the safety and convenience of millions of customers ¬∑ Cool office space and other awesome benefits! Our Core Values: EPIC Empathetic: We begin making decisions by looking at the world from the perspective of our customers, teammates, and partners. Passionate: We are here to build something great, not just for the money. We are always looking to improve the experience of our millions of customers Innovative: We experiment with ideas to get to the best solution. Any constraint is a challenge, and we love looking for creative ways to solve them. Collaborative: When it comes to people, we think the whole is greater than its parts and that everyone has a role to play in the success! To know more about us ,check out our glassdoor page-https://www.glassdoor.co.in/Reviews/TOYOTA-Connected-Corporation-Reviews-E3305334.htm",,,"Python, SQL",
4233513361,Data Engineer,Tatvic ¬Æ,"Ahmedabad, Gujarat, India (On-site)",On-site,Full-time,,"About the job Company Description Tatvic, is a marketing analytics company focusing on generating insights from data using long association with Google & its infrastructure. We breed,recognize and reward performance. As a company we are growing very fast & we are under transformation. To enable this transformation we need future leaders with eyesight which balances execution & strategic understanding. Website: www.tatvic.com Accountabilities Technical Responsibilities: Design and Build reusable solutions which can be used for multiple customers. Maintain Cloud projects optimally and securely. Research and introduce changes in new technologies and methodologies in the existing setup. Performing RCA and fixing data issues for Marketing Attribution and user behavioral datasets Doing exploratory data analysis on raw user behavioral data ETL (Building pipelines from raw events data and commerce data) Create clear documentation on architecture, design concepts and data operations undertaken in the project. Stakeholder Management with Marketing, Insights and Product team. Monitoring pipelines, troubleshooting the failures and resolving them accordingly. User Acceptance Testing on datasets post ETL Responsibilities W.r.t Customer Keep the promises made to the customer in terms of deliverables and stakeholder management. Designing solutions using Cloud Technologies. Efficient Architectures of Pipelines and Clever usage of Multiple Cloud technologies like but not limited to Compute, Networking, and Security Services, Data Services. Have a broad understanding of suitable applications and limitations of multiple cloud services. Research and Adhere to Cloud best practices, including Security, Networking and IAM in the Project. Learn new cloud technologies to revamp existing services for more efficient and easy executions. Introduce Cost optimizations and Best practices in the client project. Understand the Cloud Capabilities of the client or their cloud partners. Performing Data Analysis and Pipeline creation and scheduling as per client‚Äôs requirements. Team Responsibilities Participate in the recruitment process based on your seniority. This includes interviews and creating tests as required, based on the need to recruit people who are compatible with our culture and skilled to accomplish the job. Share and coach colleagues in making the team more effective. Share and create content for training and publishing. These include blogs and webinars. Identify potential team members who are worthy of band change and prepare them with the required guidance Identify the repetitive tasks and manage to delegate to other team members or automate the process to reduce TAT & improve productivity Mandatory Technical Skills Proficient in SQL Python Should have strong understanding of clickstream data and building ETL Pipelines Cloud platform like GCP, AWS or Azure Exploratory Data Analysis Airflow Statistics Preferred Technical Skills Databricks Soft Skills Strong Problem Solving Business Acumen Strong Communication Cross-functional collaboration Stakeholder Management Certifications [Optional]: Google Cloud Certification - Professional Data Engineer Google Cloud Certification - Associate Data Practitioner Google Analytics 4 Certification (GAIQ)",Associate,,"Python, SQL, R, Data Analysis",
4247342694,GCP certified Data Engineer IRC178984,GlobalLogic,"Delhi, Delhi, India (On-site)",On-site,Full-time,,"About the job Description Join GlobalLogic, to be a valid part of the team working on a huge software project for the world-class company providing M2M / IoT 4G/5G modules e.g. to the automotive, healthcare and logistics industries. Through our engagement, we contribute to our customer in developing the end-user modules‚Äô firmware, implementing new features, maintaining compatibility with the newest telecommunication and industry standards, as well as performing analysis and estimations of the customer requirements. Requirements BA / BS degree in Computer Science, Mathematics or related technical field, or equivalent practical experience. Experience in Cloud SQL and Cloud Bigtable Experience in Dataflow, BigQuery, Dataproc, Datalab, Dataprep, Pub / Sub and Genomics Experience in Google Transfer Appliance, Cloud Storage Transfer Service, BigQuery Data Transfer Experience with data processing software (such as Hadoop, Kafka, Spark, Pig, Hive) and with data processing algorithms (MapReduce, Flume). Experience working with technical customers. Experience in writing software in one or more languages such as Java, Python 6-10 years of relevant consulting, industry or technology experience Strong problem solving and troubleshooting skills Strong communicator Job responsibilities Experience working data warehouses, including data warehouse technical architectures, infrastructure components, ETL / ELT and reporting / analytic tools and environments. Experience in technical consulting. Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments such as Google Cloud Platform (mandatory) and AWS / Azure(good to have) Experience working with big data, information retrieval, data mining or machine learning as well as experience in building multi-tier high availability applications with modern web technologies (such as NoSQL, Kafka,NPL, MongoDB, SparkML, Tensorflow). Working knowledge of ITIL and / or agile methodologies Google Data Engineer certified What we offer Culture of caring. At GlobalLogic, we prioritize a culture of caring. Across every region and department, at every level, we consistently put people first. From day one, you‚Äôll experience an inclusive culture of acceptance and belonging, where you‚Äôll have the chance to build meaningful connections with collaborative teammates, supportive managers, and compassionate leaders. Learning and development. We are committed to your continuous learning and development. You‚Äôll learn and grow daily in an environment with many opportunities to try new things, sharpen your skills, and advance your career at GlobalLogic. With our Career Navigator tool as just one example, GlobalLogic offers a rich array of programs, training curricula, and hands-on opportunities to grow personally and professionally. Interesting & meaningful work. GlobalLogic is known for engineering impact for and with clients around the world. As part of our team, you‚Äôll have the chance to work on projects that matter. Each is a unique opportunity to engage your curiosity and creative problem-solving skills as you help clients reimagine what‚Äôs possible and bring new solutions to market. In the process, you‚Äôll have the privilege of working on some of the most cutting-edge and impactful solutions shaping the world today. Balance and flexibility. We believe in the importance of balance and flexibility. With many functional career areas, roles, and work arrangements, you can explore ways of achieving the perfect balance between your work and life. Your life extends beyond the office, and we always do our best to help you integrate and balance the best of work and life, having fun along the way! High-trust organization. We are a high-trust organization where integrity is key. By joining GlobalLogic, you‚Äôre placing your trust in a safe, reliable, and ethical global company. Integrity and trust are a cornerstone of our value proposition to our employees and clients. You will find truthfulness, candor, and integrity in everything we do. About GlobalLogic GlobalLogic, a Hitachi Group Company, is a trusted digital engineering partner to the world‚Äôs largest and most forward-thinking companies. Since 2000, we‚Äôve been at the forefront of the digital revolution ‚Äì helping create some of the most innovative and widely used digital products and experiences. Today we continue to collaborate with clients in transforming businesses and redefining industries through intelligent products, platforms, and services.",manager,,"Python, SQL, Machine Learning",
4251668965,Remote Python AI Engineer - 17852,Turing,"Delhi, India (Remote)",Save Remote Python AI Engineer - 17852¬†  at Turing,Contract,,"About the job Work on Real-World Problems with Global Tech Experts Join a leading U.S.-based technology company as a Python Developer / AI Engineer, where you‚Äôll tackle real-world challenges and build innovative solutions alongside top global experts. This is a fully remote, contract-based opportunity ideal for developers passionate about Python, data analysis, and AI-driven work. Key Responsibilities: Write efficient, production-grade Python code to solve complex problems. Analyze public datasets and extract meaningful insights using Python and SQL. Collaborate with researchers and global teams to iterate on data-driven ideas. Document all code and development decisions in Jupyter Notebooks or similar platforms. Maintain high-quality standards and contribute to technical excellence. Job Requirements: Open to all levels: junior, mid-level, or senior engineers. Degree in Computer Science, Engineering, or equivalent practical experience. Proficient in Python programming for scripting, automation, or backend development. Experience with SQL/NoSQL databases is a plus. Familiarity with cloud platforms (AWS, GCP, Azure) is advantageous. Must be able to work 5+ hours overlapping with Pacific Time (PST/PT). Strong communication and collaboration skills in a remote environment. Perks & Benefits: Work on cutting-edge AI and data projects impacting real-world use cases. Collaborate with top minds from Meta, Stanford, and Google. 100% remote ‚Äì work from anywhere. Contract role with flexibility and no traditional job constraints. Competitive compensation in USD, aligned with global tech standards. Selection Process: Shortlisted developers may be asked to complete an assessment. If you clear the assessment, you will be contacted for contract assignments with expected start dates, durations, and end dates. Some contract assignments require fixed weekly hours, averaging 20/30/40 hours per week for the duration of the contract assignment.",,,"Python, SQL, Data Analysis",
4243268545,Data Engineer,Dailoqa,"Noida, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Key Responsibilities: Data Pipeline Development: Design, develop, and optimize scalable data pipelines using Databricks, Snowflake, and Azure Data Factory (ADF). Implement ETL/ELT processes for structured and unstructured data across data lakes and warehouses. Data Modeling & Optimization: Create efficient data models (e.g., star schema, snowflake schema) for Snowflake to ensure optimal query performance. Optimize Databricks workflows for performance tuning and scalability. Integration & Collaboration: Integrate Snowflake with other systems (e.g., ADLS Gen2, Salesforce) and configure APIs for seamless data flow. Collaborate with cross-functional teams, including data scientists, analysts, and infrastructure engineers, to deliver end-to-end solutions. Automation & CI/CD: Develop reusable jobs and implement CI/CD pipelines using GitHub Actions or Azure DevOps. Automate source-to-target mappings and data lineage tracking using tools like Collibra. Troubleshooting & Documentation: Identify and resolve performance bottlenecks in Snowflake queries or Databricks jobs. Document workflows, processes, and technical solutions to ensure clarity and maintainability. Required Skills Hands-on experience with Snowflake, including schema design, query optimization, and SnowSQL scripting. Proficiency in Databricks, including PySpark workflows and distributed computing frameworks. Strong knowledge of ETL/ELT processes and tools like Azure Data Factory (ADF) or DBT Labs. Expertise in SQL for data manipulation and transformation; familiarity with NoSQL databases is a plus. Experience integrating cloud platforms such as Azure or AWS with Snowflake/Databricks. Familiarity with CI/CD pipelines using GitHub Actions or Azure DevOps. Qualifications Bachelor‚Äôs degree in Computer Science, Information Systems, or a related field. 2‚Äì4 years of experience as a Database Engineer or Data Engineer working with Snowflake and Databricks. Certifications in Snowflake or Databricks are a plus (e.g., SnowPro Certification). Desired Skills and Experience Databricks,Snowflake,Pyspark,Azure,ETL",,,SQL,
4250232412,AWS Data Engineer,Mphasis,Greater Bengaluru Area (Remote),Remote,Contract,,"About the job We are looking for 10+ years of experience Data/Observability Engineer, who would leverage powerfully insightful data to inform our systems and solutions, and we‚Äôre seeking an experienced pipeline-centric data engineer. -Three or more years of experience with Python, SQL, and data visualization/exploration tools -Familiarity with the AWS ecosystem, specifically Redshift and RDS. -Strong understanding of distributed systems: They need to understand the complexities of modern architectures, including microservices, cloud-native environments, and hybrid infrastructure. -Proficiency in observability tools: They are familiar with tools for logging, metrics, and tracing, such as ELK Stack, Prometheus, Grafana, and distributed tracing systems. -Data analysis and visualization skills: They can analyze telemetry data to identify trends and patterns and create visualizations to communicate insights. -Should have experience with cloud platforms like AWS, Azure, and GCP -Experience in Insurance domain - Need candidates who knows Japanese language.",,,"Python, SQL, Data Analysis",
4185639419,Data Engineer I / II,Zeta,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Zeta is a Next-Gen Banking Tech company that empowers banks and fintechs to launch banking products for the future. It was founded by Bhavin Turakhia and Ramki Gaddipati in 2015. Our flagship processing platform - Zeta Tachyon - is the industry‚Äôs first modern, cloud-native, and fully API-enabled stack that brings together issuance, processing, lending, core banking, fraud & risk, and many more capabilities as a single-vendor stack. 20M+ cards have been issued on our platform globally. Zeta is actively working with the largest Banks and Fintechs in multiple global markets transforming customer experience for multi-million card portfolios. Zeta has over 1700+ employees - with over 70% roles in R&D - across locations in the US, EMEA, and Asia. We raised $280 million at a $1.5 billion valuation from Softbank, Mastercard, and other investors in 2021. Learn more @ www.zeta.tech , careers.zeta.tech , Linkedin , Twitter Responsibilities Design, develop & maintain data platform services for varied use cases such as Data Extraction, Data Processing, Data Ingestion, Data Observability, and Data Discovery. Own and operate the analytical data warehouse/data lake. Manage the deployment of services onto multiple data centers across the globe. Enable performance monitoring and auto-scaling of the platform services. Contribute to establishing and enforcing coding & operational best practices for the team. Skills Excellent command of one or more programming languages, preferably Python or Java. Excellent SQL skills. Knowledge of Flink, Airflow. Knowledge of DBT. Experience working with Kubernetes. Strong knowledge of architecture & internals of Apache Spark with multiple years of hand-on experience. Experience working with distributed SQL engines like Athena / Presto. Experience in building ETL Data Pipelines. Experience And Qualifications 3-5 years of experience in data engineering, BI engineering, and data warehouse development. Bachelor‚Äôs/Master‚Äôs degree in engineering (computer science, information systems). Ability to cut through the buzzwords and pick the right tools for building systems centred on core principles of reliability, scalability and maintainability. Life At Zeta At Zeta, we want you to grow to be the best version of yourself by unlocking the great potential that lies within you. This is why our core philosophy is ‚ÄòPeople Must Grow.‚Äô We recognize your aspirations; act as enablers by bringing you the right opportunities, and let you grow as you chase disruptive goals. is adventurous and exhilarating at the same time. You get to work with some of the best minds in the industry and experience a culture that values the diversity of thoughts. If you want to push boundaries, learn continuously and grow to be the best version of yourself, Zeta is the place to be! Explore the life at zeta Zeta is an equal opportunity employer. At Zeta, we are committed to equal employment opportunities regardless of job history, disability, gender identity, religion, race, marital/parental status, or another special status. We are proud to be an equitable workplace that welcomes individuals from all walks of life if they fit the roles and responsibilities.",,,"Python, SQL, R",
4250537836,Data Engineer,Nielsen,"Gurugram, Haryana, India (Hybrid)",Hybrid,Full-time,,"About the job Job Title: Principal Data Engineer (MTS4 / Principal Engineer) About the Role As a Principal Data Engineer, you will drive the strategy, architecture, and execution of large-scale data solutions across our function. This role involves tackling highly ambiguous, complex challenges where the business problem may not be fully defined at the outset. You will partner closely with cross-functional teams (Engineering, Product, Operations) to shape and deliver our data roadmap. Your work will have a profound impact on our functions' data capabilities, influencing multiple teams‚Äô technical and product direction. You should bring deep expertise in designing and developing robust data pipelines and platforms, leveraging technologies such as Spark, Airflow, Kafka, and other emerging tools. You will set standards and best practices that raise the bar for engineering excellence across the organization. Key Responsibilities Architect & Define Scope Own end-to-end design of critical data pipelines and platforms in an environment characterized by high ambiguity. Translate loosely defined business objectives into a clear technical plan, breaking down complex problems into achievable milestones. Technology Leadership & Influence Provide thought leadership in data engineering, driving the adoption of Spark, Airflow, Kafka, and other relevant technologies (e.g., Hadoop, Flink, Kubernetes, Snowflake, etc.). Lead design reviews and champion best practices for coding, system architecture, data quality, and reliability. Influence senior stakeholders (Engineers, EMs, Product Managers) on technology decisions and roadmap priorities. Execution & Delivery Spearhead strategic, multi-team projects that advance the organization‚Äôs data infrastructure and capabilities. Deconstruct complex architectures into simpler components that can be executed by various teams in parallel. Drive operational excellence, owning escalations and ensuring high availability, scalability, and cost-effectiveness of our data solutions. Mentor and develop engineering talent, fostering a culture of collaboration and continuous learning. Impact & Technical Complexity Shape how the organization operates by introducing innovative data solutions and strategic technical direction. Solve endemic, highly complex data engineering problems with robust, scalable, and cost-optimized solutions. Continuously balance short-term business needs with long-term architectural vision. Process Improvement & Best Practices Set and enforce engineering standards that elevate quality and productivity across multiple teams. Lead by example in code reviews, automation, CI/CD practices, and documentation. Champion a culture of continuous improvement, driving adoption of new tools and methodologies to keep our data ecosystem cutting-edge. Qualifications Education & Experience : Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Engineering, or related field (or equivalent experience). 5+ years of software/data engineering experience, with significant exposure to large-scale distributed systems. Technical Expertise : Demonstrated proficiency with Spark, Airflow, Kafka, and at least one major programming language (e.g., Python, Scala, Java). Experience with data ecosystem technologies such as Hadoop, Flink, Snowflake, Kubernetes, etc. Proven track record of architecting and delivering highly scalable data infrastructure solutions. Leadership & Communication : Ability to navigate and bring clarity in ambiguous situations. Strong cross-functional collaboration skills, influencing both technical and non-technical stakeholders. Experience coaching and mentoring senior engineers. Problem-Solving : History of tackling complex, ambiguous data challenges and delivering tangible results. Comfort making informed trade-offs between opportunity vs. architectural complexity.",Manager,,Python,
4228900910,Principal Data Engineer,MakeMyTrip,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Position: Principal Data Engineer Experience: Must have 8+ years of experience About Role: We are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences. About us: Nurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India‚Äôs online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010. Post-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018. GO-MMT is the corporate entity of three giants in the Online Travel Industry‚ÄîGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry. About the team: MakeMyTrip as India‚Äôs leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs. Data Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement. There are two key responsibilities of Data Engineering team: One to develop the platform for data capture, storage, processing, serving and querying. Second is to develop data products starting from; o personalization & recommendation platform o customer segmentation & intelligence o data insights engine for persuasions and o the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users We developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences. Technology experience : Extensive experience working with large data sets with hands-on technology skills to design and build robust data architecture Extensive experience in data modeling and database design At least 6+ years of hands-on experience in Spark/BigData Tech stack Stream processing engines ‚Äì Spark Structured Streaming/Flink Analytical processing on Big Data using Spark At least 6+ years of experience in Scala Hands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems At least 2+ years of cloud deployment experience ‚Äì AWS | Azure | Google Cloud Platform At least 2+ product deployments of big data technologies ‚Äì Business Data Lake, NoSQL databases etc Awareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies Should have experience in architecting and implementing domain centric big data solutions Ability to frame architectural decisions and provide technology leadership & direction Excellent problem solving, hands-on engineering, and communication skills",,,"SQL, Machine Learning",
4232505385,Data Engineer,PayPal,"Chennai, Tamil Nadu, India",,Full-time,,"About the job The Company PayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy. We operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers. We offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade. Our beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do ‚Äì and they push us to ensure we take care of ourselves, each other, and our communities. Job Description Summary: Meet Your Team At the heart of our fintech innovation lies the Data Engineering & Analytics Team ‚Äî a tight-knit group of engineers, data scientists, and analysts building the real-time intelligence layer that powers everything from fraud detection to personalized financial recommendations. You‚Äôll work alongside team members who bring experience from top-tier tech, finance, and environments, and who value clean architecture, reproducible data science, and high-velocity experimentation. We don‚Äôt just crunch numbers ‚Äî we build the data backbone that makes real-time decisioning possible across millions of user interactions. Whether it‚Äôs building low-latency pipelines, optimizing streaming systems, or unlocking insights from noisy data, we‚Äôre driven by impact ‚Äî and we‚Äôre looking for someone who‚Äôs excited to build with us. What you need to know about the role As a Data Engineer in our team, you‚Äôll play a critical role in building and maintaining real-time and near real-time data pipelines that transform raw data into meaningful, trustworthy datasets for our stakeholders across the organization. This role involves backend development primarily in Java, with a focus on building scalable, low-latency systems that support timely and accurate decision-making. You‚Äôll solve both technical and domain challenges ‚Äî from optimizing data ingestion and transformation, to aligning with evolving compliance and regulatory needs. You‚Äôll collaborate closely with engineering peers, compliance analysts, and product managers to deliver solutions that are audit-ready, scalable, and aligned with our broader data strategy. This is a high-impact role that offers deep exposure to the company‚Äôs products, platforms, and data governance practices ‚Äî an excellent opportunity to shape how compliance data is consumed in real time and help ensure the organization remains both agile and compliant. Job Description: Your way to impact At PayPal, Backend Software Engineers are the architects of our global payment platform. You'll design, develop, and optimize core systems that power millions of transactions daily, directly impacting our customers' experiences and our company's success. Your day-to-day As a Senior Software Engineer - Backend , you'll design and implement backend solutions. You'll collaborate with cross-functional teams to deliver high-quality products. Design and develop scalable backend systems. Optimize system performance and reliability. Mentor junior engineers. What Do You Need To Bring Bachelor's degree in Computer Science or related field. 3-5 years of backend development experience. Proficiency in at least one backend language (Java, Python, Ruby on Rails) Advanced proficiency in backend development with either Java EE frameworks, including experience with Spring MVC, or Hibernate. Experience designing and implementing RESTful services, focusing on scalability and reliability, using Java. Proven ability to mentor junior engineers and contribute to code reviews and design discussions. Experience with cloud platforms (AWS, GCP, Azure) Experience with databases (SQL, NoSQL) Strong understanding of database design, including SQL and NoSQL databases, and experience with ORM tools. Preferred Qualifications Experience with large-scale, high-performance systems. Knowledge of the payment processing industry and relevant regulations. Experience with cloud platforms (AWS, GCP, Azure). Contributions to open-source projects. ""We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don‚Äôt hesitate to apply."" For the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations. Our Benefits: At PayPal, we‚Äôre committed to building an equitable and inclusive global economy. And we can‚Äôt do this without our most important asset‚Äîyou. That‚Äôs why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you. We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com Who We Are: To learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx Commitment to Diversity and Inclusion PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com. Belonging at PayPal: Our employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal. Any general requests for consideration of your skills, please Join our Talent Community. We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don‚Äôt hesitate to apply. REQ ID R0125613",manager,,"Python, SQL",
4245421380,Data Engineer-Data Platforms-Google,IBM,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results Preferred Education Master's Degree Required Technical And Professional Expertise Develop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.). Expose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities. Analyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.). Prepare the effort estimates, WBS, staffing plan, RACI, RAID etc. . Leads the team to adopt right tools for various migration and modernization method Preferred Technical And Professional Experience You thrive on teamwork and have excellent verbal and written communication skills. Ability to communicate with internal and external clients to understand and define business needs, providing analytical solutions Ability to communicate results to technical and non-technical audiences",Associate,,Machine Learning,
4229160736,Financial Markets- Data Engineer,PwC Acceleration Centers in India,Greater Kolkata Area (On-site),On-site,Full-time,,"About the job At PwC, our people in finance consulting specialise in providing consulting services related to financial management and strategy. These individuals analyse client needs, develop financial solutions, and offer guidance and support to help clients optimise their financial performance, improve decision-making, and achieve their financial goals. As a finance consulting generalist at PwC, you will possess a broad understanding of various aspects of finance consulting. Your work will involve providing comprehensive guidance and support to clients in optimising their financial performance, improving decision-making, and achieving their financial goals. You will be responsible for analysing client needs, developing financial solutions, and offering recommendations tailored to specific business requirements. Driven by curiosity, you are a reliable, contributing member of a team. In our fast-paced environment, you are expected to adapt to working with a variety of clients and team members, each presenting varying challenges and scope. Every experience is an opportunity to learn and grow. You are expected to take ownership and consistently deliver quality work that drives value for our clients and success as a team. As you navigate through the Firm, you build a brand for yourself, opening doors to more opportunities. Skills Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to: Apply a learning mindset and take ownership for your own development. Appreciate diverse perspectives, needs, and feelings of others. Adopt habits to sustain high performance and develop your potential. Actively listen, ask questions to check understanding, and clearly express ideas. Seek, reflect, act on, and give feedback. Gather information from a range of sources to analyse facts and discern patterns. Commit to understanding how the business works and building commercial awareness. Learn and apply professional and technical standards (e.g. refer to specific PwC tax and audit guidance), uphold the Firm's code of conduct and independence requirements. Role Overview We are seeking a highly motivated Data Engineer - Associate to join our dynamic team. The ideal candidate will have a strong foundation in data engineering, particularly with Python and SQL, and will have exposure to cloud technologies and data visualization tools such as Power BI, Tableau, or QuickSight. The Data Engineer will work closely with data architects and business stakeholders to support the design and implementation of data pipelines and analytics solutions. This role o∆Øers an opportunity to grow technical expertise in cloud and data solutions, contributing to projects that drive business insights and innovation. Key Responsibilities Data Engineering: ÔÇ∑ Develop, optimize, and maintain data pipelines and workflows to ensure e∆Øicient data integration from multiple sources. ÔÇ∑ Use Python and SQL to design and implement scalable data processing solutions. ÔÇ∑ Ensure data quality and consistency throughout data transformation and storage processes. ÔÇ∑ Collaborate with data architects and senior engineers to build data solutions that meet business and technical requirements. Cloud Technologies ÔÇ∑ Work with cloud platforms (e.g., AWS, Azure, or Google Cloud) to deploy and maintain data solutions. ÔÇ∑ Support the migration of on-premise data infrastructure to the cloud environment when needed. ÔÇ∑ Assist in implementing cloud-based data storage solutions, such as data lakes and data warehouses. Data Visualization ÔÇ∑ Provide data to business stakeholders for visualizations using tools such as Power BI, Tableau, or QuickSight. ÔÇ∑ Collaborate with analysts to understand their data needs and optimize data structures for reporting. Collaboration And Support ÔÇ∑ Work closely with cross-functional teams, including data scientists and business analysts, to support data-driven decision-making. ÔÇ∑ Troubleshoot and resolve issues in the data pipeline and ensure timely data delivery. ÔÇ∑ Document processes, data flows, and infrastructure for team knowledge sharing. Required Skills And Experience ÔÇ∑ 0+ years of experience in data engineering, working with Python and SQL. ÔÇ∑ Exposure to cloud platforms such as AWS, Azure, or Google Cloud is preferred. ÔÇ∑ Familiarity with data visualization tools (e.g., Power BI, Tableau, QuickSight) is a plus. ÔÇ∑ Basic understanding of data modeling, ETL processes, and data warehousing concepts. ÔÇ∑ Strong analytical and problem-solving skills, with attention to detail. Qualifications ÔÇ∑ Bachelor‚Äôs degree in Computer Science, Data Science, Information Technology, or related fields. ÔÇ∑ Basic knowledge of cloud platforms and services is advantageous. ÔÇ∑ Strong communication skills and the ability to work in a team-oriented environment.",Associate,,"Python, SQL, Tableau, Power BI",
4245420585,Data Engineer-Data Platforms-AWS,IBM,"Kochi, Kerala, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform Responsibilities Experienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases on Cloud Data Platforms (AWS) or HDFS Experienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform Experience in developing streaming pipelines Experience to work with Hadoop / AWS eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc Preferred Education Master's Degree Required Technical And Professional Expertise Minimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala ; Minimum 3 years of experience on Cloud Data Platforms on AWS; Experience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB Good to excellent SQL skills Exposure to streaming solutions and message brokers like Kafka technologies Preferred Technical And Professional Experience Certification in AWS and Data Bricks or Cloudera Spark Certified developers",,,"Python, SQL",
4230266391,Data Engineer,Tata Consultancy Services,"Karnataka, India (On-site)",On-site,Full-time,,"About the job TCS is hiring for ETL Testing role:- Skill- ETL Testing with strong SQL, Query Surge Exp. range- 6 to 8 years Location- Pan India Looking for immediate joiners Ex TCSers please do not apply Interested candidates can connect with me over my mail Id- survi.shrivastava@tcs.com with their updated resume... Thanks & Regards Survi Shrivastava HR associate, Talent Acquisition Group (TCS)",associate,,SQL,
4212335283,"Associate Manager, Data Engineer",MSD,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Job Description Associate Manager, Data Engineer The Opportunity Based in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare. Be part of an organisation driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products. Drive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the world's greatest health threats. Our Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our companys‚Äô IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy. A focused group of leaders in each Tech Center helps ensure we can manage and improve each location, from investing in the growth, success, and well-being of our people to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. Together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers. Role Overview A Data Engineer is responsible for designing, building, and maintaining robust data pipelines and infrastructure that facilitate the collection, storage, and processing of large datasets. They collaborate with data scientists and analysts to ensure data is accessible, reliable, and optimized for analysis. Key tasks include data integration, ETL (Extract, Transform, Load) processes, and managing databases and cloud-based systems. Data engineers play a crucial role in enabling data-driven decision-making and ensuring data quality across organizations. What Will You Do In This Role Design, implement, and maintain complex data engineering solutions to acquire and prepare data. Create and maintain data pipelines to connect data within and between data stores, applications, and organizations. Support and monitor the implementation of information, record management policies, standard practice, effective control for internal delegation, audit, and control relating to information and records management. Identify issues related to software development activities and propose practical solutions to resolve issues. Establish, modify, maintain, and communicate the details of the data structures and associated components to others using the data structures, standard data modeling, and design techniques. Prepare design options for the working environment of methods, procedures, techniques, tools, and people. Select systems and software life cycle (SDLC) working practices for software components and micro-services. Create and document detailed designs for simple data applications and components. Contribute to the design of components of larger data systems. Establish, modify and maintain data structures, data pipelines and associated components. Work within a matrix organizational structure, reporting to both the functional manager and the project manager. Participate in project planning, execution, and delivery, ensuring alignment with both functional and project goals. What Should You Have Bachelors‚Äô degree in Information Technology, Computer Science or any Technology stream. 1+ years of developing data pipelines & data infrastructure, ideally within a drug development or life sciences context. Demonstrated expertise in delivering large-scale information management technology solutions encompassing data integration and self-service analytics enablement. Experienced in software/data engineering practices (including versioning, release management, deployment of datasets, agile & related software tools). Strong working knowledge of at least one large-scale data processing technology (e.g., High-performance computing, distributed computing), databases, and underlying technology (cloud or on-prem environments, containerization, distributed storage & databases) Cloud-native, ideally AWS certified. Good interpersonal and communication skills (verbal and written). Proven record of delivering high-quality results. Product and customer-centric approach. Innovative thinking, experimental mindset. Our technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation. Who We Are We are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world. What We Look For Imagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are among the intellectually curious, join us‚Äîand start making your impact today. #HYDIT2025 Current Employees apply HERE Current Contingent Workers apply HERE Search Firm Representatives Please Read Carefully Merck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails. Employee Status Regular Relocation VISA Sponsorship Travel Requirements Flexible Work Arrangements Hybrid Shift Valid Driving License Hazardous Material(s) Required Skills Business Intelligence (BI), Database Administration, Data Engineering, Data Management, Data Modeling, Data Visualization, Design Applications, Information Management, Software Development, Software Development Life Cycle (SDLC), System Designs Preferred Skills Job Posting End Date 07/1/2025 A job posting is effective until 11 59 59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date. Requisition ID R337347",Associate,,,
4219861877,Data Engineer,United Airlines,"Gurugram, Haryana, India",,Full-time,,"About the job Achieving our goals starts with supporting yours. Grow your career, access top-tier health and wellness benefits, build lasting connections with your team and our customers, and travel the world using our extensive route network. Come join us to create what‚Äôs next. Let‚Äôs define tomorrow, together. Description Description - External United's Kinective Media Data Engineering team designs, develops, and maintains massively scaling ad- technology solutions brought to life with innovative architectures, data analytics, and digital solutions. Our Values : At United Airlines, we believe that inclusion propels innovation and is the foundation of all that we do. Our Shared Purpose: ""Connecting people. Uniting the world."" drives us to be the best airline for our employees, customers, and everyone we serve, and we can only do that with a truly diverse and inclusive workforce. Our team spans the globe and is made up of diverse individuals all working together with cutting-edge technology to build the best airline in the history of aviation. With multiple employee-run ""Business Resource Group"" communities and world-class benefits like health insurance, parental leave, and space available travel, United is truly a one-of-a-kind place to work that will make you feel welcome and accepted. Come join our team and help us make a positive impact on the world. Job Overview And Responsibilities Data Engineering organization is responsible for driving data driven insights & innovation to support the data needs for commercial projects with a digital focus. Data Engineer will be responsible to partner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives that create sustainable revenue and share growth. Execute unit tests and validating expected results to ensure accuracy & integrity of data and applications through analysis, coding, writing clear documentation and problem resolution. This role will also drive the adoption of data processing and analysis within the AWS environment and help cross train other members of the team. Leverage strategic and analytical skills to understand and solve customer and business centric questions. Coordinate and guide cross-functional projects that involve team members across all areas of the enterprise, vendors, external agencies and partners Leverage data from a variety of sources to develop data marts and insights that provide a comprehensive understanding of the business. Develop and implement innovative solutions leading to automation Use of Agile methodologies to manage projects Mentor and train junior engineers. This position is offered on local terms and conditions. Expatriate assignments and sponsorship for employment visas, even on a time-limited visa status, will not be awarded. This position is for United Airlines Business Services Pvt. Ltd - a wholly owned subsidiary of United Airlines Inc. Qualifications Qualifications - External Required BS/BA, in computer science or related STEM field 2+ years of IT experience in software development 2+ years of development experience using Java, Python, Scala 2+ years of experience with Big Data technologies like PySpark, Hadoop, Hive, HBASE, Kafka, Nifi 2+ years of experience with database systems like redshift,MS SQL Server, Oracle, Teradata. Creative, driven, detail-oriented individuals who enjoy tackling tough problems with data and insights Individuals who have a natural curiosity and desire to solve problems are encouraged to apply 2+ years of IT experience in software development 2+ years of development experience using Java, Python, Scala Must be legally authorized to work in India for any employer without sponsorship Successful completion of interview required to meet job qualification Reliable, punctual attendance is an essential function of the position Must be legally authorized to work in India for any employer without sponsorship Must be fluent in English (written and spoken) Successful completion of interview required to meet job qualification Reliable, punctual attendance is an essential function of the position Preferred Masters in computer science or related STEM field Experience with cloud based systems like AWS, AZURE or Google Cloud Certified Developer / Architect on AWS Strong experience with continuous integration & delivery using Agile methodologies Data engineering experience with transportation/airline industry Strong problem-solving skills Strong knowledge in Big Data GGN00002011",,,"Python, SQL",
4212328981,"Associate Manager, Data Engineer",MSD,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Job Description Associate Manager, Data Engineer The Opportunity Based in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare. Be part of an organisation driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products. Drive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the world's greatest health threats. Our Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our companys‚Äô IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy. A focused group of leaders in each Tech Center helps ensure we can manage and improve each location, from investing in the growth, success, and well-being of our people to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. Together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers. Role Overview A Data Engineer is responsible for designing, building, and maintaining robust data pipelines and infrastructure that facilitate the collection, storage, and processing of large datasets. They collaborate with data scientists and analysts to ensure data is accessible, reliable, and optimized for analysis. Key tasks include data integration, ETL (Extract, Transform, Load) processes, and managing databases and cloud-based systems. Data engineers play a crucial role in enabling data-driven decision-making and ensuring data quality across organizations. What Will You Do In This Role Design, implement, and maintain complex data engineering solutions to acquire and prepare data. Create and maintain data pipelines to connect data within and between data stores, applications, and organizations. Support and monitor the implementation of information, record management policies, standard practice, effective control for internal delegation, audit, and control relating to information and records management. Identify issues related to software development activities and propose practical solutions to resolve issues. Establish, modify, maintain, and communicate the details of the data structures and associated components to others using the data structures, standard data modeling, and design techniques. Prepare design options for the working environment of methods, procedures, techniques, tools, and people. Select systems and software life cycle (SDLC) working practices for software components and micro-services. Create and document detailed designs for simple data applications and components. Contribute to the design of components of larger data systems. Establish, modify and maintain data structures, data pipelines and associated components. Work within a matrix organizational structure, reporting to both the functional manager and the project manager. Participate in project planning, execution, and delivery, ensuring alignment with both functional and project goals. What Should You Have Bachelors‚Äô degree in Information Technology, Computer Science or any Technology stream. 1+ years of developing data pipelines & data infrastructure, ideally within a drug development or life sciences context. Demonstrated expertise in delivering large-scale information management technology solutions encompassing data integration and self-service analytics enablement. Experienced in software/data engineering practices (including versioning, release management, deployment of datasets, agile & related software tools). Strong working knowledge of at least one large-scale data processing technology (e.g., High-performance computing, distributed computing), databases, and underlying technology (cloud or on-prem environments, containerization, distributed storage & databases) Cloud-native, ideally AWS certified. Good interpersonal and communication skills (verbal and written). Proven record of delivering high-quality results. Product and customer-centric approach. Innovative thinking, experimental mindset. Our technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation. Who We Are We are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world. What We Look For Imagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are among the intellectually curious, join us‚Äîand start making your impact today. #HYDIT2025 Current Employees apply HERE Current Contingent Workers apply HERE Search Firm Representatives Please Read Carefully Merck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails. Employee Status Regular Relocation VISA Sponsorship Travel Requirements Flexible Work Arrangements Hybrid Shift Valid Driving License Hazardous Material(s) Required Skills Business Intelligence (BI), Database Administration, Data Engineering, Data Management, Data Modeling, Data Visualization, Design Applications, Information Management, Software Development, Software Development Life Cycle (SDLC), System Designs Preferred Skills Job Posting End Date 07/1/2025 A job posting is effective until 11 59 59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date. Requisition ID R337348",Associate,,,
4222071443,Data Engineer,IDFC FIRST Bank,"Mumbai, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Job Requirements Job Requirements Role/ Job Title: Data Engineer - Gen AI Function/ Department: Data & Analytics Place of Work: Mumbai Job Purpose The data engineer will be working with our data scientists who are building solutions using generative AI in the domain of text, audio and images and tabular data. They will be responsible for working with large volumes of structured and unstructured data in its storage, retrieval and augmentation with our GenAI solutions which use the said data. Job & Responsibilities Build data engineering pipeline focused on unstructured data pipelines Conduct requirements gathering and project scoping sessions with subject matter experts, business users, and executive stakeholders to discover and define business data needs in GenAI. Design, build, and optimize the data architecture and extract, transform, and load (ETL) pipelines to make them accessible for Data Scientists and the products built by them. Work on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer, versed with API and its usability Drive the highest standards in data reliability, data integrity, and data governance, enabling accurate, consistent, and trustworthy data sets A suitable candidate will also demonstrate experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, MongoDB, DynamoDB, etc. Creating Technical Design Documentation of the projects/pipelines Good skills in technical debugging of the code in case of issues. Also, working with git for code versioning Education Qualification Graduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA) Post-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA Experience Range : 5-10 years of relevant experience",executive,,,
4247775513,Product Data Engineer,Responsive,"Coimbatore, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job We are seeking a highly skilled Product Data Engineer with expertise in building, maintaining, and optimizing data pipelines using Python scripting. The ideal candidate will have experience working in a Linux environment, managing large-scale data ingestion, processing files in S3, and balancing disk space and warehouse storage efficiently. This role will be responsible for ensuring seamless data movement across systems while maintaining performance, scalability, and reliability. Key Responsibilities: ETL Pipeline Development: Design, develop, and maintain efficient ETL workflows using Python to extract, transform, and load data into structured data warehouses. Data Pipeline Optimization: Monitor and optimize data pipeline performance, ensuring scalability and reliability in handling large data volumes. Linux Server Management: Work in a Linux-based environment, executing command-line operations, managing processes, and troubleshooting system performance issues. File Handling & Storage Management: Efficiently manage data files in Amazon S3, ensuring proper storage organization, retrieval, and archiving of data. Disk Space & Warehouse Balancing: Proactively monitor and manage disk space usage, preventing storage bottlenecks and ensuring warehouse efficiency. Error Handling & Logging: Implement robust error-handling mechanisms and logging systems to monitor data pipeline health. Automation & Scheduling: Automate ETL processes using cron jobs, Airflow, or other workflow orchestration tools. Data Quality & Validation: Ensure data integrity and consistency by implementing validation checks and reconciliation processes. Security & Compliance: Follow best practices in data security, access control, and compliance while handling sensitive data. Collaboration with Teams: Work closely with data engineers, analysts, and product teams to align data processing with business needs. Skills Required: Proficiency in Python: Strong hands-on experience in writing Python scripts for ETL processes. Linux Expertise: Experience working with Linux servers, command-line operations, and system performance tuning. Cloud Storage Management: Hands-on experience with Amazon S3, including handling file storage, retrieval, and lifecycle policies. Data Pipeline Management: Experience with ETL frameworks, data pipeline automation, and workflow scheduling (e.g., Apache Airflow, Luigi, or Prefect). SQL & Database Handling: Strong SQL skills for data extraction, transformation, and loading into relational databases and data warehouses. Disk Space & Storage Optimization: Ability to manage disk space efficiently, balancing usage across different systems. Error Handling & Debugging: Strong problem-solving skills to troubleshoot ETL failures, debug logs, and resolve data inconsistencies. Nice to Have: Experience with cloud data warehouses (e.g., Snowflake, Redshift, BigQuery). Knowledge of message queues (Kafka, RabbitMQ) for data streaming. Familiarity with containerization tools (Docker, Kubernetes) for deployment. Exposure to infrastructure automation tools (Terraform, Ansible). Qualifications: Bachelor‚Äôs degree in Computer Science, Data Engineering, or a related field. 4+ years of experience in ETL development, data pipeline management, or backend data engineering. Strong analytical mindset and ability to handle large-scale data processing efficiently. Ability to work independently in a fast-paced, product-driven environment.",,,"Python, SQL",
4229433220,Data Engineer,Takeda,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job By clicking the ‚ÄúApply‚Äù button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda‚Äôs Privacy Notice and Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge. Job Description: The Future Begins Here : At Takeda, we are leading digital evolution and global transformation. By building innovative solutions and future-ready capabilities, we are meeting the need of patients, our people, and the planet. Bengaluru, the city, which is India‚Äôs epicenter of Innovation, has been selected to be home to Takeda‚Äôs recently launched Innovation Capability Center. We invite you to join our digital transformation journey. In this role, you will have the opportunity to boost your skills and become the heart of an innovative engine that is contributing to global impact and improvement. At Takeda‚Äôs ICC we Unite in Diversity: Takeda is committed to creating an inclusive and collaborative workplace, where individuals are recognized for their backgrounds and abilities they bring to our company. We are continuously improving our collaborators journey in Takeda, and we welcome applications from all qualified candidates. Here, you will feel welcomed, respected, and valued as an important contributor to our diverse team. The Opportunity: As a Data Engineer you will be building and maintaining data systems and construct datasets that are easy to analyse and support Business Intelligence requirements as well as downstream systems. Responsibilities: Develops and maintains scalable data pipelines and builds out new integrations using AWS native technologies to support continuing increases in data source, volume, and complexity. Automate and Manage job scheduling for ETL processes across various applications and platforms within an enterprise. Design, implement, and maintain robust CI/CD pipelines to automate software deployments and data pipelines. Implements processes and systems to drive data reconciliation, monitor data quality, ensuring production data is always accurate and available for key stakeholders, downstream systems, and business processes that depend on it. Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Works closely with enterprise teams including Enterprise Architecture, Security, and Enterprise Data Backbone Engineering to design and develop data integration patterns/solutions along with proper data models supporting different data and analytics use cases. Skills and Qualifications: Bachelors‚Äô Degree, from an accredited institution in Engineering, Computer Science, or related field. 5+ years of total experience with data integration tools, workflow automation and DevOps. 3+ years of Experience with Tidal Automation and Tidal Repository for scheduling jobs and managing batch processes across different environments. Proficiency in Amazon Managed Apache Airflow to automate workflows and data pipelines. Strong experience with CI/CD tools like Github, Gitlab or Jforg. Strong programming and scripting skills in Python. Focusing on AWS native services and optimizing the data landscape through the adoption of these services. Experience with Agile development methodologies. Curiosity and adaptability to learn new technologies and improve existing processes. Excellent written and verbal communication skills including the ability to interact effectively with multifunctional teams. BENEFITS: It is our priority to provide competitive compensation and a benefit package that bridges your personal life with your professional career. Amongst our benefits are Competitive Salary + Performance Annual Bonus Flexible work environment, including hybrid working Comprehensive Healthcare Insurance Plans for self, spouse, and children Group Term Life Insurance and Group Accident Insurance programs Health & Wellness programs including annual health screening, weekly health sessions for employees. Employee Assistance Program 3 days of leave every year for Voluntary Service in additional to Humanitarian Leaves Broad Variety of learning platforms Diversity, Equity, and Inclusion Programs Reimbursements ‚Äì Home Internet & Mobile Phone Employee Referral Program Leaves ‚Äì Paternity Leave (4 Weeks) , Maternity Leave (up to 26 weeks), Bereavement Leave (5 calendar days) ABOUT ICC IN TAKEDA: Takeda is leading a digital revolution. We‚Äôre not just transforming our company; we‚Äôre improving the lives of millions of patients who rely on our medicines every day. As an organization, we are committed to our cloud-driven business transformation and believe the ICCs are the catalysts of change for our global organization. Locations: IND - Bengaluru Worker Type: Employee Worker Sub-Type: Regular Time Type: Full time",,,"Python, Data Analysis",
4188378780,Data Engineer 3,Comcast,"Karnataka, India (Remote)",Remote,Full-time,,"About the job Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary Responsible for designing, building and overseeing the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. Work with data modelers/analysts to understand the business problems they are trying to solve then create or augment data assets to feed their analysis. Has in-depth experience, knowledge and skills in own discipline. Usually determines own work priorities. Acts as resource for colleagues with less experience. Overview Job Description This key role is responsible for development and configuration of SAP Analytics cloud tool with in UNIVERSAL Corporate technology. It should deliver an efficient SAP Planning and Analytical capability to support multiple business sectors requirements. Key Responsibilities ‚Äì As an experienced SAP BW Developer you will be responsible for design, and build of BW solutions specific to SAP HANA, SAP S/4HANA, SAP Analytics Cloud and SAP BW4/HANA. Success in this role requires you to develop BW and BPC solutions on the BW/4 HANA platform and Datasphere, that includes integration of data sources and building complex reports. From there ‚Äì this resource should have the ability to build business reports based on HANA CDS Views from S/4 HANA and BW/4HANA. You will have the opportunity to work with a variety of technologies such as, native HANA modelling, HANA Smart Data Integration, SAP SLT for replication, ABAP CDS View development for analytics, SAP Analytics Cloud, SAP Datasphere and SAP BW4/HANA, Reporting tools like Power BI, Google Big Query and AWS redshift You will be required to build complex analytics solutions that requires data blending from various sources. You should be able to work at individual capacity and own the entire design to deployment of solution with minimal supervision. Lead and build of proposed analytic solutions, supporting development, unit testing, user acceptance testing, and go-live Strong programming experience in SAP ABAP. Strong understanding of SAP BPC and data integration to SAP BPC Work with extended Data and Analytics team to work on blended uses cases and combine data for these use cases. Create Data Analytics Reports using SAP AFO, SAP Cloud Analytic, Power BI and Big Query Collaborate with functional teams to translate user stories and problem statements into project/solution requirements. Work on latest technology developments using Python scripting for build AI based analytics and ML based data quality solutions in SAP platforms Develop solutions quickly through an Agile process utilizing incremental minimal viable products to obtain user feedback and iterate towards a final solution. Job requires working multiple projects in parallel and maintaining a set delivery schedule on all projects. Provide recommendations, develop customized solutions as needed, and manage follow-up & evaluation for all data and analytics solutions Developing new Planning scenarios, interfaces, DSO‚Äôs & other reporting structures, as and when identified by business needs/projects. Assist with identified stabilisation & simplification initiatives to improve overall SAP Analytics processes & performance. Demonstrate BW and BPC experience, including experience in various Data and Analytics solutions based on AWS, GCP and Azure. Demonstrated experience in various reporting technologies like SAP AOE, EPM, Power BI and SAP SAC Support and service Reviewing/Updating support & operations documentation where required Participating in production support rota during key activity periods when required month-end, quarter-end, year-end etc. Ensuring consistency of master data between SAC and related systems Reviewing & improving existing master data interfaces to eliminate master data issues in SAC, BW and BPC Liaising with other Business Engagement teams to ensure timely resolution of all issues and to promote good working relationships Other Responsibilities Knowledge of investigating data flows to and from SAC with SAP BW, SAP BPC systems, SAP S/4 and other non SAP sources Proven ability to analyse source systems and investigate data extracts for both SAP and non-SAP sources. Strong understanding of financial analytics and experience in delivering finance and HCM analytics Skills And Experience 5+ Years of SAP SAC planning and analytics experience - ability to link development requirements back to business requirements. Knowledge or certification on SAP Datasphere Knowledge of implementing AI and ML solutions in SAP data space Relationship building and influencing ability both inside and outside the organisation. Coaching and mentoring ability Taking ownership and ensuring high quality results Active in seeking feedback and making necessary changes. Specific Previous Experience ‚Äì Proven experience in implementing SAP SAC, SAP BOBJ, AFO and Power BI in a multinational environment. Experience in providing infrastructure requirements. Experience in providing SAP SAC user training. Requirements Written and verbal communication in English Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law. Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits to eligible employees. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That‚Äôs why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality ‚Äì to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Relevant Work Experience 5-7 Years",,,"Python, Power BI, Machine Learning",
4230896284,Data Engineer,Barclays,"Chennai, Tamil Nadu, India",,Full-time,,"About the job Join us as a Data Engineer at Barclays, where you will spearhead the evolution of our infrastructure and deployment pipelines, driving innovation and operational excellence. You will harness cutting-edge technology to build and manage robust, scalable and secure infrastructure, ensuring seamless delivery of our digital solutions. To be successful as a Data Engineer, you should have experience with: Hands on experience in pyspark and strong knowledge on Dataframes, RDD and SparkSQL. Hands on Experience in developing, testing and maintaining applications on AWS Cloud. Strong hold on AWS Data Analytics Technology Stack (Glue, S3, Lambda, Lake formation, Athena). Design and implement scalable and efficient data transformation/storage solutions using Snowflake. Experience in Data ingestion to Snowflake for different storage format such Parquet, Iceberg, JSON, CSV etc. Experience in using DBT (Data Build Tool) with snowflake for ELT pipeline development. Experience in Writing advanced SQL and PL SQL programs. Hands On Experience for building reusable components using Snowflake and AWS Tools/Technology. Should have worked at least on two major project implementations. Exposure to data governance or lineage tools such as Immuta and Alation is added advantage. Experience in using Orchestration tools such as Apache Airflow or Snowflake Tasks is added advantage. Knowledge on Abinitio ETL tool is a plus. Some Other Highly Valued Skills May Include Ability to engage with Stakeholders, elicit requirements/ user stories and translate requirements into ETL components. Ability to understand the infrastructure setup and be able to provide solutions either individually or working with teams. Good knowledge of Data Marts and Data Warehousing concepts. Resource should possess good analytical and Interpersonal skills. Implement Cloud based Enterprise data warehouse with multiple data platform along with Snowflake and NoSQL environment to build data movement strategy. You may be assessed on key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen, strategic thinking and digital and technology, as well as job-specific technical skills. The role is based out of Chennai. Purpose of the role To build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure. Accountabilities Build and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data. Design and implementation of data warehoused and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures. Development of processing and analysis algorithms fit for the intended data complexity and volumes. Collaboration with data scientist to build and deploy machine learning models. Analyst Expectations To meet the needs of stakeholders/ customers through specialist advice and support Perform prescribed activities in a timely manner and to a high standard which will impact both the role itself and surrounding roles. Likely to have responsibility for specific processes within a team They may lead and supervise a team, guiding and supporting professional development, allocating work requirements and coordinating team resources. They supervise a team, allocate work requirements and coordinate team resources. If the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L ‚Äì Listen and be authentic, E ‚Äì Energise and inspire, A ‚Äì Align across the enterprise, D ‚Äì Develop others. OR for an individual contributor, they manage own workload, take responsibility for the implementation of systems and processes within own work area and participate on projects broader than direct team. Execute work requirements as identified in processes and procedures, collaborating with and impacting on the work of closely related teams. Check work of colleagues within team to meet internal and stakeholder requirements. Provide specialist advice and support pertaining to own work area. Take ownership for managing risk and strengthening controls in relation to the work you own or contribute to. Deliver your work and areas of responsibility in line with relevant rules, regulation and codes of conduct. Maintain and continually build an understanding of how all teams in area contribute to the objectives of the broader sub-function, delivering impact on the work of collaborating teams. Continually develop awareness of the underlying principles and concepts on which the work within the area of responsibility is based, building upon administrative / operational expertise. Make judgements based on practise and previous experience. Assess the validity and applicability of previous or similar experiences and evaluate options under circumstances that are not covered by procedures. Communicate sensitive or difficult information to customers in areas related specifically to customer advice or day to day administrative requirements. Build relationships with stakeholders/ customers to identify and address their needs. All colleagues will be expected to demonstrate the Barclays Values of Respect, Integrity, Service, Excellence and Stewardship ‚Äì our moral compass, helping us do what we believe is right. They will also be expected to demonstrate the Barclays Mindset ‚Äì to Empower, Challenge and Drive ‚Äì the operating manual for how we behave.",,,"SQL, Machine Learning",
4230227519,Data Research Engineer - AI/ML,Forbes Advisor,Mumbai Metropolitan Region (Remote),Remote,Full-time,,"About the job Company Description Forbes Advisor is a new initiative for consumers under the Forbes Marketplace umbrella that provides journalist- and expert-written insights, news and reviews on all things personal finance, health, business, and everyday life decisions. We do this by providing consumers with the knowledge and research they need to make informed decisions they can feel confident in, so they can get back to doing the things they care about most. At Marketplace, our mission is to help readers turn their aspirations into reality. We arm people with trusted advice and guidance, so they can make informed decisions they feel confident in and get back to doing the things they care about most. We are an experienced team of industry experts dedicated to helping readers make smart decisions and choose the right products with ease. Marketplace boasts decades of experience across dozens of geographies and teams, including Content, SEO, Business Intelligence, Finance, HR, Marketing, Production, Technology and Sales. The team brings rich industry knowledge to Marketplace‚Äôs global coverage of consumer credit, debt, health, home improvement, banking, investing, credit cards, small business, education, insurance, loans, real estate and travel. The Data Extraction Team is a brand-new team who plays a crucial role in our organization by designing, implementing, and overseeing advanced web scraping frameworks. Their core function involves creating and refining tools and methodologies to efficiently gather precise and meaningful data from a diverse range of digital platforms. Additionally, this team is tasked with constructing robust data pipelines and implementing Extract, Transform, Load (ETL) processes. These processes are essential for seamlessly transferring the harvested data into our data storage systems, ensuring its ready availability for analysis and utilization. A typical day in the life of a Data Research Engineer will involve coming up with ideas regarding how the company/team can best harness the power of AI/LLM, and use it not only simplify operations within the team, but also to streamline the work of the research team in gathering/retrieving large sets of data. The role is that of a leader who sets a vision for the future of AI/LLM‚Äôs use within the team and the company. They think outside the box and are proactive in engaging with new technologies and developing new ideas for the team to move forward in the AI/LLM field. The candidate should also at least be willing to acquire some basic skills in scraping and data pipelining. Job Description Responsibilities : Develop methods to leverage the potential of LLM and AI within the team. Proactive at finding new solutions to engage the team with AI/LLM, and streamline processes in the team. Be a visionary with AI/LLM tools and predict how the use of future technologies could be harnessed early on so that when these technologies come out, the team is ahead of the game regarding how it could be used. Assist in acquiring and integrating data from various sources, including web crawling and API integration. Stay updated with emerging technologies and industry trends. Explore third-party technologies as alternatives to legacy approaches for efficient data pipelines. Contribute to cross-functional teams in understanding data requirements. Assume accountability for achieving development milestones. Prioritize tasks to ensure timely delivery, in a fast-paced environment with rapidly changing priorities. Collaborate with and assist fellow members of the Data Research Engineering Team as required. Leverage online resources effectively like StackOverflow, ChatGPT, Bard, etc., while considering their capabilities and limitations. Qualifications Skills and Experience Bachelor's degree in Computer Science, Data Science, or a related field. Higher Qualifications Is a Plus. Think proactively and creatively regarding the next AI/LLM technologies and how to use Them To The Team‚Äôs And Company‚Äôs Benefits. ‚ÄúThink outside the box‚Äù mentality. Experience prompting LLMs in a streamlined way, taking into account how the LLM can potentially ‚Äúhallucinate‚Äù and return wrong information. Experience building agentic AI platforms with modular capabilities and autonomous task execution. (crewai, lagchain, etc.) Proficient in implementing Retrieval-Augmented Generation (RAG) pipelines for dynamic knowledge integration. (chromadb, pinecone, etc) Experience managing a team of AI/LLM experts is a plus: this includes setting up goals and objectives for the team and fine-tuning complex models. Strong proficiency in Python programming Proficiency in SQL and data querying is a plus. Familiarity with web crawling techniques and API integration is a plus but not a must. Experience in AI/ML engineering and data extraction Experience with LLMs, NLP frameworks (spaCy, NLTK, Hugging Face, etc.) Strong understanding of machine learning frameworks (TensorFlow, PyTorch) Design and build AI models using LLMs Integrate LLM solutions with existing systems via APIs Collaborate with the team to implement and optimize AI solutions Monitor and improve model performance and accuracy Familiarity with Agile development methodologies is a plus. Strong problem-solving and analytical skills with attention to detail. Creative and critical thinking. Ability to work collaboratively in a team environment. Good and effective communication skills. Experience with version control systems, such as Git, for collaborative development. Ability to thrive in a fast-paced environment with rapidly changing priorities. Comfortable with autonomy and ability to work independently. Additional Information Perks : Day off on the 3rd Friday of every month (one long weekend each month) Monthly Wellness Reimbursement Program to promote health well-being Monthly Office Commutation Reimbursement Program Paid paternity and maternity leaves",,,"Python, SQL, Machine Learning",
4251694094,Data Engineer,NymCard,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Our Company In a digital world where users want easy transactions, Nymcard aims to simplify payments. Built from the ground up, our platform enables developers to launch new programs with speed, security, scalability and support. We are obsessed with making the complex easy. With the modern infrastructure that we offer through our open API‚Äôs, we believe we are the leading card issuing platform in the region. We are not just in the business of building software, we are in the business of enabling interactions. Companies of all sizes within the MENA region and across the globe from FinTech, E-commerce, Travel, On-Demand Delivery, Expense Management & Digital Banking trust NymCard to help them interact better with their users and enable them to experience frictionless payments at every transaction. Our mission is to enable companies to launch frictionless payment programmes with our modern infrastructure, at record speed. Main Duties And Responsibilities Redesign of our existing data warehouse using industry best practices and dimensional modeling techniques (star schema) Collaborate with our analytics and engineering teams to translate critical business reports into efficient data models Develop and implement data quality frameworks to ensure data integrity and reliability Propose and implement efficient approaches for data modeling based on our business requirements Create documentation for data models, ETL processes, and data pipeline architecture Work with the current team to transfer knowledge and upskill team members on data warehouse maintenance Skills And Qualifications 5+ years of experience in data modeling and data warehouse design Strong expertise in dimensional modeling techniques, particularly star schema implementation Proficiency with modern data stack tools including DBT, Airflow, and Postgres Experience with data quality tools like DBT-elementary or DBT expectations Strong Python programming skills for data pipeline development Experience migrating or redesigning existing data warehouses Ability to translate business requirements into technical specifications and data models You can leverage LLM skillfully to enhance your developer workflows Excellent communication skills with the ability to explain complex technical concepts to non-technical stakeholders Experience in collaborative development and knowledge transfer Experience with ClickHouse or other columnar database systems Knowledge of cloud data platforms (AWS, GCP, or Azure) or experience working with Cloud Data warehouse Background in financial services industry, especially cards Why Us? We‚Äôre the region‚Äôs leading Banking-as-a-Service platform, trusted to issue and process virtual, physical and tokenized cards for the most innovative fintechs and enterprises. Your code (or craft) will reach millions of end-users across multiple countries the moment it ships. Our 1,000-plus public APIs and real-time platform let product and engineering teams move from idea to launch in weeks, not quarters. You‚Äôll work with modern stacks, clean architectures and plenty of green-field projects. . Hybrid & remote-friendly roles, relocation support, generous annual leave, equity for key positions. Flat structure, candid feedback, and autonomy mean your voice counts from day one. Whether you‚Äôre crafting micro-services, designing intuitive UIs, or closing enterprise deals, you‚Äôll see the direct commercial impact of your work and celebrate it with a team that loves what they do. Join us at NymCard and help shape the future of payments‚Äîfaster, smarter, everywhere.",,,Python,
4249666910,Data Engineer 1,JLL,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job What this job involves: JLL, an international real estate management company, is seeking an Data Engineer to join our JLL Technologies Team. We are seeking candidates that are self-starters to work in a diverse and fast-paced environment that can join our Enterprise Data team. We are looking for a candidate that is responsible for designing and developing of data solutions that are strategic for the business using the latest technologies Azure Databricks, Python, PySpark, SparkSQL, Azure functions, Delta Lake, Azure DevOps CI/CD. Responsibilities Develop solutions leveraging cloud big data technology to ingest, process and analyze large, disparate data sets to exceed business requirements. Develop data lake solution to store structured and unstructured data from internal and external sources and provide technical guidance to help migrate colleagues to modern technology platform. Contribute and adhere to CI/CD processes, development best practices and strengthen the discipline in Data Engineering Org. Develop systems that ingest, cleanse and normalize diverse datasets, develop data pipelines from various internal and external sources and build structure for previously unstructured data. Using PySpark and Spark SQL, extract, manipulate, and transform data from various sources, such as databases, data lakes, APIs, and files, to prepare it for analysis and modeling. Perform the unit testing, system integration testing, regression testing and assist with user acceptance testing. Consults with the business to develop documentation and communication materials to ensure accurate usage and interpretation of JLL data. Implement data security best practices, including data encryption, access controls, and compliance with data protection regulations. Ensure data privacy, confidentiality, and integrity throughout the data engineering processes. Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Experience & Education Minimum of 2 years of experience as a data developer using Python, PySpark, Spark Sql, ETL knowledge, SQL Server, ETL Concepts. Bachelor‚Äôs degree in Information Science, Computer Science, Mathematics, Statistics or a quantitative discipline in science, business, or social science. Experience in Azure Cloud Platform, Databricks, Azure storage. Effective written and verbal communication skills, including technical writing. Excellent technical, analytical and organizational skills. Technical Skills & Competencies Experience handling un-structured, semi-structured data, working in a data lake environment, leveraging data streaming and developing data pipelines driven by events/queues Hands on Experience and knowledge on real time/near real time processing and ready to code Hands on Experience in PySpark, Databricks, and Spark Sql. Knowledge on json, Parquet and Other file format and work effectively with them No Sql Databases Knowledge like Hbase, Mongo, Cosmos etc. Preferred Cloud Experience on Azure or AWS Python-spark, Spark Streaming, Azure SQL Server, Cosmos DB/Mongo DB, Azure Event Hubs, Azure Data Lake Storage, Azure Search etc. Team player, Reliable, self-motivated, and self-disciplined individual capable of executing on multiple projects simultaneously within a fast-paced environment working with cross functional teams. What we can do for you: You‚Äôll join an entrepreneurial, inclusive culture. One where we succeed together ‚Äì across the desk and around the globe. Where like-minded people work naturally together to achieve great things. Our Total Rewards program reflects our commitment to helping you achieve your ambitions in career, recognition, well-being, benefits and pay. Join us to develop your strengths and enjoy a fulfilling career full of varied experiences. Keep those ambitions in sights and imagine where JLL can take you.",,,"Python, SQL, Data Analysis",
4248728564,Data Engineer / Testing,Zoetis,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job POSITION SUMMARY Summarize the primary purpose & key accountabilities of the position, including scope of responsibility in 5-7 concise sentences. (i.e. Global vs. Country/Region) It may be helpful to complete this section after you have finished the other sections of the document. Zoetis, Inc. is the world's largest producer of medicine and vaccinations for pets and livestock. The Zoetis Tech & Digital (ZTD) Global ERP organization is as a key building block of ZTD comprising of enterprise applications and systems platforms. Join us at Zoetis India Capability Center (ZICC) in Hyderabad, where innovation meets excellence. As part of the world's leading animal healthcare company, ZICC is at the forefront of driving transformative advancements and applying technology to solve the most complex problems. Our mission is to ensure sustainable growth and maintain a competitive edge for Zoetis globally by leveraging the exceptional talent in India. At ZICC, you'll be part of a dynamic team that partners with colleagues worldwide, embodying the true spirit of One Zoetis. Together, we ensure seamless integration and collaboration, fostering an environment where your contributions can make a real impact. Be a part of our journey to pioneer innovation and drive the future of animal healthcare. We are seeking a skilled Data Engineer to join our team. The ideal candidate will have experience with the Microsoft Azure technical stack and a passion for building robust data solutions. You will work closely with our data engineering team to design, develop, and maintain data pipelines and infrastructure, that serve our customers across our ZTD ecosystem. Join us in shaping the future of data-driven applications and engineering within the DX Data team. POSITION RESPONSIBILITIES In order of importance, list the primary responsibilities critical to the performance of the position. It is recommended not to list actual tasks but focus on essential responsibilities that highlight accountability and level of judgment required. Percent of Time (sum of responsibilities should equal to 100%) Data Solution Design: o Collaborate with cross-functional teams to deliver on data strategy, architecture, and governance. o Design and deploy data solutions in the cloud, ensuring scalability, performance, and cost-effectiveness. Data Pipelines and Integration: o Create and maintain data pipelines using Azure Databricks, Azure Data Factory, and other Azure services. o Extract, transform, and load (ETL) data from various sources into data warehouses or data lakes. o Ensure efficient data cleaning, conversion, and loading processes. Data Storage and Management: o Work with various Azure data storage options, including: ÔÇß Azure SQL Database ÔÇß Azure Data Lake Storage ÔÇß Azure Cosmos DB ÔÇß Azure Blob Storage o Design storage systems that meet organizational requirements. o Support operational Azure SQL databases, including performance monitoring, query optimization, and DBA type tasks. 65% Big Data and Analytics: o Utilize big data technologies such as Azure Databricks and Apache Spark, Delta. o Develop data processing workflows and pipelines to handle and analyze large volumes of data. o Support data analytics, machine learning, and other data-driven applications. 25% * Azure Functions Development: o Proficiency in creating, deploying, and managing serverless functions using Azure Functions. o Knowledge of triggers (e.g., HTTP, timer, queue, blob) and bindings (e.g., Cosmos DB, Azure Storage, Service Bus). * Event-Driven Architecture: o Understanding of event-driven design patterns and how to build scalable, event-triggered workflows. o Ability to integrate Azure Functions with other Azure services (e.g., Logic Apps, Event Grid). * Serverless Best Practices: o Familiarity with best practices for optimizing performance, monitoring, and error handling in serverless applications. 10% ORGANIZATIONAL RELATIONSHIPS Provide the primary groups or key positions that this position will have interaction with as a regular part of the position responsibilities. Include any external interactions as appropriate. Collaboration with product managers, product owners, scrum masters, development and testing teams. EDUCATION AND EXPERIENCE Indicate the formal education, certification or license required and/or preferred. Include the minimum number of years of relevant experience required for the position (where legally permissible). * A bachelor's degree in computer science, information technology, or a related field is required. * Minimum 8+ years of proven experience as a Data Engineer or in a similar role. TECHNICAL SKILLS REQUIREMENTS Indicate the technical skills required and/or preferred, as applicable. Required: * Azure Services: Strong hands-on experience with Azure Databricks, ADF, Azure SQL, CosmosDB, and Azure Synapse. * Data Technologies: Proficiency with Data Lake, data warehouse, Big Data, and Spark. * Programming Languages: Experience with Python, SQL, and Scala. * Data Processes: Familiarity with data modeling, ETL processes, and data warehousing concepts. * Analytical Skills: Excellent analytical and problem-solving skills with attention to detail. * Data Verification: Strong experience in verifying, comparing, and troubleshooting data across environments. * Version Control: Experience with applying version control systems for create and migration scripts. * Database Optimization: Expertise in best practices for database performance optimization. * Stored Procedures: Strong experience in creating complex functions and stored procedures. * SQL Knowledge: In-depth knowledge of SQL and relational database management systems (RDBMS). * Data Modeling: Experience in building and evolving data models (Conceptual, Logical, Physical). * Database Standards: In-depth knowledge of database best practices and standards. * Warehousing Concepts: Familiarity with data warehousing and business intelligence tools. * Database Systems: Experience with a range of database systems, from traditional RDBMS/SQL to NoSQL and other scalable database technologies. * Attention to Detail: Excellent attention to detail and a keen eye for usability issues. * Testing Techniques: Experience with manual and exploratory testing techniques. * Effectiveness: Proactive and collaborative team player with a strong commitment to meeting deadlines and delivering high-quality work. Efficient time management and prioritization skills Desired: * Passion for solving complex problems and making a difference. * Experience with product development and collaborating in agile teams. * Strong aptitude for learning new technologies, and continuous professional development. * The ability to collaborate with a diverse group of people. * Highly motivated and self-starting individual * Familiarity with bug tracking and collaboration tools (e.g., Jira, Zephyr Scale, Confluence) is a plus. PHYSICAL POSITION REQUIREMENTS Note the physical conditions in which work will be performed, if applicable to the position. Examples: Lifting, sitting, standing, walking, ability to travel, drive, unusual attendance requirements, weekend work or travel requirements, etc. Regular working hours are from 1:00 PM to 10:00 PM IST Sometimes, more overlap with the EST Time zone is required during production go-live. Some weekend work may be required to assist in data changes to systems during releases (estimated once every 2-3 months). This description indicates the general nature and level of work expected. It is not designed to cover or contain a comprehensive listing of activities or responsibilities required of the incumbent. Incumbent may be asked to perform other duties as required. Additional position specific requirements/responsibilities are contained in approved training curricula. About Zoetis At Zoetis , our purpose is to nurture the world and humankind by advancing care for animals. As a Fortune 500 company and the world leader in animal health, we discover, develop, manufacture and commercialize vaccines, medicines, diagnostics and other technologies for companion animals and livestock. We know our people drive our success. Our award-winning culture, built around our Core Beliefs, focuses on our colleagues' careers, connection and support. We offer competitive healthcare and retirement savings benefits, along with an array of benefits, policies and programs to support employee well-being in every sense, from health and financial wellness to family and lifestyle resources. Global Job Applicant Privacy Notice",manager,,"Python, SQL, Machine Learning",
4232297728,Data Engineer-Data Modeling,IBM,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities Translates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems. Skills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL. Data modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations Preferred Education Master's Degree Required Technical And Professional Expertise Translates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems. Skills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL. Data modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations. Therefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system. There are three different types of data models produced while progressing from requirements to the actual database to be used for the information system Preferred Technical And Professional Experience Translates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems. Skills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL. Data modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations",,,,
4248301791,"Lead Data Engineer, ITC",Nike,"Karnataka, India (On-site)",On-site,Full-time,,"About the job Who You‚Äôll Work With This role is part of the Nike‚Äôs Content Technology team within Consumer Product and Innovation (CP&I) organization, working very closely with the globally distributed Engineering and Product teams. This role will roll up to the Director Software Engineering based out of Nike India Tech Centre. Who We Are Looking For We are looking for experienced Technology focused and hands on Lead Engineer to join our team in Bengaluru, India. As a Lead Data Engineer, you will be responsible for designing, building, and maintaining scalable data pipelines and analytics solutions. As a Lead Data Engineer, you will play a key role in ensuring that our data products are robust and capable of supporting our Advanced Analytics and Business Intelligence initiatives. Bachelor‚Äôs degree or higher in Computer Science, Engineering, or a related field, or equivalent experience. An experienced data engineer with 6+ years in data engineering and at least 2 years in technical leadership roles. Deep hands-on expertise with Databricks, Snowflake, Spark, Delta Lake, and Apache Airflow (or similar workflow management tools). Expert in SQL and Spark optimization techniques. Strong command of Medallion architecture and data modeling. Experience integrating ML/GenAI pipelines is a plus. Exposure to Tableau or other BI tools. Strong leadership skills with a proven ability to lead and mentor data engineering teams. Excellent problem-solving skills and the ability to design solutions for complex data challenges. Skilled in collaborating cross-functionally and mentoring engineers. Effective communicator, able to work cross-functionally and translate technical concepts for non-technical stakeholders. Preferred: Experience with Kafka/Kinesis or real-time data processing, and certifications in Databricks or Spark (strongly preferred). What You‚Äôll Work On Architect and lead the development of scalable data pipelines and platform features. Translate business needs into technical solutions in collaboration with product teams, business stakeholders, and data science teams. Enforce best practices across teams, including governance, quality, and coding standards. Lead code reviews, pair programming, and mentoring. Troubleshoot complex systems and optimize distributed pipelines. Automate deployments using CI/CD and DevOps practices.",Director,,"SQL, Tableau",
4247110254,Data Engineer,IntraEdge,"Chennai, Tamil Nadu, India (Hybrid)",Hybrid,Full-time,,"About the job Role - Data Engineer (Backend Java & Python) Location - Chennai Work Modal - Hybrid We are looking for a highly skilled Backend Data Engineer to join our growing FinTech team. In this role, you will design and implement robust data models and architectures, build scalable data ingestion pipelines, and ensure data quality across financial datasets. You will play a key role in enabling data-driven decision-making by developing efficient and secure data infrastructure tailored to the fast-paced FinTech environment. Key Responsibilities: Design and implement scalable data models and data architecture to support financial analytics, risk modeling, and regulatory reporting. Build and maintain data ingestion pipelines using Python or Java to process high-volume, high-velocity financial data from diverse sources. Lead data migration efforts from legacy systems to modern cloud-based platforms. Develop and enforce data validation processes to ensure accuracy, consistency, and compliance with financial regulations. Create and manage task schedulers to automate data workflows and ensure timely data availability. Collaborate with product, engineering, and data science teams to deliver reliable and secure data solutions. Optimize data processing for performance, scalability, and cost-efficiency in a cloud environment. Required Skills & Qualifications: Proficiency in Python and/or Java for backend data engineering tasks. Strong experience in data modelling , ETL/ELT pipeline development , and data architecture . Hands-on experience with data migration and transformation in financial systems. Familiarity with task scheduling tools (e.g., Apache Airflow, Cron, Luigi). Solid understanding of SQL and experience with relational and NoSQL databases. Knowledge of data validation frameworks and best practices in financial data quality. Experience with cloud platforms (AWS, GCP, or Azure), especially in data services. Understanding of data security , compliance , and regulatory requirements in FinTech. Preferred Qualifications: Experience with big data technologies (e.g., Spark, Kafka, Hadoop). Familiarity with CI/CD pipelines , containerization (Docker), and orchestration (Kubernetes). Exposure to financial data standards (e.g., FIX, ISO 20022) and regulatory frameworks (e.g., GDPR, PCI-DSS).",,,"Python, SQL",
4153232349,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. A career in IBM Consulting embraces long-term relationships and close collaboration with clients across the globe. You will collaborate with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio, including IBM Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you will be supported by mentors and coaches who will encourage you to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground-breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and learning opportunities in an environment that embraces your unique skills and experience. Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results Your Primary Responsibilities Include Develop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools. Liaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT. Work with data scientist and business analytics team to assist in data ingestion and data-related technical issues. Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter Knowledge of Cloud, Power BI, Data migration on cloud skills. Experience in Unix shell scripting and python Experience with relational SQL, Big Data etc Preferred Technical And Professional Experience Knowledge of MS-Azure Cloud Experience in Informatica PowerCenter Experience in Unix shell scripting and python",,,"Python, SQL, Power BI, Machine Learning",
4210593279,Risk Data Engineer Senior-AWS-Data Scientist,EY,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job At EY, you‚Äôll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we‚Äôre counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. RCE-Risk Data Engineer/Leads Description ‚Äì External Job Description: - Our Technology team builds innovative digital solutions rapidly and at scale to deliver the next generation of Financial and Non- Financial services across the globe. The Position is a senior technical, hands-on delivery role, requiring the knowledge of data engineering, cloud infrastructure and platform engineering, platform operations and production support using ground-breaking cloud and big data technologies. The ideal candidate with 3-6 years of experience, will possess strong technical skills, an eagerness to learn, a keen interest on 3 keys pillars that our team support i.e. Financial Crime, Financial Risk and Compliance technology transformation, the ability to work collaboratively in fast-paced environment, and an aptitude for picking up new tools and techniques on the job, building on existing skillsets as a foundation. In this role you will: Develop a thorough understanding of the data science lifecycle, including data exploration, preprocessing, modelling, validation, and deployment Design, build, and maintain tree-based predictive models, such as decision trees, random forests, and gradient-boosted trees, with a low-level understanding of their algorithms and functioning. Ingestion and provisioning of raw datasets, enriched tables, and/or curated, re-usable data assets to enable variety of use cases. Evaluate modern technologies, frameworks, and tools in the data engineering space to drive innovation and improve data processing capabilities. Core/Must Have Skills. Significant data analysis experience using python, SQL and spark. Experience in python or spark to write scripts for data transformation, integration and automation tasks. 3-6 years‚Äô experience with cloud ML (AWS) or any similar tools. Design, build, and maintain tree-based predictive models, such as decision trees, random forests, and gradient-boosted trees, with a low-level understanding of their algorithms and functioning. Strong Experience with statistical analytical techniques, data mining and predictive models. Conduct A/B testing and other model validation techniques to ensure the accuracy and reliability of data models.. Experience with optimization modelling, machine learning, forecasting and/or natural language processing. Hands-on experience with Amazon S3 data storage, data lifecycle policies, and integration with other AWS services. Maintain, optimize, and scale AWS Redshift clusters to ensure efficient data storage, retrieval, and query performance Utilize Amazon S3 to store raw data, manage large datasets, and integrate with other AWS services to ensure secure, scalable, and cost-effective data solutions. Experience in implementing CI/CD Pipelines in AWS. At least 4+ years of experience in Database Design and Dimension modelling using SQL Advanced working SQL Knowledge and experience working with relational and NoSQL databases as well as working familiarity with a variety of databases (SQL Server, Neo4J) Strong analytical and critical thinking skills, with ability to identify and resolve issues in data pipelines and systems. Strong communication skills to effectively collaborate with team members and present findings to stakeholders. Collaborate with cross-functional teams to ensure successful implementation of solutions. Experience with OLAP, OLTP databases, and data structuring/modelling with understanding of key data points. Good to have: Apply domain knowledge (if applicable) in financial fraud to enhance predictive modelling and anomaly detection capabilities. Knowledge of AWS IAM for managing secure access to data resources. Familiarity with DevOps practices and automation tools like Terraform or CloudFormation. Experience with data visualization tools like Quick Sight or integrating Redshift data with BI tools (Tableau, PowerBI, etc.). AWS certifications such as AWS Certified Data Analytics ‚Äì Specialty or AWS Certified Solutions Architect are a plus. EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",,,"Python, SQL, Tableau, Machine Learning, Data Analysis",
4240634813,Computer Science Engineer - MSc (Freelancer),Soul AI,India (Remote),Remote,Part-time,,"About the job About Soul AI: Soul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data, AI-first scaled operations services, and more. Based in SF and Hyderabad, we are a young, fast-moving team on a mission to build AI for Good, driving innovation and positive societal impact. Who you are & how you can contribute? We are looking for highly skilled Computer Science Engineer ( freelancers) to contribute to AI training through data annotation and domain-specific expertise. This is a remote, flexible opportunity where you can work on cutting-edge AI projects, applying your domain knowledge to refine AI‚Äôs understanding and performance. Key Responsibilities: Develop and validate AI-driven software models. Annotate and evaluate code quality and software structures. Provide insights into algorithms and data structures. Must Required Traits: Strong academic background in Computer Science (MSc preferred) . Teaching, tutoring, or content creation experience in the field is a plus. Familiarity with AI, machine learning, or data annotation is beneficial. Strong analytical skills and attention to detail. Why join us? Competitive hourly pay: upto ‚Çπ1500 per hour. Fully remote and flexible work schedule. Opportunity to contribute to the advancement of AI technology. NOTE: Pay will vary by project and typically is up to ‚Çπ1500 per hour . If you work an average of 3 hours every day, you could earn up to ‚Çπ90,000 per month once you clear our screening process. Join us to play a vital role in shaping the future of AI through high-quality training and data solutions!",,,Machine Learning,
4223536673,Senior Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Sr Developer with special emphasis and experience of 8 to 10 years on Python and Pyspark along with hands on experience on AWS Data components like AWS Glue, Athena etc.,. Also have good knowledge on Data ware house tools to understand the existing system. Candidate should also have experience on Datalake, Teradata and Snowflake. Should be good at terraform. 8-10 years of experience in designing and developing Python and Pyspark applications Creating or maintaining data lake solutions using Snowflake,taradata and other dataware house tools. Should have good knowledge and hands on experience on AWS Glue , Athena etc., Sound Knowledge on all Data lake concepts and able to work on data migration projects. Providing ongoing support and maintenance for applications, including troubleshooting and resolving issues. Expertise in practices like Agile, Peer reviews and CICD Pipelines. Desired Skills and Experience Snowflake, UNIX, PySpark, AWS Glue, CI & CD, Python",,,Python,
4243326731,Analyst Data Engineer,Deloitte,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job What impact will you make? Every day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration and high performance. As the undisputed leader in professional services, Deloitte is where you will find unrivaled opportunities to succeed and realize your full potentiaL Deloitte is where you will find unrivaled opportunities to succeed and realize your full potential. The Team Deloitte‚Äôs practice can help you uncover and unlock the value buried deep inside vast amounts of data. Our global network provides strategic guidance and implementation services to help companies manage data from disparate sources and convert it into accurate, actionable information that can support fact-driven decision-making and generate an insight-driven advantage. Our practice addresses the continuum of opportunities in business intelligence & visualization, data management, performance management and next-generation analytics and technologies, including big data, cloud, cognitive and machine learnin g. Learn more about Analytics and Information Management Prac tice. Role Purpose Our purpose in the B&PB Data Engineering team is to develop and deliver best-in-class data assets and manage data domains for our Business and Private Banking customers and colleagues‚Äîseamlessly and reliably every time. We are passionate about simplicity and meeting the needs of our stakeholders, while continuously innovating to drive value. As a Data Engineer , you will bring strong expertise in data handling and curation to the team. You will be responsible for building reusable datasets and improving the way we work within BPB. This role requires a strong team player mindset and a focus on contributing to the team‚Äôs overall success. Job Title: Analyst Data Engineer Division: Business and Private Banking (BPB) Team Name: Data and Analytics, Data Analytics & Strategy Execution Reporting to (People Leader Position): Manager, Data Engineering Location: Gurgaon Core Responsibilities Manage ETL jobs and ensure data requirements from BPB reporting and business teams are met. Assist with operational data loads and support ongoing data ingestion processes. Translate business requirements into technical specifications. Work alongside senior data engineers to deliver scalable, efficient data solutions. Key Role Responsibilities Actively participate in the development, testing, deployment, monitoring, and refinement of data services. Manage and resolve incidents/problems; apply fixes and resolve systematic issues. Collaborate with stakeholders to triage issues and implement solutions that restore productivity. Risk Proactively manage risk in accordance with all policy and compliance requirements. Perform appropriate controls and adhere to all relevant processes and procedures. Promptly escalate any events, issues, or breaches as they are identified. Understand and own the risk responsibilities associated with the role. Accountabilities Build effective working relationships with BPB teams to ensure alignment with the overall Data Analytics Strategy. Deliver ETL pipelines that meet business reporting and data needs. Orchestrate and automate data workflows to ensure timely and reliable dataset delivery. Translate business goals into technical data engineering requirements. People Accountability People Accountability: Individual Contributor Number of Direct Reports: 0 Essential Capabilities Individuals with a minimum of 1‚Äì2 years of experience in a similar data engineering or technical role. A tertiary qualification in Computer Science or a related discipline. Critical thinkers who use networks, knowledge, and data to drive better outcomes for the business and customers. Continuous improvers who challenge the status quo and advocate for better solutions. Team players who value diverse skills and perspectives. Customer-focused individuals who define problems and develop solutions based on stakeholder needs. Required Technical Skills: Experience with design, build, and implementation of data engineering pipelines using: SQL Python Airflow Databricks (or Snowflake) Experience with cloud-based data solutions (preferably AWS). Familiarity with on-premises data environments such as Oracle. Strong development and performance tuning skills with RDBMS platforms including: Oracle Teradata Snowflake Redshift Our purpose Deloitte is led by a purpose: To make an impact tha t matters. Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the Communities in which we live and work‚Äîalways striving to be an organization that is held up as a role model of quality, integrity, and positi ve change. Learn more about Deloitte's impact o n the world.",Manager,,"Python, SQL",
4244568829,Cloud Data Engineer,Uplers,"Raipur, Chhattisgarh, India (Remote)",Remote,‚Çπ1.8M/yr - ‚Çπ2.8M/yr,,"About the job Experience : 5.00 + years Salary : INR 1800000-2800000 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Intelebee LLC) (*Note: This is a requirement for one of Uplers' client - Intelebee LLC) What do you need for this opportunity? Must have skills required: Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory Intelebee LLC is Looking for: Data Engineer:We are seeking a skilled and hands-on Cloud Data Engineer with 5-8 years of experience to drive end-to-end data engineering solutions. The ideal candidate will have a deep understanding of dimensional modeling, data warehousing (DW), Lakehouse architecture, and the Medallion architecture. This role will focus on leveraging Azure's/AWS ecosystem to build scalable, efficient, and secure data solutions. You will work closely with customers to understand requirements, create technical specifications, and deliver solutions that scale across both on-premise and cloud environments. Key Responsibilities: End-to-End Data Engineering Lead the design and development of data pipelines for large-scale data processing, utilizing Azure/AWS tools such as Azure Data Factory, Azure Synapse, Azure functions, Logic Apps , Azure Databricks, and Data Lake Storage. Tools, AWS Lambda, AWS Glue Develop and implement dimensional modeling techniques and data warehousing solutions for effective data analysis and reporting. Build and maintain Lakehouse and Medallion architecture solutions for streamlined, high-performance data processing. Implement and manage Data Lakes on Azure/AWS, ensuring that data storage and processing is both scalable and secure. Handle large-scale databases (both on-prem and cloud) ensuring high availability, security, and performance. Design and enforce data governance policies for data security, privacy, and compliance within the Azure ecosystem. 5 How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory",,,Data Analysis,
4249668976,Senior Associate Data Engineer,"NTT DATA, Inc.","Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Make an impact with NTT DATA Join a company that is pushing the boundaries of what is possible. We are renowned for our technical excellence and leading innovations, and for making a difference to our clients and society. Our workplace embraces diversity and inclusion ‚Äì it‚Äôs a place where you can grow, belong and thrive. Your day at NTT DATA The Senior Associate Data Engineer is a developing specialist role, tasked with supporting the transformation of data into a structured format that can be easily analyzed in a query or report. This role is responsible for developing structured data sets that can be reused or compliment other data sets and reports. This role analyzes the data sources and data structure and designs and develops data models to support the analytics requirements of the business which includes management / operational / predictive / data science capabilities. Key responsibilities: Contributes to the creation of data models in a structured data format to enable analysis thereof. Proactively supports the design and development of scalable extract, transform and loading (ETL) packages from the business source systems and the development of ETL routines to populate data from sources. Participates in the transformation of object and data models into appropriate database schemas within design constraints. Interprets installation standards to meet project needs and produce database components as required. Receives instructions from various stakeholders to create test scenarios and be responsible for participating in thorough testing and validation to support the accuracy of data transformations. Proactively supports the running of data migrations across different databases and applications, e.g. MS Dynamics, Oracle, SAP and other ERP systems. Support the definition and implementation of data table structures and data models based on requirements. Contributes to analysis, and development of ETL and migration documentation. Receives instructions from various stakeholders to evaluate potential data requirements. Supports the definition and management of scoping, requirements, definition, and prioritization activities for small-scale changes and assist with more complex change initiatives. Contributes to the recommendation of improvements in automated and non-automated components of the data tables, data queries and data models. To thrive in this role, you need to have: Knowledge of the definition and management of scoping requirements, definition and prioritization activities. Understanding of database concepts, object and data modelling techniques and design principles and conceptual knowledge of building and maintaining physical and logical data models. Knowledge of Microsoft Azure Data Factory, SQL Analysis Server, SAP Data Services, SAP BTP. Understanding of data architecture landscape between physical and logical data models. Analytical mindset with good business acumen skills. Problem-solving aptitude with the ability to communicate effectively, both written and verbal. Ability to build effective relationships at all levels within the organization. Seasoned expert in programing languages (Perl, bash, Shell Scripting, Python, etc.). Academic qualifications and certifications: Bachelor's degree or equivalent in computer science, software engineering, information technology, or a related field. Relevant certifications preferred such as SAP, Microsoft Azure etc. Certified Data Engineer, Certified Professional certification preferred. Required experience: Moderate level experience in data engineering, data mining within a fast-paced environment. Familiarity with building modern data analytics solutions that delivers insights from large and complex data sets with multi-terabyte scale. Moderate level experience with architecture and design of secure, highly available and scalable systems. Familiarity with automation, scripting and proven examples of successful implementation. Familiarity with scripts using scripting language (Perl, bash, Shell Scripting, Python, etc.). Moderate level experience with big data tools like Hadoop, Cassandra, Storm etc. Moderate level experience in any applicable language, preferably .NET. Familiarity with working with SAP, SQL, MySQL databases and Microsoft SQL. Moderate level experience working with data sets and ordering data through MS Excel functions, e.g. macros, pivots. Workplace type: Hybrid Working About NTT DATA NTT DATA is a $30+ billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long-term success. We invest over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure, and connectivity. We are also one of the leading providers of digital and AI infrastructure in the world. NTT DATA is part of NTT Group and headquartered in Tokyo. Equal Opportunity Employer NTT DATA is proud to be an Equal Opportunity Employer with a global culture that embraces diversity. We are committed to providing an environment free of unfair discrimination and harassment. We do not discriminate based on age, race, colour, gender, sexual orientation, religion, nationality, disability, pregnancy, marital status, veteran status, or any other protected category. Join our growing global team and accelerate your career with us. Apply today.",Associate,,"Python, SQL, Excel, R",
4245238269,Senior Data Engineer,Luxoft India,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Project Description: Are you passionate about leveraging the latest technologies for strategic change? Do you enjoy problem solving in clever ways? Are you organized enough to drive change across complex data systems? If so, you could be the right person for this role. As an experienced data engineer, you will join a global data analytics team in our Group Chief Technology Officer / Enterprise Architecture organization supporting our strategic initiatives which ranges from portfolio health to integration. Responsibilities: ‚Ä¢ Help Group Enterprise Architecture team to develop our suite of EA tools and workbenches ‚Ä¢ Work in the development team to support the development of portfolio health insights ‚Ä¢ Build data applications from cloud infrastructure to visualization layer ‚Ä¢ Produce clear and commented code ‚Ä¢ Produce clear and comprehensive documentation ‚Ä¢ Play an active role with technology support teams and ensure deliverables are completed or escalated on time ‚Ä¢ Provide support on any related presentations, communications, and trainings ‚Ä¢ Be a team player, working across the organization with skills to indirectly manage and influence ‚Ä¢ Be a self-starter willing to inform and educate others Mandatory Skills Description: ‚Ä¢ B.Sc./M.Sc. degree in computing or similar ‚Ä¢ 5-8+ years' experience as a Data Engineer, ideally in a large corporate environment ‚Ä¢ In-depth knowledge of SQL and data modelling/data processing ‚Ä¢ Strong experience working with Microsoft Azure ‚Ä¢ Experience with visualisation tools like PowerBI (or Tableau, QlikView or similar) ‚Ä¢ Experience working with Git, JIRA, GitLab ‚Ä¢ Strong flair for data analytics ‚Ä¢ Strong flair for IT architecture and IT architecture metrics ‚Ä¢ Excellent stakeholder interaction and communication skills ‚Ä¢ Understanding of performance implications when making design decisions to deliver performant and maintainable software. ‚Ä¢ Excellent end-to-end SDLC process understanding. ‚Ä¢ Proven track record of delivering complex data apps on tight timelines ‚Ä¢ Passionate about development with focus on data and cloud ‚Ä¢ Analytical and logical, with strong problem solving skills ‚Ä¢ A team player, comfortable with taking the lead on complex tasks ‚Ä¢ An excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences ‚Ä¢ Comfortable with working in cross-functional global teams to effect change ‚Ä¢ Passionate about learning and developing your hard and soft professional skills Nice-to-Have Skills Description: ‚Ä¢ Experience working in the financial industry ‚Ä¢ Experience in complex metrics design and reporting ‚Ä¢ Experience in using artificial intelligence for data analytics Languages: English: C1 Advanced",,,"SQL, Tableau",
4251668973,Remote Python AI Engineer - 17852,Turing,"Delhi, India (Remote)",Save Remote Python AI Engineer - 17852¬†  at Turing,Contract,,"About the job Work on Real-World Problems with Global Tech Experts Join a leading U.S.-based technology company as a Python Developer / AI Engineer, where you‚Äôll tackle real-world challenges and build innovative solutions alongside top global experts. This is a fully remote, contract-based opportunity ideal for developers passionate about Python, data analysis, and AI-driven work. Key Responsibilities: Write efficient, production-grade Python code to solve complex problems. Analyze public datasets and extract meaningful insights using Python and SQL. Collaborate with researchers and global teams to iterate on data-driven ideas. Document all code and development decisions in Jupyter Notebooks or similar platforms. Maintain high-quality standards and contribute to technical excellence. Job Requirements: Open to all levels: junior, mid-level, or senior engineers. Degree in Computer Science, Engineering, or equivalent practical experience. Proficient in Python programming for scripting, automation, or backend development. Experience with SQL/NoSQL databases is a plus. Familiarity with cloud platforms (AWS, GCP, Azure) is advantageous. Must be able to work 5+ hours overlapping with Pacific Time (PST/PT). Strong communication and collaboration skills in a remote environment. Perks & Benefits: Work on cutting-edge AI and data projects impacting real-world use cases. Collaborate with top minds from Meta, Stanford, and Google. 100% remote ‚Äì work from anywhere. Contract role with flexibility and no traditional job constraints. Competitive compensation in USD, aligned with global tech standards. Selection Process: Shortlisted developers may be asked to complete an assessment. If you clear the assessment, you will be contacted for contract assignments with expected start dates, durations, and end dates. Some contract assignments require fixed weekly hours, averaging 20/30/40 hours per week for the duration of the contract assignment.",,,"Python, SQL, Data Analysis",
4249143931,Python Developer,MyCareernet,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Company:IT Services Organization Key Skills: Python Development, AI/Gen-AI, PL/SQL, Microservices (FastAPI/Django), REST/SOAP APIs, Web Services, Open-Source Tools & Frameworks, Insurance Domain Knowledge, Cloud Platforms (Azure Preferred), Maven/Gradle, Git, Frontend (ReactJS/AngularJS). Roles and Responsibilities: Develop and maintain high-quality Python applications, with a strong focus on AI/Generative AI solutions. Design and build efficient, reusable, and reliable Python code aligned with project and business requirements. Collaborate with cross-functional teams to understand functional requirements and translate them into technical solutions. Build and consume web services using RESTful and SOAP protocols. Implement microservices using frameworks such as FastAPI or Django, ensuring scalability and performance. Contribute to the full software development life cycle (SDLC), including design, development, deployment, and support. Leverage open-source tools, libraries, and frameworks for efficient development and rapid prototyping. Maintain high standards of code quality through code reviews, unit testing, and automation practices. Work with PL/SQL for backend database interactions and complex data processing. Participate in agile/scrum development cycles, contributing to planning, estimations, and sprint reviews. Collaborate with stakeholders to understand business requirements in the insurance domain and align technical solutions accordingly. Ensure timely delivery and take ownership of assigned tasks. Experience Requirements: 4-10 years of hands-on experience in Python development. Proficient in AI and Generative AI technologies with real-world application experience. Strong experience in working with PL/SQL and microservices architecture. Practical knowledge of RESTful APIs, web services, and API integration. Familiar with Maven/Gradle, Git, and modern version control systems. Exposure to modern frontend frameworks like ReactJS, AngularJS, or BackboneJS is a plus. Prior experience in insurance or related industries is preferred. Experience working in cloud environments (preferably Azure) is a strong advantage. Hands-on experience in deploying and maintaining services in production environments. Education: Any Graduation.",,,"Python, SQL",
4227260632,Senior Associate - Data Engineer - Python,Acuity Knowledge Partners,"Gurugram, Haryana, India",,Full-time,,"About the job Job Purpose We're seeking a skilled Software Engineer with expertise in Python and it would be nice to have experience of working on Large Language Models (LLM) to join our team. Essential Skills Desired Skills and Experience Minimum of a bachelor‚Äôs degree in a technical or quantitative field with strong academic background Demonstrated ability to implement data engineering pipelines and real-time applications in python (C++ is a plus) Proficiency with object oriented programming in python is a must Experience with Linux/Unix shell/ scripting languages and Git is a must Experience with python based tools like Jupyter notebook, coding standards like pep8 is a plus Strong problem-solving skills and understanding of data structures and algorithms Experience with large-scale data processing and pipeline development Understanding of various LLM frameworks and experience with prompt engineering using Python or other scripting languages Nice to have Knowledge of natural language processing (NLP) concepts, familiarity with integrating and leveraging LLM APIs for various applications Key Responsibilities Design, develop, and maintain projects using Python along with operational support Transform a wide range of structured and unstructured data into standardized outputs for quantitative analysis and financial engineering Participate in code reviews, ensure coding standards, and contribute to the improvement of the codebase Develop the utility tools that can further automate the software development, testing and deployment workflow Collaborate with internal and external cross-functional teams Key Metrics Python Behavioral Competencies Good communication (verbal and written), critical thinking, and attention to detail",Associate,,Python,
4232551162,Data Governance Engineer,BNP Paribas,"Chennai, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job About BNP Paribas Group BNP Paribas is a top-ranking bank in Europe with an international profile. It operates in 71 countries and has almost 199 000 employees. The Group ranks highly in its three core areas of activity: Domestic Markets and International Financial Services (whose retail banking networks and financial services are grouped together under Retail Banking & Services) and Corporate & Institutional Banking, centred on corporate and institutional clients. The Group helps all of its clients (retail, associations, businesses, SMEs, large corporates and institutional) to implement their projects by providing them with services in financing, investment, savings and protection. In its Corporate & Institutional Banking and International Financial Services activities, BNP Paribas enjoys leading positions in Europe, a strong presence in the Americas and has a solid and fast-growing network in the Asia/Pacific region. About BNP Paribas India Solutions Established in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, a leading bank in Europe with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 6000 employees, to provide support and develop best-in-class solutions. About Businessline/Function For 150 years, BNP Paribas Wealth Management has been committed to protecting clients‚Äô wealth, developing it, and eventually passing it on to their loved ones. We deliver tailor-made experience, with outstanding attention to detail and expertise from precise local knowledge to the global know-how that we access from the Group. Our goal is to create a new wealth management experience fit for a world where digital interactions have come to enhance human ones. Wealth Management Investment Solution Hub (WMIS Hub) provides a global IT solution for BNP Paribas Wealth Management where we develop, maintain and evolve IT applications which fits to the specific needs of BNP Paribas Wealth Management business users. Job Title Data Governance Engineer / SME Date 01-Nov-2024 Department APAC ‚Äì Wealth Management IT Location: Chennai Business Line / Function IPS - WM Reports To (Direct) Grade (if applicable) (Functional) Number Of Direct Reports Directorship / Registration: NA Position Purpose As a member of WM IT DCIO team, the candidate will work on WM Data Governance workstreams as prioritized (Data Quality, Data literacy, Data Lineage/Architecture, Data Privacy by Design, Data Protection workstreams). Support WM IT DCIO team in driving the Data initiatives transversally with WM Leadership, Application Development & Maintenance (ADM), Engineering & Production (E&P), Security (CISO) teams. Support the development & testing of software / applications for Data Management. Note: DCIO complements Chief Data Office (CDO) within Wealth Management IT organization. Responsibilities Direct Responsibilities Work closely with WM IT DCIO team to execute the Data Governance Implementation across Data Initiatives e.g., RAMI (Data Retention), Data Privacy by Design, Data Quality, etc. Create and test proof-of-concept / solutions to support the strategic evolution of the software applications. Data Governance SME within Wealth Management operationally working with Data Custodian IT Officer (DCIO), DPO (Data Protection Officer), CDO (Chief Data Officer) teams. Hands-on with the development, testing, configuration, deployment of software systems in the Data Transversal organization Operationalize Data Policies / Frameworks including Business Glossaries, Data Dictionaries, Data Profiling, etc. Technical & Behavioral Competencies Minimum 7+ years of experience as: Data expertise (At least 2 of the following: Data Governance, Data Quality, Data Privacy & Protection, Data Management) Bachelor‚Äôs degree in Engineering (Computer science or Electronic & Communications) QualiÔ¨Åcations Hands-on experience in working with Data (Data Profiling, Scorecards/BI) Previously worked in Data Governance and Data Security Financial Services products and Applications knowledge Working knowledge across Excel, SQL, Python, Collibra, PowerBI, Cloud Plus Collibra Developer or Ranger Certified or similar certification is preferred. Skills Required Knowledge about Data and Compliance / Regulatory environment( global and local Data Regulations) Demonstrates flexibility and willingness to accept assignments and challenges in rapidly changing environment. Understand how data is used (e.g., Analytics, Business Intelligence, etc.) Working knowledge on Data lifecycle, and Data Transformations / Data Lineage At least 2 of the following: Data Quality, Data Architecture, Database Management, Data Privacy & Protection, Security of data Ability to define relevant key performance indicators (KPI) Problem solving and team collaboration Self-motivated and results driven Project management and business analysis Agile thinking Transversal Skills Proficient in design new process, adaptation of Group IT processes to Wealth Management IT Strong communication to elaborate across stakeholders, and support change Minimum 7 years of experience in Data / Tech",Director,,"Python, SQL, Excel",
4246344720,Cloud Data Engineer,Google,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Gurugram, Haryana, India; Bengaluru, Karnataka, India; Hyderabad, Telangana, India . Minimum qualifications: Bachelor‚Äôs degree in Engineering, Computer Science, a related field, or equivalent practical experience. Experience coding with one or more programming languages (e.g., Java, C/C++, Python). Experience troubleshooting technical issues for internal/external partners or customers. Preferred qualifications: Experience in distributed data processing frameworks and modern age investigative and transactional data stores. Experience in working with/on data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools, environments, and data structures. Experience in big data, information retrieval, data mining. Experience in building multi-tier, high availability applications with modern technologies such as NoSQL, MongoDB. Experience with Infrastructure as Code (IaC) and Continuous Integration/Continuous Delivery (CICD) tools like Terraform, Ansible, Jenkins etc. Understanding of at least one database type with the ability to write complex SQLs. About the job The Google Cloud Platform team helps customers transform and build what's next for their business ‚Äî all with technology built in the cloud. Our products are developed for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. Our teams are dedicated to helping our customers ‚Äî developers, small and large businesses, educational institutions and government agencies ‚Äî see the benefits of our technology come to life. As part of an entrepreneurial team in this rapidly growing business, you will play a key role in understanding the needs of our customers and help shape the future of businesses of all sizes use technology to connect with customers, employees and partners. As a Strategic Cloud Data Engineer, you will guide customers on how to ingest, store, process, analyze, and explore/visualize data on the Google Cloud Platform. You will work on data migrations and modernization projects, and with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform/product challenges. You will have an in-depth understanding of data governance and security controls. You will travel to customer sites to deploy solutions and deliver workshops to educate and empower customers. Additionally, you will work closely with Product Management and Product Engineering teams to build and constantly drive excellence in our products.Google Cloud accelerates every organization‚Äôs ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google‚Äôs cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems. Responsibilities Interact with stakeholders to translate complex customer requirements into recommendations for appropriate solution architectures and advisory services. Engage with technical leads, and partners to lead high velocity migration and modernization to Google Cloud Platform (GCP). Design, Migrate/Build and Operationalize data storage and processing infrastructure using Cloud native products. Develop and implement data quality and governance procedures to ensure the accuracy and reliability of data. Take various project requirements and organize them into clear goals and objectives, and create a work breakdown structure to manage internal and external stakeholders. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .",,,Python,
4240113694,Data Engineer,Impetus,"Indore, Madhya Pradesh, India (On-site)",On-site,Full-time,,"About the job Open Location - Indore, Noida, Gurgaon, Bangalore, Hyderabad, Pune Job Description 3-8 years' experience working on Data engineering & ETL/ELT processes, data warehousing, and data lake implementation with AWS services Hands on experience in designing and implementing solutions like creating/deploying jobs, Orchestrating the job/pipeline and infrastructure configurations Expertise in designing and implementing pySpark and Spark SQL based solutions Design and implement data warehouses using Amazon Redshift, ensuring optimal performance and cost efficiency. Good understanding of security, compliance, and governance standards. Roles & Responsibilities Design and implement robust and scalable data pipelines using AWS/Azure services Drive architectural decisions for data solutions on AWS, ensuring scalability, security, and cost-effectiveness. Hands-on experience of Develop and deploy ETL/ELT processes using Glue/Azure data factory, Lambda/Azure functions, Step function/Azure logic apps/MWAA, S3 and Lake formation from various data sources. Strong Proficiency in pySpark, SQL, Python. Proficiency in SQL for data querying and manipulation. Experience with data modelling, ETL processes, and data warehousing concepts. Create and maintain documentation for data pipelines, processes, and following best practices. Knowledge of various Spark Optimization technique, Monitoring and Automation would be a plus. Participate in code reviews and ensure adherence to coding standards and best practices. Understanding of data governance, compliance, and security best practices. Strong problem-solving and troubleshooting skills. Excellent communication and collaboration skills ‚Äì with understanding on stakeholder mapping Good to Have: Understanding of databricks is good to have. GenAI, Working with LLMs are good to have Mandatory Skills - AWS OR Azure Cloud, Python Programming, SQL, Spark SQL, Hive, Spark optimization techniques and Pyspark. Share resume at sonali.mangore@impetus.com with details (CTC, Expected CTC, Notice Period)",,,"Python, SQL",
4195893851,Associate Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Join Amgen‚Äôs Mission of Serving Patients At Amgen, if you feel like you‚Äôre part of something bigger, it‚Äôs because you are. Our shared mission‚Äîto serve patients living with serious illnesses‚Äîdrives all that we do. Since 1980, we‚Äôve helped pioneer the world of biotech in our fight against the world‚Äôs toughest diseases. With our focus on four therapeutic areas ‚ÄìOncology, Inflammation, General Medicine, and Rare Disease‚Äì we reach millions of patients each year. As a member of the Amgen team, you‚Äôll help make a lasting impact on the lives of patients as we research, manufacture, and deliver innovative medicines to help people live longer, fuller happier lives. Our award-winning culture is collaborative, innovative, and science based. If you have a passion for challenges and the opportunities that lay within them, you‚Äôll thrive as part of the Amgen team. Join us and transform the lives of patients while transforming your career. Associate Data Engineer What You Will Do Let‚Äôs do this. Let‚Äôs change the world. In this vital role We are seeking a Associate Data Engineer to design, build, and maintain scalable data solutions that drive business insights. You will work with large datasets, cloud platforms (AWS preferred), and big data technologies to develop ETL pipelines, ensure data quality, and support data governance initiatives. Develop and maintain data pipelines, ETL/ELT processes, and data integration solutions. Design and implement data models, data dictionaries, and documentation for accuracy and consistency. Ensure data security, privacy, and governance standard processes. Use Databricks, Apache Spark (PySpark, SparkSQL), AWS, Redshift, for scalable data processing. Collaborate with cross-functional teams to understand data needs and deliver actionable insights. Optimize data pipeline performance and explore new tools for efficiency. Follow best practices in coding, testing, and infrastructure-as-code (CI/CD, version control, automated testing). What We Expect Of You We are all different, yet we all use our unique contributions to serve patients. Strong problem-solving, critical thinking, and communication skills. Ability to collaborate effectively in a team setting. Proficiency in SQL, data analysis tools, and data visualization. Hands-on experience with big data technologies (Databricks, Apache Spark, AWS, Redshift ). Experience with ETL tools, workflow orchestration, and performance tuning for big data. Basic Qualifications: Bachelor‚Äôs degree and 0 to 3 years of experience OR Diploma and 4 to 7 years of experience in Computer science, IT or related field. Preferred Qualifications: Knowledge of data modeling, warehousing, and graph databases Experience with Python, SageMaker, and cloud data platforms. AWS Certified Data Engineer or Databricks certification preferred. What You Can Expect Of Us As we work to develop treatments that take care of others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we‚Äôll support your journey every step of the way. In addition to the base salary, Amgen offers competitive and comprehensive Total Rewards Plans that are aligned with local industry standards. Apply now and make a lasting impact with the Amgen team. careers.amgen.com As an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease. Amgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Associate,,"Python, SQL, Data Analysis",
4239696515,Data Engineer,R Systems,Greater Delhi Area (Hybrid),Hybrid,Full-time,,"About the job About the Company - About- R Systems International Limited ( https://www.rsystems.com/about-us/factsheet/ ) R Systems is a Blackstone portfolio Company founded in 1993, Headquartered at El Dorado Hills, California, United States (USA) and offshore delivery centers located at Noida, Pune and Chennai. R Systems International Limited is listed publically at NSE and BSE with current share price at around RS 500+. It is a leading digital product engineering company that designs and builds next-gen products, platforms, and digital experiences empowering clients across various industries to overcome digital barriers, put their customers first, and achieve higher revenues as well as operational efficiency. We constantly innovate and bring fresh perspectives to harness the power of the latest technologies like cloud, automation, AI, ML, analytics, Mixed Reality etc. Role- Data Engineer Responsibilities - Strong expertise in ETL processes and tools like Informatica, with experience in designing and maintaining data pipelines. Proficiency in Pyspark for big data processing and distributed computing. Advanced SQL skills and experience with PL/SQL for managing and querying large datasets in relational databases. Exposure to cloud platforms like AWS and GCP for data storage, data processing, and deployment of ETL solutions. Experience in data integration, transformation, and loading between different systems and databases. Familiarity with data modeling, data warehousing, and database performance tuning. Understanding of Agile methodologies and participation in sprint-based development cycles. Qualifications - BE/Btech",Manager,,"SQL, R",
4233216212,Data Engineer,Chevron,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job About The Position Utilizes software engineering principles to deploy and maintain fully automated data transformation pipelines that combine a large variety of storage and computation technologies to handle a distribution of data types and volumes in support of data architecture design. A Data Engineer designs data products and data pipelines that are resilient to change, modular, flexible, scalable, reusable, and cost effective. Key Responsibilities Design, develop, and maintain data pipelines and ETL processes using Microsoft Azure services (e.g., Azure Data Factory, Azure Synapse, Azure Databricks, Azure Fabric). Utilize Azure data storage accounts for organizing and maintaining data pipeline outputs. (e.g., Azure Data Lake Storage Gen 2 & Azure Blob storage). Collaborate with data scientists, data analysts, data architects and other stakeholders to understand data requirements and deliver high-quality data solutions. Optimize data pipelines in the Azure environment for performance, scalability, and reliability. Ensure data quality and integrity through data validation techniques and frameworks. Develop and maintain documentation for data processes, configurations, and best practices. Monitor and troubleshoot data pipeline issues to ensure timely resolution. Stay current with industry trends and emerging technologies to ensure our data solutions remain cutting-edge. Manage the CI/CD process for deploying and maintaining data solutions. Required Qualifications Bachelor‚Äôs degree in Computer Science, Engineering, or a related field (or equivalent experience) and able to demonstrate high proficiency in programming fundamentals. 3-5 years experience At least 2 years of proven experience as a Data Engineer or similar role dealing with data and ETL processes. Strong knowledge of Microsoft Azure services, including Azure Data Factory, Azure Synapse, Azure Databricks, Azure Blob Storage and Azure Data Lake Gen 2. Experience utilizing SQL DML to query modern RDBMS in an efficient manner (e.g., SQL Server, PostgreSQL). Strong understanding of Software Engineering principles and how they apply to Data Engineering (e.g., CI/CD, version control, testing). Experience with big data technologies (e.g., Spark). Strong problem-solving skills and attention to detail. Excellent communication and collaboration skills. Preferred Qualifications Learning agility Technical Leadership Consulting and managing business needs Strong experience in Python is preferred but experience in other languages such as Scala, Java, C#, etc is accepted. Experience building spark applications utilizing PySpark. Experience with file formats such as Parquet, Delta, Avro. Experience efficiently querying API endpoints as a data source. Understanding of the Azure environment and related services such as subscriptions, resource groups, etc. Understanding of Git workflows in software development. Using Azure DevOps pipeline and repositories to deploy and maintain solutions. Understanding of Ansible and how to use it in Azure DevOps pipelines. Chevron ENGINE supports global operations, supporting business requirements across the world. Accordingly, the work hours for employees will be aligned to support business requirements. The standard work week will be Monday to Friday. Working hours are 8:00am to 5:00pm or 1.30pm to 10.30pm. Chevron participates in E-Verify in certain locations as required by law.",,5 years experience,"Python, SQL",
4230048718,Data Engineer,Uplers,Greater Delhi Area (Remote),Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4230048714,Data Engineer,Uplers,Greater Lucknow Area (Remote),Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4229656539,Data Engineer,hackajob,India (Remote),Remote,Full-time,,"About the job hackajob is collaborating with NearForm to connect them with exceptional tech professionals for this role. This is a remote permanent position for those based in India. The Opportunity Nearform is a digital and AI engineering consultancy with a reputation for experience-led modernization. We focus on creating transformative digital products for enterprise customers across the UK and Ireland. Our approach blends deep technical expertise with modern, cloud-native solutions to help clients overcome legacy constraints and unlock new business value. We are on a growth journey, with ambitions to scale rapidly while broadening our service offering with AI solutions and Modern Managed Services. The Role We are looking for a Senior Data Engineer who can help our enterprise clients translate their requirements into functional and appealing interactive applications. You would be responsible for building server-side components of these applications. You will be outcome driven, and have a drive to deliver the best quality output for our clients within specific timeframes. You will be working in high performing software teams who are distributed across different locations. Designing & building data pipelines and solutions using Python, GCP, Pyspark, SQL, and BigQuery. Supporting system design, development and maintenance and take responsibility for personal technical quality standards within the project team. Assisting with defining structured practices, especially in source code management, building and deployment. Designing and implementing data storage solutions. Implementing security and data protection. Executing meticulous automated tests in line with best practice. Troubleshooting and debugging applications. Optimising applications for maximum speed and scalability. Using appropriate tools to maintain version control and build processes. Getting feedback from, and building solutions for, users and clients. Working with and supporting the Delivery Architect (team lead) in project execution and timely delivery. Collaborating with client teams. Writing documentation and guides. Following new and emerging technologies. Contributing to NearForm tooling and open-source projects. Understand and embrace the NearForm culture. Requirements As a Senior Data Engineer you will be largely focused on the server side of the application development process. We are keen to hire people who can hit the ground running, as such, there are some skills and experience we really need you to have: Experience delivering at a Senior Developer level in an enterprise environment. Practical experience of delivering in an agile environment. Practical experience of knowledge of developing real-world solutions and platforms. Deep understanding of server side code, with experience developing in Python and utilising SQL Production experience of Snowflake or Databrick platforms. Deep understanding of ELT/ETL pipelines and toolchains. Deep understanding of Relational and Document data stores. Deep understanding of how modern web services are built. Deep understanding of modern testing methodologies, tools and practices. Deep understanding of versioning control tools including Git. Understanding of tools such as AWS serverless compute, Apigee, CloudRun and containerisation. Understanding of security and performance considerations. Understanding of architectural and design patterns in one or more of AWS/GCP/Azure. Understanding of deployment tools and systems integration. Understanding with a wide variety of open-source technologies and tools. Excellent analytical and multitasking skills. Excellent communication and collaboration skills. Empathy and people skills. Optional Experience Experience with tools such as FastAPI, Pydantic, Flask and Django. Experience with other programming languages e.g. Javascript and Java. Experience with multiple cloud technologies. Experience debugging infrastructure. Infrastructure as code technology such as Terraform and Cloud Formation. Previous consultancy experience. Benefits Work remotely; we have a genuine dedication to work/life balance. Work flexibly; we appreciate there are more important things in life than work so our flexible working culture allows you to work around what matters - school run, no problem! The Wellness Hub: We have a genuine commitment to fostering/improving NearFormers‚Äô wellbeing; we offer resources and support, including a NearForm advice line which offers confidential support for anything from relationship issues to staying healthy. Please get in touch to discuss remuneration. Recruitment/Application process A TalentCall with someone from the Talent team (30 mins approx.) to introduce to you NearForm and the role, and to make sure your experience is relevant for the position and to discuss life as a NearFormer An assessment with a member of the hiring team (1 hr approx.), this session is carried out using a screen share. You will be given a challenge and asked to provide a working solution to a specified problem. A video call with one of our Hiring Managers (45 mins approx.) which allows us to gain a better insight into what interests you, your technical background, and give you an overview of the work we do at NearForm. Although we are widely dispersed, NearFormers are a tightly-knit team. We trust one another and care about our colleagues. We all get together in person at our annual company retreat, of course when we‚Äôre not faced with a global pandemic! Building on our open-source origins, we promote the sharing of thoughts, knowledge and ideas. NearForm is committed to shaping a better world in all that we do. Our global team is built based on respect, inclusivity, diversity and excellence.",Manager,,"Python, SQL",
4221512463,Data Engineer-2-R-246504,Mastercard,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Our Purpose Mastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we‚Äôre helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential. Title And Summary Data Engineer-2 Overview We are the global technology company behind the world‚Äôs fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless¬Æ. We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities. Our Team Within Mastercard ‚Äì Services The Services org is a key differentiator for Mastercard, providing the cutting-edge services that are used by some of the world's largest organizations to make multi-million dollar decisions and grow their businesses. Focused on thinking big and scaling fast around the globe, this agile team is responsible for end-to-end solutions for a diverse global customer base. Centered on data-driven technologies and innovation, these services include payments-focused consulting, loyalty and marketing programs, business Test & Learn experimentation, and data-driven information and risk management services. Data Analytics And AI Solutions (DAAI) Program Within the D&S Technology Team, the DAAI program is a relatively new program that is comprised of a rich set of products that provide accurate perspectives on Portfolio Optimization, and Ad Insights. Currently, we are enhancing our customer experience with new user interfaces, moving to API and web application-based data publishing to allow for seamless integration in other Mastercard products and externally, utilizing new data sets and algorithms to further analytic capabilities, and generating scalable big data processes. We are looking for an innovative software engineering lead who will lead the technical design and development of an Analytic Foundation. The Analytic Foundation is a suite of individually commercialized analytical capabilities (think prediction as a service, matching as a service or forecasting as a service) that also includes a comprehensive data platform. These services will be offered through a series of APIs that deliver data and insights from various points along a central data store. This individual will partner closely with other areas of the business to build and enhance solutions that drive value for our customers. Engineers work in small, flexible teams. Every team member contributes to designing, building, and testing features. The range of work you will encounter varies from building intuitive, responsive UIs to designing backend data models, architecting data flows, and beyond. There are no rigid organizational structures, and each team uses processes that work best for its members and projects. Here are a few examples of products in our space: Portfolio Optimizer (PO) is a solution that leverages Mastercard‚Äôs data assets and analytics to allow issuers to identify and increase revenue opportunities within their credit and debit portfolios. Ad Insights uses anonymized and aggregated transaction insights to offer targeting segments that have high likelihood to make purchases within a category to allow for more effective campaign planning and activation. Help found a new, fast-growing engineering team! Position Responsibilities As a Data Engineer within DAAI, you will: Play a large role in the implementation of complex features Push the boundaries of analytics and powerful, scalable applications Build and maintain analytics and data models to enable performant and scalable products Ensure a high-quality code base by writing and reviewing performant, well-tested code Mentor junior engineers and teammates Drive innovative improvements to team development processes Partner with Product Managers and Customer Experience Designers to develop a deep understanding of users and use cases and apply that knowledge to scoping and building new modules and features Collaborate across teams with exceptional peers who are passionate about what they do Ideal Candidate Qualifications 4+ years of data engineering experience in an agile production environment Experience leading the design and implementation of large, complex features in full-stack applications Ability to easily move between business, data management, and technical teams; ability to quickly intuit the business use case and identify technical solutions to enable it Experience leveraging open source tools, predictive analytics, machine learning, Advanced Statistics, and other data techniques to perform analyses High proficiency in using Python or Scala, Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi, Scoop), SQL to build Big Data products & platforms Experience in building and deploying production-level data-driven applications and data processing workflows/pipelines and/or implementing machine learning systems at scale in Java, Scala, or Python and deliver analytics involving all phases like data ingestion, feature engineering, modeling, tuning, evaluating, monitoring, and presenting Experience in cloud technologies like Databricks/AWS/Azure Strong technologist with proven track record of learning new technologies and frameworks Customer-centric development approach Passion for analytical / quantitative problem solving Experience identifying and implementing technical improvements to development processes Collaboration skills with experience working with people across roles and geographies Motivation, creativity, self-direction, and desire to thrive on small project teams Superior academic record with a degree in Computer Science or related technical field Strong written and verbal English communication skills Corporate Security Responsibility All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must: Abide by Mastercard‚Äôs security policies and practices; Ensure the confidentiality and integrity of the information being accessed; Report any suspected information security violation or breach, and Complete all periodic mandatory security trainings in accordance with Mastercard‚Äôs guidelines. R-246504",Manager,,"Python, SQL, R, Machine Learning",
4236572714,Data Engineer,Tata Consultancy Services,"Mumbai, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Dear Associates Greetings from TATA Consultancy Services!! Thank you for expressing your interest in exploring a career possibility with the TCS Family Hiring For : Data Engineer Must Have : 5+ years in Data Engineering with experience in AWS cloud, Databricks and Snowflake,SQL) Expertise in SQL, Pyspark/Python Expertise in Data warehousing concepts, dimensional modelling and semantic modelling ( DAX) Hands on experience in cloud computing platforms (AWS, Databricks, Snowflake) . Proficiency in AWS services (S3, EMR, EC2, Airflow, Lambda) Strong experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using cloud based data integration technologies including ETL/ELT, data replication. Architect, implement, optimize and maintain complex and scalable data pipelines. Proficient in CI/CD tools specially on GIT HUB. Test cases development and execution Implement best practises for data management including data quality and security. Drive automation using innovative and modern tools, techniques, and architectures to minimize manual and error-prone processes and improve productivity Interaction with Business stakeholders and ability to understand,explain the data scenarios in business terms Working experience in Agile teams for quick turnaround of user stories Experience: 7 + yrs Location : Mumbai If interested kindly fill the details and send your resume at nitu.sadhukhan@tcs.com . Note: only Eligible candidates with Relevant experience will be contacted further Name Contact No: Email id: Current Location: Preferred Location: Highest Qualification (Part time / Correspondence is not Eligible) : Year of Passing (Highest Qualification): Total Experience: Relevant Experience : Current Organization: Notice Period: Current CTC: Expected CTC: Pan Number : Gap in years if any (Education / Career): Updated CV attached (Yes / No) ? IF attended any interview with TCS in last 6 months : Available For walk In drive on 31st Of May _Mumbai : Thanks & Regards, Nitu Sadhukhan Talent Acquisition Group Tata Consultancy Services Lets Connect : linkedin.com/in/nitu-sadhukhan-16a580179 Nitu.sadhukhan@tcs.com",Associate,,"Python, SQL",
4207825025,Data Engineer III,JPMorganChase,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Job Description Are you ready to advance your career in data engineering? Join our dynamic Data Engineering Team at JPMorgan Chase in Bengaluru, where your expertise will contribute to designing and delivering market-leading technology products. We offer a collaborative environment that fosters career growth and innovation, allowing you to make a significant impact on our business objectives. As a Data Engineer III at JPMorgan Chase within the Data Engineering Team, you will be tasked with the design and delivery of reliable technology solutions that align with the firm's business goals. Your role as a crucial member of an agile team will involve the creation of secure, stable, and scalable data products that inform our business strategies and improve our understanding of our customers. Job Responsibilities Design, develop, and manage ETL jobs, data marts, and event collection and processing tools. Build data pipelines and tooling to support stakeholders across the project. Create secure and high-quality production code and maintain algorithms that run synchronously with appropriate systems. Write and maintain documentation of technical architecture. Participate in regular code reviews to maintain best code quality and adhere to best practices. Identify areas for quick wins to improve the experience of end users. Stay updated with the latest trends and technologies in data engineering. Work effectively in a team environment and contribute to team goals. Add to a team culture of diversity, equity, inclusion, and respect. Required Qualifications, Capabilities, And Skills Formal training or certification on data engineering concepts and 3+ years applied experience Proficiency in one or more programming languages such as Python, Java, or Scala. Ability to design and implement scalable data pipelines for batch and real-time data processing. Experience with big data technologies such as Spark, Hadoop, Hive, EMR, etc. Experience working with modern data warehouse platforms like Amazon Redshift, Google BigQuery, or Snowflake. Experience with cloud platforms such as AWS, GCP, or Microsoft Azure. Experience in developing, debugging, and maintaining code in a large corporate environment. Overall knowledge of the Software Development Life Cycle. Solid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security. Preferred Qualifications, Capabilities, And Skills Knowledge of modern data lake table formats, i.e., Iceberg, Hudi, etc. Experience with Kubernetes for container orchestration, including deploying, scaling, and managing containerized applications. Certifications in relevant technologies or platforms, such as AWS Certified Big Data ‚Äì Specialty, Google Professional Data Engineer, or Microsoft Certified: Azure Data Engineer Associate, can be advantageous. Relevant industry experience, preferably in a data engineering role. #ICBCareer #ICBEngineering About Us JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world‚Äôs most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management. We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants‚Äô and employees‚Äô religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation. About The Team Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we‚Äôre setting our businesses, clients, customers and employees up for success.",Associate,,Python,
4204361160,Staff I Data Engineer,BlackLine,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Get to Know Us: It's fun to work in a company where people truly believe in what they're doing! At BlackLine, we're committed to bringing passion and customer focus to the business of enterprise applications. Since being founded in 2001, BlackLine has become a leading provider of cloud software that automates and controls the entire financial close process. Our vision is to modernize the finance and accounting function to enable greater operational effectiveness and agility, and we are committed to delivering innovative solutions and services to empower accounting and finance leaders around the world to achieve Modern Finance. Being a best-in-class SaaS Company, we understand that bringing in new ideas and innovative technology is mission critical. At BlackLine we are always working with new, cutting edge technology that encourages our teams to learn something new and expand their creativity and technical skillset that will accelerate their careers. Work, Play and Grow at BlackLine! Make Your Mark: As a member of the Data & BI Engineering team you will primarily focus on advancing our Enterprise Data Platform to allow the organization to make data-driven decisions. The successful candidate will work closely with cross-functional teams to identify business requirements, design, and develop data models, data warehouses, and data visualization solutions that help support the organization's strategic goals. The Staff Data Engineer will work in a dynamic environment and will be required to stay current with the latest trends and technologies in the business intelligence field. The ideal candidate will be able to pick up business domain and internal process knowledge and leverage that knowledge to think strategically, communicate effectively, and manage multiple projects simultaneously. The team is also responsible for administering tools and platforms around reporting, analytics, and data visualization while promoting best practices. The role requires a strong combination of technical expertise, leadership skills, and a deep understanding of data engineering principles and best practices. We are looking for a driven, detail-oriented, and passionate engineer to come to join our team. You'll Get To: Provide technical expertise and leadership in technology direction, road-mapping, architecture definition, design, development, and delivery of enterprise-class solutions while adhering to timelines, coding standards, requirements, and quality. Architect, design, develop, test, troubleshoot, debug, optimize, scale, perform the capacity planning, deploy, maintain, and improve software applications, driving the delivery of high-quality value and features to Blackline‚Äôs customers. Work collaboratively across the company to design, communicate and further assist with adoption of best practices in architecture and implementation. Deliver robust architectural solutions for complex design problems. Implement, refine, and enforce data engineering best practices to ensure that delivered features meet performance, security, and maintainability expectations. Research, test, benchmark, and evaluate new tools and technologies, and recommend ways to implement them in data platform. Identify and create solutions that are likely to contribute to the development of new company concepts while keeping in mind the business strategy, short- and long-term roadmap, and architectural considerations to support them in a highly scalable and easy extensible manner. Actively participate in research, development, support, management, and other company initiatives designing solutions to optimally address current and future business requirements and infrastructure plans. Inspire a forward-thinking team of developers, acting as an agent of change and evangelist for a quality-first culture within the organization. Mentor and coach key technical staff and guide them to solutions on complex design issues. Act as a conduit for questions and information flow when those outside of Engineering have ideas for new technology applications. Speak in terms relevant to audience, translating technical concepts into non-technical language and vice versa. Facilitate consensus building while striving for win/win scenarios and elicit value-add contributions from all team members in group settings. Maintain a strong sense of business value and return on investment in planning, design, and communication. Proactively identify issues, bottlenecks, gaps, or other areas of concern or opportunity and work to either directly affect change, or advocate for that change by working with peers and leadership to build consensus and act. Perform critical maintenance, deployment, and release support activities, including occasional off-hours support. What You'll Bring: Bachelor's or Master's degree in Computer Science, Data Science, or a related field.10+ years as a data engineer.10+ years of experience using RDBMS, SQL, NoSQL, Python, Java, or other programming languages is a plus. 10+ years of experience designing, developing, testing, and implementing Extract, Transform and Load (ELT/ETL) solutions using enterprise ELT/ETL tools and Open source. 5+ years working experience with SQL and familiarity with Snowflake data warehouse, strong working knowledge in stored procedures, CTEs, and UDFs, RBACKnowledge of data integration and data quality best practicesFamiliarity with data security and privacy regulations.Experience in working in a startup-type environment, good team player, and can work independently with minimal supervision Experience with cloud-native architecture and data solutions.Strong working knowledge in data modeling, data partitioning, and query optimization Demonstrated knowledge of development processes and agile methodologies.Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms.Proficient in managing large volumes of data. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams. Experience in providing technical support and troubleshooting for data-related issues. Expertise with at least one cloud environment and building cloud native data services. Prior experience driving data governance, quality, security initiatives. We‚Äôre Even More Excited If You Have: Experience with Google Cloud or similar cloud provider Significant experience with open source platforms and technologies. Experience with data science and machine learning tools and technologies is a plus. Thrive at BlackLine Because You Are Joining: A technology-based company with a sense of adventure and a vision for the future. Every door at BlackLine is open. Just bring your brains, your problem-solving skills, and be part of a winning team at the world's most trusted name in Finance Automation! A culture that is kind, open, and accepting. It's a place where people can embrace what makes them unique, and the mix of cultural backgrounds and varying interests cultivates diverse thought and perspectives. A culture where BlackLiner's continued growth and learning is empowered. BlackLine offers a wide variety of professional development seminars and inclusive affinity groups to celebrate and support our diversity. BlackLine is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity or expression, race, ethnicity, age, religious creed, national origin, physical or mental disability, ancestry, color, marital status, sexual orientation, military or veteran status, status as a victim of domestic violence, sexual assault or stalking, medical condition, genetic information, or any other protected class or category recognized by applicable equal employment opportunity or other similar laws. BlackLine recognizes that the ways we work and the workplace itself have shifted. We innovate in a workplace that optimizes a combination of virtual and in-person interactions to maximize collaboration and nurture our culture. Candidates who live within a reasonable commute to one of our offices will work in the office at least 2 days a week.",,,"Python, SQL, Machine Learning",
4080833817,Senior Data Engineer [Immediate joiners only],Luxoft,India (Remote),Remote,Full-time,,"About the job Project Description: As a Senior Engineer in the Data Engineering & Analytics team, you will develop data & analytics solutions that sit atop vast datasets gathered by retail stores, restaurants, banks, and other consumer-focused companies. The challenge will be to create high-performance algorithms, cutting-edge analytical techniques including machine learning and artificial intelligence, and intuitive workflows that allow our users to derive insights from big data that in turn drive their businesses. You will have the opportunity to create high-performance analytic solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data. You will have the opportunity to develop data-driven innovative analytical solutions and identify opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations/decisions through activities like building ML models, automated data pipelines, designing data architecture/schema, performing jobs in big data cluster by using different execution engines and program languages such as Hive/Impala, Python, Java, Kafka, Spark, R, etc. Responsibilities: ‚Ä¢ Hands-on developer who writes good quality, secure code that is modular, functional, and testable. ‚Ä¢ Drive the evolution of Data & Services products/platforms with an impact-focused on data science and engineering. ‚Ä¢ Design and implement scalable data architecture and data pipelines ‚Ä¢ Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks. ‚Ä¢ Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers. ‚Ä¢ Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc. ‚Ä¢ Discover, ingest, and incorporate new sources of real-time, streaming, batch, and API-based data into our platform to enhance the insights we get from running tests and expand the ways and properties on which we can test Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines. ‚Ä¢ Participate in the development of data and analytic infrastructure for product development Continuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations ‚Ä¢ Partner with roles across the organization including consultants, engineering, and sales to determine the highest priority problems to solve ‚Ä¢ Evaluate trade-offs between many possible analytics solutions to a problem, taking into account usability, technical feasibility, timelines, and differing stakeholder opinions to make a decision Break large solutions into smaller, releasable milestones to collect data and feedback from product managers, clients, and other stakeholders ‚Ä¢ Evangelize releases to users, incorporating feedback, and tracking usage to inform future development Work with small, cross-functional teams to define the vision, establish team culture and processes Consistently focus on key drivers of organization value and prioritize operational activities accordingly ‚Ä¢ Escalate technical errors or bugs detected in project work ‚Ä¢ Maintain awareness of relevant technical and product trends through self-learning/study, training classes, and job shadowing. ‚Ä¢ Support the building of scaled machine learning production systems by designing pipelines and engineering infrastructure. Mandatory Skills Description: ‚Ä¢ Working proficiency in using Python/Scala, Spark (tuning jobs), SQL, Hadoop platforms to build Big Data products & platforms. ‚Ä¢ Good programming skills in Java and spring boot and Junit. ‚Ä¢ Knowledge in software development test approaches & frameworks ‚Ä¢ Familiarity with RESTful APIs and micro-services architectures ‚Ä¢ Experience in working with CI/CD ‚Ä¢ Experience in working with SQL database like Postgres, Oracle ‚Ä¢ Preferably with hands-on experience with Hadoop big data tools (Hive, Impala, Spark) ‚Ä¢ Experience with data pipeline and workflow management tools: NIFI, Airflow. ‚Ä¢ Comfortable in developing shell scripts for automation. ‚Ä¢ Good troubleshooting and debugging skills. ‚Ä¢ Proficient in standard software development, such as version control, testing, and deployment ‚Ä¢ Demonstrated basic knowledge of statistical analytical techniques, coding, and data engineering ‚Ä¢ Ability to quickly learn and implement new technologies ‚Ä¢ Ability to Solve complex problems with multi-layered data sets ‚Ä¢ Ability to innovate and determine new approaches & technologies to solve business problems and generate business insights & recommendations. ‚Ä¢ Ability to multi-task and strong attention to detail ‚Ä¢ Flexibility to work as a member of a matrix based diverse and geographically distributed project teams ‚Ä¢ Good communication skills - both verbal and written - and strong relationship, collaboration skills, and organizational skills Nice-to-Have Skills: ‚Ä¢ Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts ‚Ä¢ Experience in working with Cloud APIs (e.g., Azure, AWS) ‚Ä¢ Experience participating in complex engineering projects in an Agile setting e.g. Scrum",manager,,"Python, SQL, R, Machine Learning",
4245894914,Data Engineer,Egen,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Experience Level: 4 to 6 years of relevant IT experience Job Overview: We are looking for a skilled and motivated Data Engineer with strong experience in Python programming and Google Cloud Platform (GCP) to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust and scalable ETL (Extract, Transform, Load) data pipelines. The role involves working with various GCP services, implementing data ingestion and transformation logic, and ensuring data quality and consistency across systems. Key Responsibilities: ‚óè Design, develop, test, and maintain scalable ETL data pipelines using Python. ‚óè Work extensively on Google Cloud Platform (GCP) services such as: ‚óã Dataflow for real-time and batch data processing ‚óã Cloud Functions for lightweight serverless compute ‚óã BigQuery for data warehousing and analytics ‚óã Cloud Composer for orchestration of data workflows (based on Apache Airflow) ‚óã Google Cloud Storage (GCS) for managing data at scale ‚óã IAM for access control and security ‚óã Cloud Run for containerized applications ‚óè Perform data ingestion from various sources and apply transformation and cleansing logic to ensure high-quality data delivery. ‚óè Implement and enforce data quality checks, validation rules, and monitoring. ‚óè Collaborate with data scientists, analysts, and other engineering teams to understand data needs and deliver efficient data solutions. ‚óè Manage version control using GitHub and participate in CI/CD pipeline deployments for data projects. ‚óè Write complex SQL queries for data extraction and validation from relational databases such as SQL Server, Oracle, or PostgreSQL. ‚óè Document pipeline designs, data flow diagrams, and operational support procedures. Required Skills: ‚óè 4‚Äì6 years of hands-on experience in Python for backend or data engineering projects. ‚óè Strong understanding and working experience with GCP cloud services (especially Dataflow, BigQuery, Cloud Functions, Cloud Composer, etc.). ‚óè Solid understanding of data pipeline architecture, data integration, and transformation techniques. ‚óè Experience in working with version control systems like GitHub and knowledge of CI/CD practices. ‚óè Strong experience in SQL with at least one enterprise database (SQL Server, Oracle, PostgreSQL, etc.). Good to Have (Optional Skills): ‚óè Experience working with Snowflake cloud data platform. ‚óè Hands-on knowledge of Databricks for big data processing and analytics. ‚óè Familiarity with Azure Data Factory (ADF) and other Azure data engineering tools. Additional Details: ‚óè Excellent problem-solving and analytical skills. ‚óè Strong communication skills and ability to collaborate in a team environment.",,,"Python, SQL",
4250881229,Python  developer Fresher,Orbion Infotech,India (Remote),Remote,‚Çπ20K/month - ‚Çπ40K/month,,"About the job Tips: Provide a summary of the role, what success in the position looks like, and how this role fits into the organization overall. Responsibilities [Be specific when describing each of the responsibilities. Use gender-neutral, inclusive language.] Example: Determine and develop user requirements for systems in production, to ensure maximum usability Qualifications [Some qualifications you may want to include are Skills, Education, Experience, or Certifications.] Example: Excellent verbal and written communication skills Skills: organization,css,python,javascript,communication skills,sql,html",manager,,"Python, SQL",
4248103100,Data Engineer,Gloroots,"Gurugram, Haryana, India (On-site)",On-site,Full-time,"Fit : Experience supporting RecSys, ML, or content feeds in social or consumer platforms. ML/RecSys Awareness : Understands how features feed into models, or has worked closely with ML teams before. Product Empathy : Understands how data impacts user experience, not just analytics. Tools Fluency : Proficient in Kafka, Spark, Flink, Airflow, dbt, Redis, BigQuery, Feast, Terraform; can pick the best tool for the job. Nice to have: Experience with personalization systems, recommender pipelines, or ranking models Exposure to A/B testing platforms or online experimentation infrastructure Passion for consumer social or dating products What we offer: Join a founding data team where your work is core to product experience Shape key decisions for a platform that will directly impact how people form meaningful relationships Significant ESOPs and wealth creation with market competitive cash compensation Job search faster with Premium Access company insights like strategic priorities, headcount trends, and more Sivaraman and millions of other members use Premium Retry Premium for ‚Çπ0 1-month free trial. Cancel whenever. We‚Äôll remind you 7 days before your trial ends. About the company Gloroots 12,761 followers Follow Technology, Information and Internet 11-50 employees 15 on LinkedIn Gloroots helps AI-first companies build and scale world-class dev teams‚Äîacross borders, without barriers. From sourcing exceptional talent to managing global compliance, we make hiring across 100+ countries as seamless as hiring locally. Whether you're looking for AI developers, full-time engineers, contractors, or need EOR support, Gloroots gives you the infrastructure, expertise, and speed to grow your team fast‚Äîwithout friction. We‚Äôre trusted by AI-native startups and growth-stage companies to: ‚úÖ Recruit top-tier developers, ML engineers, and AI infrastructure talent ‚úÖ Hire globally with Employer of Record (EOR) and contractor solutions ‚úÖ Manage onboarding, payroll, benefits, and compliance‚Äîall in one place Build your AI dream team‚Äîwithout the headache. üì© Learn more at www.gloroots.com ‚Ä¶ show more Show more","About the job Role: Data Engineer Function: Data Platform Location: Gurgaon Type: Full-time Compensation: 30 - 60 LPA + ESOPs About the Company: An early-stage, US-based venture-backed technology company focused on creating innovative platforms for building meaningful relationships. They aim to transform how people connect by harnessing artificial intelligence, fostering community engagement, and delivering tailored content. Rather than developing another conventional social app, they‚Äôre crafting a unique experience that resonates deeply with users, making them feel truly understood. Central to our platform is a dynamic, machine-learning-powered recommendation system, drawing inspiration from the personalised discovery engines of leading music and video platforms. With strong financial backing from top-tier venture capital firms in India and the United States, they are well-positioned to advance our mission with innovation and impact. Company Philosophy: They believe: Great data + Good models = Great recommendations Good data + Great models = Average recommendations That‚Äôs why they‚Äôre investing in data infrastructure from our inception and foundation. Position Overview: We are looking for a Data Engineer to help design, build, and scale the data platform and infrastructure that powers our core recommendation systems and personalisation engines. You‚Äôll be a founding data engineer helping us lay the pipelines and infrastructure that feed our recommendation systems, onboarding flows, engagement metrics, dashboards, and ML models. You'll work hand-in-hand with our ML team to build data adapters and interfaces for model training, serving, and experimentation. Role & Responsibilities: Build and scale ETL from raw app events to ML-ready features Build real-time and batch pipelines for user actions and events Contribute to the feature store , embedding pipelines, data validation, and freshness monitoring systems Design data layers for use in A/B tests , product metrics , and ML training Partner with ML engineers to deliver data adapters for model inputs, embedding generation, and training loops Help evolve the core event schema and data model with privacy, observability, and performance in mind Ideal Profile: You‚Äôre a systems thinker who starts with data and designs for scale. You‚Äôve likely been at early-stage or high-scale consumer platforms ‚Äî social, gaming, transactions, or media. Experience : 3‚Äì6 years building scalable data systems in fast-moving environments. Industry Fit : Experience supporting RecSys, ML, or content feeds in social or consumer platforms. ML/RecSys Awareness : Understands how features feed into models, or has worked closely with ML teams before. Product Empathy : Understands how data impacts user experience, not just analytics. Tools Fluency : Proficient in Kafka, Spark, Flink, Airflow, dbt, Redis, BigQuery, Feast, Terraform; can pick the best tool for the job. Nice to have: Experience with personalization systems, recommender pipelines, or ranking models Exposure to A/B testing platforms or online experimentation infrastructure Passion for consumer social or dating products What we offer: Join a founding data team where your work is core to product experience Shape key decisions for a platform that will directly impact how people form meaningful relationships Significant ESOPs and wealth creation with market competitive cash compensation",,,,
4229958432,Data Engineer,L&T Technology Services,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Data Engineer Location: Bangalore Exp: 5-8 yrs ‚óè Expertise in data analysis methodologies and processes and their linkages to other processes ‚óè Technical expertise with data models, data mining, and segmentation techniques ‚óè Advanced SQL skills and experience with relational databases and database design. ‚óè Strong knowledge on Python, PySpark, SQL, Java Script . ‚óè Experience building and deploying machine learning models ‚óè Experience with integration efforts (packaged and customized applications) from a data analysis perspective ‚óè Strong business analyst skills, able to work with many different stakeholders to elicit and document requirements ‚óè Solid customer service skills and interpersonal skills. ‚óè Critical thinking skills and attention to detail. ‚óè Good judgement, initiative, commitment and resourcefulness ‚óè Proficient with Skywise platform and tools e.g. Contour, Code-Workbook, Code, Slate and Ontology definitions. ‚óè Knowledge of airline and MRO operations (preferred). ‚óè AWS Cloud Services Skills on services such as EC2, RDS, and Redshift. (Good to have) Responsibilities: ‚óè Interact with and work collaboratively with product teams ‚óè Analyze and organize raw data ‚óè Build data systems and pipelines ‚óè Evaluate business needs and objectives ‚óè Interpret trends and patterns ‚óè Conduct complex data analysis and report on results ‚óè Combine raw information from different sources ‚óè Explore ways to enhance data quality and reliability ‚óè Independently develop solutions (workflows, small apps, and dashboards) on the Skywise platform. ‚óè Collaborate with peers from other teams to deliver best in class solutions (workflows, small apps, and dashboards) to end users in the airlines. ‚óè Strong airline domain knowledge to engage with airline end users to articulate the pain points / business requirements of the airline end-users. ‚óè Create a repository of solutions developed.",,,"Python, SQL, Machine Learning, Data Analysis",
4249429499,DataTerrain - Amazon Data Engineer,"DataTerrain, Inc.","Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job This job is sourced from a job board. Learn More Location : Hyderabad (Onsite) Job Summary We are looking for a highly skilled Data Engineer to join our team. The ideal candidate should have strong experience working in an AWS environment, proficiency in ETL tools, and expertise in reporting tools. The candidate must be able to work onsite at the Amazon Hyderabad office on a daily basis. Key Responsibilities Design, develop, and maintain data pipelines using AWS technologies. Work with ETL tools to process and transform large datasets efficiently. Develop and optimize reports and dashboards using reporting tools. Ensure data integrity, security, and governance across all data pipelines. Collaborate with cross-functional teams to understand data requirements. Troubleshoot and resolve data-related issues proactively. Required Skills & Qualifications Strong experience in AWS services Proficiency in any ETL tool Proficiency in any reporting tool (e.g., Tableau, Power BI). Strong understanding of data modeling, warehousing, and big data concepts. Ability to work onsite daily at the Amazon Hyderabad office. Bachelor's degree in Computer Science, Software Engineering, or a related field (ref:hirist.tech)",,,"Tableau, Power BI",
4250584841,Data Engineer,Ericsson,"Bengaluru, Karnataka, India",,Full-time,,"About the job Grow with us About this opportunity: Ericsson is a leading provider of telecommunications equipment and services to mobile and fixed network operators globally. We are seeking a highly skilled and experienced Data Engineer to join our dynamic team at Ericsson. As a Data Engineer, you will be responsible for leveraging advanced analytics and machine learning techniques to drive actionable insights and solutions for our telecom domain. This role requires a deep understanding of data science methodologies, strong programming skills, and proficiency in cloud-based environments. What you will do: Experience in analyzing complex problems and translate it into algorithms High customer focus with high accountability for delivering high-quality products Very strong skills in software development using Advanced Python Backend development in RestAPIs using Flask, FastAPI Deployment experience with CI/CD pipelines Working knowledge of handling data sets and data pre-processing through PySpark Have experience of GCP cloud Solid communication and presentation skills Furthermore, we believe you are curious, innovative, high own self-drive, and collaborative Writing queries to target Casandra, PostgreSQL database Design Principles in application development The skills you bring: Bachelor's degree in Computer Science, Statistics, Mathematics, or a related field. A Master's degree or PhD is preferred. 2-5 years of experience in Python and in advanced Python preferably within the telecommunications or related industry. Strong programming skills in Python and SQL. Excellent problem-solving skills and ability to work independently as well as part of a team. Strong communication and presentation skills, with the ability to explain complex analytical concepts to non-technical stakeholders. Why join Ericsson? At Ericsson, you¬¥ll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of what¬¥s possible. To build solutions never seen before to some of the world‚Äôs toughest problems. You¬¥ll be challenged, but you won‚Äôt be alone. You¬¥ll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next. What happens once you apply? Click Here to find all you need to know about what our typical hiring process looks like. Encouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more. Primary country and city: India (IN) || Noida Req ID: 765999",,,"Python, SQL, Machine Learning",
4232852537,Azure Data Engineer,KEVELL GURU,"Madurai, Tamil Nadu, India (On-site)",On-site,Internship,,About the job Tasks Hands-on experience working with Azure tools and technologies Mentorship from industry experts in data engineering and analysis Networking opportunities with professionals in the field A pathway to secure a placement with us! Requirements Freshers/College Students/Career gap candidates can also eligible to apply There is no arrears before enrolling and also the overall percentage doesn‚Äôt matter. Strong analytical and problem-solving skills A passion for data and a desire to learn,,,,
4237419133,Big Data Engineer,Kresta Softech Private Limited,India (Remote),Remote,Full-time,,"About the job We‚Äôre looking for a skilled Big Data Engineer. Role Highlights: Position: Big Data Engineer Experience: 4+ years Location- Remote Work mode- WFH Notice Period: Immediate/15 days joiners Mandatory. Key Skills:- Big Data, AWS, CI/CD Pipelines, Scala , Python or Java or C++",,,Python,
4242203708,Data Engineer,LTI - Larsen & Toubro Infotech,"Chennai, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job Role : Snowflake Developer Experience : 6 to 9 Years Notice Period :Immediate to 30 days PRIMARY SKILL: Snowflake , SQL",,,SQL,
4131446588,Staff Data Engineer,Tide,Greater Delhi Area,,Full-time,,"About the job About Tide At Tide, we are building a business management platform designed to save small businesses time and money. We provide our members with business accounts and related banking services, but also a comprehensive set of connected administrative solutions from invoicing to accounting. Launched in 2017, Tide is now used by over 1 million small businesses across the world and is available to UK, Indian and German SMEs. Headquartered in central London, with offices in Sofia, Hyderabad, Delhi, Berlin and Belgrade, Tide employs over 2,000 employees. Tide is rapidly growing, expanding into new products and markets and always looking for passionate and driven people. Join us in our mission to empower small businesses and help them save time and money. About The Team As part of the team, you will be responsible for building and running the data pipelines and services that are required to support business functions/reports/dashboard.. We are heavily dependent on BigQuery/Snowflake, Airflow, Stitch/Fivetran, dbt , Tableau/Looker for our business intelligence and embrace AWS with some GCP. About The Role As a Staff Data Engineer you‚Äôll be: Developing end to end ETL/ELT Pipeline working with Data Analysts of business Function. Designing, developing, and implementing scalable, automated processes for data extraction, processing, and analysis in a Data Mesh architecture Mentoring Fother Junior Engineers in the Team Be a ‚Äúgo-to‚Äù expert for data technologies and solutions Ability to provide on the ground troubleshooting and diagnosis to architecture and design challenges Troubleshooting and resolving technical issues as they arise Looking for ways of improving both what and how data pipelines are delivered by the department Translating business requirements into technical requirements, such as entities that need to be modelled, DBT models that need to be build, timings, tests and reports Owning the delivery of data models and reports end to end Perform exploratory data analysis in order to identify data quality issues early in the process and implement tests to ensure prevent them in the future Working with Data Analysts to ensure that all data feeds are optimised and available at the required times. This can include Change Capture, Change Data Control and other ‚Äúdelta loading‚Äù approaches Discovering, transforming, testing, deploying and documenting data sources Applying, help defining, and championing data warehouse governance: data quality, testing, coding best practices, and peer review Building Looker Dashboard for use cases if required What We Are Looking For You have 9+ years of extensive development experience using snowflake or similar data warehouse technology You have working experience with dbt and other technologies of the modern data stack, such as Snowflake, Apache Airflow, Fivetran, AWS, git ,Looker You have experience in agile processes, such as SCRUM You have extensive experience in writing advanced SQL statements and performance tuning them You have experience in Data Ingestion techniques using custom or SAAS tool like fivetran You have experience in data modelling and can optimise existing/new data models You have experience in data mining, data warehouse solutions, and ETL, and using databases in a business environment with large-scale, complex datasets You have having experience architecting analytical databases (in Data Mesh architecture) is added advantage You have experience working in agile cross-functional delivery team You have high development standards, especially for code quality, code reviews, unit testing, continuous integration and deployment You have strong technical documentation skills and the ability to be clear and precise with business users You have business-level of English and good communication skills You have basic understanding of various systems across the AWS platform ( Good to have ) Preferably, you have worked in a digitally native company, ideally fintech You have experience with python, governance tool (e.g. Atlan, Alation, Collibra) or data quality tool (e.g. Great Expectations, Monte Carlo, Soda) will be added advantage Our Tech Stack DBT Snowflake Airflow Fivetran SQL Looker What You Will Get In Return Competitive salary Self & Family Health Insurance Term & Life Insurance OPD Benefits Mental wellbeing through Plumm Learning & Development Budget WFH Setup allowance 15 days of Privilege leaves 12 days of Casual leaves 12 days of Sick leaves 3 paid days off for volunteering or L&D activities Stock Options Tidean Ways Of Working At Tide, we champion a flexible workplace model that supports both in-person and remote work to cater to the specific needs of our different teams. While remote work is supported, we believe in the power of face-to-face interactions to foster team spirit and collaboration. Our offices are designed as hubs for innovation and team-building, where we encourage regular in-person gatherings to foster a strong sense of community. TIDE IS A PLACE FOR EVERYONE At Tide, we believe that we can only succeed if we let our differences enrich our culture. Our Tideans come from a variety of backgrounds and experience levels. We consider everyone irrespective of their ethnicity, religion, sexual orientation, gender identity, family or parental status, national origin, veteran, neurodiversity or differently-abled status. We celebrate diversity in our workforce as a cornerstone of our success. Our commitment to a broad spectrum of ideas and backgrounds is what enables us to build products that resonate with our members‚Äô diverse needs and lives. We are One Team and foster a transparent and inclusive environment, where everyone‚Äôs voice is heard. At Tide, we thrive on diversity, embracing various backgrounds and experiences. We welcome all individuals regardless of ethnicity, religion, sexual orientation, gender identity, or disability. Our inclusive culture is key to our success, helping us build products that meet our members' diverse needs. We are One Team, committed to transparency and ensuring everyone‚Äôs voice is heard. You personal data will be processed by Tide for recruitment purposes and in accordance with Tide's Recruitment Privacy Notice .",,,"Python, SQL, Tableau, Data Analysis",
3677050386,Associate - Python Data Engineer - GDC,PwC,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Line of Service Advisory Industry/Sector Not Applicable Specialism Operations Management Level Senior Associate Job Description & Summary A career in our Analytics Technology practice, within Data and Analytics Technology services, will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. You‚Äôll make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge. Our team helps clients navigate various analytics applications to get the most value out of their technology investment and foster confidence in their business intelligence. As part of our team, you‚Äôll help our clients implement enterprise content and data management applications that improve operational effectiveness and provide impactful data analytics and insights. To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future. As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to: Use feedback and reflection to develop self awareness, personal strengths and address development areas. Delegate to others to provide stretch opportunities, coaching them to deliver results. Demonstrate critical thinking and the ability to bring order to unstructured problems. Use a broad range of tools and techniques to extract insights from current industry or sector trends. Review your work and that of others for quality, accuracy and relevance. Know how and when to use tools available for a given situation and can explain the reasons for this choice. Seek and embrace opportunities which give exposure to different situations, environments and perspectives. Use straightforward communication, in a structured way, when influencing and connecting with others. Able to read situations and modify behavior to build quality relationships. Uphold the firm's code of ethics and business conduct. Associate - Python Data Engineer Required Skills Optional Skills Desired Languages (If blank, desired languages not specified) Travel Requirements Available for Work Visa Sponsorship? Government Clearance Required? Job Posting End Date",Associate,,Python,
4248302893,"Senior Data Engineer, ITC",Nike,"Karnataka, India (On-site)",On-site,Full-time,,"About the job Who You‚Äôll Work With This role is part of the Nike‚Äôs Content Technology team within Consumer Product and Innovation (CP&I) organization, working very closely with the globally distributed Engineering and Product teams. This role will roll up to the Director Software Engineering based out of Nike India Tech Centre. Who We Are Looking For We are looking for experienced Technology focused and hands on Lead Engineer to join our team in Bengaluru, India. As a Senior Data Engineer, you will play a key role in ensuring that our data products are robust and capable of supporting our Data Engineering and Business Intelligence initiatives. A data engineer with 5+ years of experience working with cloud-native platforms. Advanced skills in SQL, PySpark, Apache Airflow (or similar workflow management tools), Databricks, and Snowflake. Deep understanding of Spark optimization, Delta Lake, and Medallion architecture. Strong experience in data modeling and data quality practices. Experience with Tableau for data validation and monitoring. Exposure to DevOps practices, CI/CD, Git, and security aspects. Effective mentorship and team collaboration skills. Strong communication skills, able to explain technical concepts clearly. Experience with Kafka or other real-time systems Preferred: Familiarity with ML/GenAI integration into pipelines. Databricks Data Engineer certification. What You‚Äôll Work On Own and optimize large-scale ETL/ELT pipelines and reusable frameworks. Collaborate with cross-functional teams to translate business requirements into technical solutions. Guide junior engineers through code reviews and design discussions. Monitor data quality, availability, and system performance. Lead CI/CD implementation and improve workflow automation.",Director,,"SQL, Tableau",
4244564815,Data Engineer,Mindera,"Bengaluru, Karnataka, India (Remote)",Remote,Full-time,,"About the job We are looking for an experienced Data Engineer to join our team. Here at Mindera, we are continuously developing a fantastic team and would love for you to join us. As a Data Engineer, you will be responsible for building, maintaining, scaling, and integrating big data-based platforms. you will also be engaged with the Data Science team in setting up and automating the Data Science models/algorithms for production use. This is a fantastic opportunity for someone who is passionate about Data and is excited to use different tools to provide data insight for further analysis and drive business decisions. Requirements You're great at Python Cloud - AWS/GCP SQL Airflow Data testing Snowflake It also would be cool if you have Exposure to DBT would be preferable Experience working with modern data platforms such as redshift or snowflake would be preferable Experience working with Airflow, Docker, Terraform and CI/CD would be preferable Experience working with docker, Scala, and Kafka would be an added advantage What You Will Be Doing Implement/support new data solutions in the data lake/warehouse built on the snowflake Develop and design data pipelines using python Design and Implement Continuous Integration/Continuous Deployments pipelines Perform Data Modelling using downstream requirements Develop transformation scripts using advanced SQL and DBT Write test cases/scenarios to ensure incident-free production release Collaborate closely with data analysts and different domains such as Finance, risk, compliance, product and engineering to fulfil requirements Debug production and development issues and provide support to colleagues where necessary Perform data quality checks to ensure the quality of the data exposed to the end users Build strong relationships with team, peers and stakeholders Contributes to overall data platform implementation Benefits We offer Flexible working hours (self-managed) Competitive salary Annual bonus, subject to company performance Access to Udemy online training and opportunities to learn and grow within the role At Mindera we use technology to build products we are proud of, with people we love. Software Engineering Applications, including Web and Mobile, are at the core of what we do at Mindera. We partner with our clients, to understand their products and deliver high-performance, resilient and scalable software systems that create an impact on their users and businesses across the world. You get to work with a bunch of great people, and the whole team owns the project together. Our culture reflects our lean and self-organisation attitude. We encourage our colleagues to take risks, make decisions, work in a collaborative way and talk to everyone to enhance communication. We are proud of our work and we love to learn all and everything while navigating through an Agile, Lean and collaborative environment. Check out our Blog: http://mindera.com/ and our Handbook: http://bit.ly/MinderaHandbook Our offices are located: Porto, Portugal | Aveiro, Portugal | Coimbra, Portugal | Leicester, UK | San Diego, USA | Chennai, India | Bengaluru, India",,,"Python, SQL",
4232299471,Data Engineer,ZS,"Pune, Maharashtra, India",,Full-time,,"About the job ZS is a place where passion changes lives. As a management consulting and technology firm focused on improving life and how we live it, our most valuable asset is our people. Here you‚Äôll work side-by-side with a powerful collective of thinkers and experts shaping life-changing solutions for patients, caregivers and consumers, worldwide. ZSers drive impact by bringing a client first mentality to each and every engagement. We partner collaboratively with our clients to develop custom solutions and technology products that create value and deliver company results across critical areas of their business. Bring your curiosity for learning; bold ideas; courage and passion to drive life-changing impact to ZS. Our most valuable asset is our people . At ZS we honor the visible and invisible elements of our identities, personal experiences and belief systems‚Äîthe ones that comprise us as individuals, shape who we are and make us unique. We believe your personal interests, identities, and desire to learn are part of your success here. Learn more about our diversity, equity, and inclusion efforts and the networks ZS supports to assist our ZSers in cultivating community spaces, obtaining the resources they need to thrive, and sharing the messages they are passionate about. Data Engineer - Data Engineering & Analytics What you'll do: Create and maintain optimal data pipeline architecture. Identify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for scalability. Design, develop and deploy high volume ETL pipelines to manage complex and near-real time data collection. Develop and optimize SQL queries and stored procedures to meet business requirements. Design, implement, and maintain REST APIs for data interaction between systems. Ensure performance, security, and availability of databases. Handle common database procedures such as upgrade, backup, recovery, migration, etc. Collaborate with other team members and stakeholders. Prepare documentations and specifications. What you'll bring: Bachelor‚Äôs degree in computer science, Information Technology, or related field 1+ years of experience SQL, TSQL, Azure Data Factory or Synapse or relevant ETL technology. Prepare documentations and specifications. Strong analytical skills (impact/risk analysis, root cause analysis, etc.) Proven ability to work in a team environment, creating partnerships across multiple levels. Demonstrated drive for results, with appropriate attention to detail and commitment. Hands-on experience with Azure SQL Database Perks & Benefits: ZS offers a comprehensive total rewards package including health and well-being, financial planning, annual leave, personal growth and professional development. Our robust skills development programs, multiple career progression options and internal mobility paths and collaborative culture empowers you to thrive as an individual and global team member. We are committed to giving our employees a flexible and connected way of working. A flexible and connected ZS allows us to combine work from home and on-site presence at clients/ZS offices for the majority of our week. The magic of ZS culture and innovation thrives in both planned and spontaneous face-to-face connections. Travel: Travel is a requirement at ZS for client facing ZSers; business needs of your project and client are the priority. While some projects may be local, all client-facing ZSers should be prepared to travel as needed. Travel provides opportunities to strengthen client relationships, gain diverse experiences, and enhance professional growth by working in different environments and cultures. Considering applying? At ZS, we're building a diverse and inclusive company where people bring their passions to inspire life-changing impact and deliver better outcomes for all. We are most interested in finding the best candidate for the job and recognize the value that candidates with all backgrounds, including non-traditional ones, bring. If you are interested in joining us, we encourage you to apply even if you don't meet 100% of the requirements listed above. ZS is an equal opportunity employer and is committed to providing equal employment and advancement opportunities without regard to any class protected by applicable law. To Complete Your Application: Candidates must possess or be able to obtain work authorization for their intended country of employment.An on-line application, including a full set of transcripts (official or unofficial), is required to be considered. NO AGENCY CALLS, PLEASE. Find Out More At: www.zs.com",,,SQL,
4232952897,Azure Data Engineer,KEVELL GURU,"Madurai, Tamil Nadu, India (On-site)",On-site,Internship,,About the job Tasks Hands-on experience working with Azure tools and technologies Mentorship from industry experts in data engineering and analysis Networking opportunities with professionals in the field A pathway to secure a placement with us! Requirements Freshers/College Students/Career gap candidates can also eligible to apply There is no arrears before enrolling and also the overall percentage doesn‚Äôt matter. Strong analytical and problem-solving skills A passion for data and a desire to learn,,,,
4246245825,Senior Data engineer,Wissen Technology,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Wissen Technology is Hiring for Senior Data engineer About Wissen Technology: Wissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges Role Overview: We are looking for a highly skilled and experienced Senior Data Engineer to join our growing data engineering team in Bangalore. In this role, you will be responsible for designing, developing, and maintaining scalable and efficient data pipelines and data architectures. You will collaborate closely with cross-functional teams to ensure data accessibility, reliability, and performance to support business intelligence, analytics, and data science initiatives Experience: 9-13 Years Location: Bangalore Key Responsibilities Design, build, and maintain scalable and reliable data pipelines for ingesting, transforming, and loading structured and unstructured data from diverse sources. Develop optimized SQL queries and modular scripts for data manipulation and transformation. Create and maintain data models and schemas to support advanced analytics, reporting, and operational processes. Automate routine data engineering tasks and data quality checks using scripting (Python, Shell, etc.) and orchestration tools. Collaborate with engineering, product, and data science teams to understand data requirements and deliver high-quality solutions. Act as a liaison between engineering and business teams to translate business needs into technical solutions. Implement monitoring, alerting, and troubleshooting mechanisms for data pipelines and dashboards to ensure data integrity and availability. Define and implement best practices for data validation, data governance, and compliance. Manage test data and QA environments, supporting test data management processes. Work in an Agile environment and contribute to continuous improvement initiatives across the data engineering landscape. Required Skills and Qualification Bachelor‚Äôs or master's degree in computer science, Information Systems, or a related field. 9+ years of professional experience in data engineering or a related field. Strong expertise in SQL development and performance tuning for both RDBMS and cloud-based databases. Hands-on experience with cloud platforms, particularly AWS (e.g., S3, Glue, Lambda, Redshift, EMR). Experience with modern data warehouse technologies like Snowflake and Amazon Redshift. Strong background in data modeling, ETL/ELT architecture, and data warehousing concepts. Proficiency in programming languages such as Python, Scala, or Java. Experience working with real-time data streaming technologies (Kafka, Kinesis, etc.). Familiarity with both SQL and NoSQL database systems. Experience with Agile development methodologies and tools (JIRA, Confluence, Git, CI/CD). Strong problem-solving skills and ability to work independently and in collaborative teams. The Wissen Group was founded in 2000. Wissen Technology, a part of the Wissen Group, was established in 2015. Wissen Technology is a specialized technology company that delivers high-end consulting for organizations in the Banking & Finance, Telecom, and Healthcare domains. We help clients build world-class products. We offer an array of services including Core Business Application Development, Artificial Intelligence & Machine Learning, Big Data & Analytics, Visualization & Business Intelligence, Robotic Process Automation, Cloud Adoption, Mobility, Digital Adoption, Agile & DevOps, Quality Assurance & Test Automation. Over the years, Wissen Group has successfully delivered $1 billion worth of projects for more than 20 Fortune 500 companies. Wissen Technology provides exceptional value in mission critical projects for its clients, through thought leadership, ownership, and assured on-time deliveries that are always ‚Äòfirst time right‚Äô. The technology and thought leadership that the company commands in the industry is the direct result of the kind of people Wissen has been able to attract. Wissen is committed to providing them with the best possible opportunities and careers, which extends to providing the best possible experience and value to our clients. We have been certified as a Great Place to Work¬Æ company for two consecutive years (2020-2022) and voted as the Top 20 AI/ML vendor by CIO Insider. Great Place to Work¬Æ Certification is recognized worldwide by employees and employers alike and is considered the ‚ÄòGold Standard‚Äô. Wissen Technology has created a Great Place to Work by excelling in all dimensions - High-Trust, High-Performance Culture, Credibility, Respect, Fairness, Pride and Camaraderie. Website: www.wissen.com LinkedIn: Desired Skills and Experience Strong proficiency in Java (Java 8/11/17). Expertise in Spring Framework (Spring Boot, Spring MVC, Spring Security, Spring Cloud). Experience in designing and developing Microservices architecture. Solid understanding of System Design principles (scalability, caching, load balancing, database optimization). Hands-on experience with Kafka for event-driven architecture. Experience with SQL and NoSQL databases (MySQL, PostgreSQL, MongoDB, Redis). Familiarity with RESTful APIs and GraphQL. Experience with Docker, Kubernetes, and cloud platforms (AWS, Azure, GCP) is a plus. Strong knowledge of CI/CD pipelines, Jenkins, and Git. Understanding of unit testing and integration testing frameworks (JUnit, Mockito). Experience working in an Agile environment",,,"Python, SQL, Machine Learning",
4239567964,Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About Amgen Amgen harnesses the best of biology and technology to fight the world‚Äôs toughest diseases, and make people‚Äôs lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what‚Äôs known today. About The Role Role Description: Let‚Äôs do this. Let‚Äôs change the world. We are looking for highly motivated expert Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management. Roles & Responsibilities: Design, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets Understand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems Design and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments Ingest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms Ensuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring Expert in data quality, data validation and verification frameworks Innovate, explore and implement new tools and technologies to enhance efficient data processing Proactively identify and implement opportunities to automate tasks and develop reusable frameworks Work in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value Use JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories. Support continuous improvement, test automation, and DevOps practices in the data engineering lifecycle Collaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions Must-Have Skills: Hands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies. Proficiency in workflow orchestration, performance tuning on big data processing. Strong understanding of AWS services Ability to quickly learn, adapt and apply new technologies Strong problem-solving and analytical skills Excellent communication and teamwork skills Experience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices. Good-to-Have Skills: Data Engineering experience in Biotechnology or pharma industry Experience in writing APIs to make the data available to the consumers Experienced with SQL/NOSQL database, vector database for large language models Experienced with data modeling and performance tuning for both OLAP and OLTP databases Experienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops Education and Professional Certifications Master‚Äôs degree and 3 to 4 + years of Computer Science, IT or related field experience OR Bachelor‚Äôs degree and 5 to 8 + years of Computer Science, IT or related field experience AWS Certified Data Engineer preferred Databricks Certificate preferred Scaled Agile SAFe certification preferred Soft Skills: Excellent analytical and troubleshooting skills. Strong verbal and written communication skills Ability to work effectively with global, virtual teams High degree of initiative and self-motivation. Ability to manage multiple priorities successfully. Team-oriented, with a focus on achieving team goals. Ability to learn quickly, be organized and detail oriented. Strong presentation and public speaking skills. EQUAL OPPORTUNITY STATEMENT Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.",Associate,,"Python, SQL",
4251648917,Data Engineer,Sharpe Labs,"Delhi, Delhi, India (On-site)",On-site,Full-time,,"About the job This job is sourced from a job board. Learn More Responsibilities Design, build, and maintain scalable data pipelines and streaming systems. Develop real-time WebSocket and API integrations for data ingestion and delivery. Manage and optimize relational and non-relational databases for performance, reliability, and scalability. Collaborate with AI and product teams to build quantitative models and serve them in production environments. Design, build, and maintain robust ETL/ELT pipelines for ingesting and transforming on-chain and off-chain data. Build a real-time data streaming infrastructure using tools like Kafka or equivalent. Architect and optimize relational and non-relational databases to support complex queries and financial data models. Collaborate with product and analytics teams to design and deploy quantitative models for TVL, yield tracking, protocol metrics, etc. Implement tools and practices to ensure data integrity, quality, and observability across the platform. Contribute to our indexing infrastructure, working with smart contract data, subgraphs, or custom indexers. Requirements 3+ years of experience in data engineering, backend systems, or infrastructure roles. Strong knowledge of databases (SQL, NoSQL) and experience with data modeling and schema design. Proficient with PostgreSQL, TimescaleDB, or other time-series/analytical databases. Hands-on experience with stream processing frameworks (Kafka, Flink, etc. ). Expertise in building and consuming RESTful APIs and WebSocket protocols. Familiarity with blockchain data or financial data. Strong programming skills in Python or Go. Experience with quantitative finance modeling, DeFi metrics, or financial KPIs is a strong plus. Solid understanding of cloud infrastructure (e. g., AWS, GCP, or similar). Nice To Have Experience with subgraphs, The Graph, or building custom blockchain indexers. Background in data visualization platforms or interactive dashboards. Knowledge of DeFi protocols, tokenomics, and governance systems. Prior experience working in a fast-paced startup or early-stage product environment. This job was posted by Utsav Agarwal from Sharpe Labs. Desired Skills and Experience Data Streaming,ETL,Finance,Financial Markets,Financial Modelling,Kafka,Python,SQL,Socket.IO",,,"Python, SQL",
4249161578,Data Engineer-Data Platforms,IBM,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution. Your primary responsibilities include: Lead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements. Strive for continuous improvements by testing the build solution and working under an agile framework. Discover and implement the latest technologies trends to maximize and build creative solutions Preferred Education Master's Degree Required Technical And Professional Expertise Experience with Apache Spark (PySpark): In-depth knowledge of Spark‚Äôs architecture, core APIs, and PySpark for distributed data processing. Big Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts. Strong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy. SQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation. Cloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems Preferred Technical And Professional Experience Define, drive, and implement an architecture strategy and standards for end-to-end monitoring. Partner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering, Good to have detection and prevention tools for Company products and Platform and customer-facing",,,"Python, SQL, Data Analysis",
4248381415,Arcgate - Data Engineer - Python,Arcgate,"Udaipur, Tripura, India (On-site)",On-site,Full-time,,"About the job This job is sourced from a job board. Learn More About Arcgate is a dynamic and rapidly growing team of 2500+ professionals passionate about data and technology. We deliver cutting-edge solutions to some of the worlds most innovative startups to market leaders across application development, quality engineering, AI data preparation, data enrichment, search relevance, and content : Design, build, and optimize Python based data pipelines that handle large, complex, and messy datasets efficiently. Develop and manage scalable data infrastructures, including databases and data warehouses such as Snowflake, Azure Data Factory etc. ensuring reliability and performance. Build, maintain, and optimize CDC processes that integrate data from multiple sources into the data warehouse. Collaborate closely with data scientists, analysts, and operations teams to gather requirements and deliver high-quality data solutions. Perform data quality checks, validation, and verification to ensure data integrity and consistency. Support and optimize data flows, ingestion, transformation, and publishing across various systems. Work with AWS infrastructure (ECS, RDS, S3), manage deployments using Docker, and package services into containers. Use tools like Prefect, Dagster and dbt to orchestrate and transform data workflows. Implement CI/CD pipelines using Harness and GitHub Actions. Monitor system health and performance using DataDog. Manage infrastructure orchestration with Terraform and Terragrunt. Stay current with industry trends, emerging tools, and best practices in data engineering. Coach and mentor junior team members, promoting best practices and skill development. Contribute across diverse projects, demonstrating flexibility and : Bachelors degree in Computer Science, Engineering, Mathematics, Physics, or a related field. 5+ years of demonstrable experience building reliable, scalable data pipelines in production environments. Strong experience with Python, SQL programming, and data architecture. Hands-on experience with data modeling in Data Lake or Data Warehouse environments (Snowflake preferred). Familiarity with Prefect, Dagster, dbt, and ETL/ELT pipeline frameworks. Experience with AWS services (ECS, RDS, S3) and containerization using Docker. Knowledge of TypeScript, React, Node.js is a plus for collaborating on the application platform. Strong command of GitHub for source control and Jira for change management. Strong analytical and problem-solving skills, with a hands-on mindset for wrangling data and solving complex challenges. Excellent communication and collaboration skills; ability to work effectively with cross- functional teams. A proactive, start-up mindset, adaptable, ambitious, responsible, and ready to contribute wherever needed. Passion for delivering high-quality solutions with meticulous attention to detail. Enjoy working in an inclusive, respectful, and highly collaborative environment where every voice matters. Benefits Competitive salary package. Opportunities for growth, learning, and professional development. Dynamic, collaborative, and innovation-driven work culture. Work with cutting-edge technologies and leading-edge startups. (ref:hirist.tech)",,,"Python, SQL",
4239892400,Data Engineer l,Barracuda,"Bengaluru East, Karnataka, India (On-site)",On-site,Full-time,,"About the job Job ID 26-106 Come join our passionate team! Barracuda is a leading cybersecurity company providing complete protection against complex threats. Our platform protects email, data, applications, and networks with innovative solutions, and a managed XDR service, to strengthen cyber resilience. Hundreds of thousands of IT professionals and managed service providers worldwide trust us to protect and support them with solutions that are easy to buy, deploy, and use. We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an employer that complies with all applicable national, state and local laws pertaining to nondiscrimination and equal opportunity regardless of race, gender, religion, sex, sexual orientation, national origin, or disability. Envision yourself at Barracuda As a Data Engineer l at Barracuda, you will be part of a dynamic team responsible for the enterprise data and data governance. You will assist in sourcing, modeling, and warehousing data from various sources, including telemetry and usage data from over a dozen products, CRM systems like Salesforce, ERP systems like NetSuite, and various marketing systems and services. Your work will support data activation, BI, and AI use cases across departments such as finance, marketing, and customer success, positively impacting the top line of the company. Tech Stack SQL, Python, PySpark Databricks AWS What You‚Äôll Be Working On Assist in the development and maintenance of ETL pipelines to reliably ingest, cleanse, transform and store data. Support the implementation of data models to organize data logically and efficiently for various analytics and ML use cases. Implement data quality checks and enforce data governance practices to ensure data integrity and reliability. Assist in building reverse ETL pipelines to activate data in external systems. Implement data infrastructure using infrastructure-as-code. What You Bring To The Role Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Information Technology, or related field 3+ years of industry experience as a Data Engineer working on large data sets. Strong programming skills in Python. Excellent knowledge of SQL. Experience with public cloud, preferably AWS and services such as Redshift, S3, ECS, EC, Kinesis etc. Excellent communication skills and the ability to work collaboratively with cross-functional teams. What You‚Äôll Get From Us A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility ‚Äì there are opportunities for cross training and the ability to attain your next career step within Barracuda. In addition, you will receive equity, in the form of non-qualifying options.",,,"Python, SQL",
4242870499,Junior Data Engineer,Zelestra,"Gurugram, Haryana, India (Hybrid)",Hybrid,contract,,"About the job Date: Jun 2, 2025 Company: Zelestra Location Gurugram, India About Us Zelestra (formerly Solarpack) is a multinational company fully focused on multi-technology renewables with a vertically integrated business model focused large-scale renewable projects in rapidly growing markets across Europe, North America, Latin America, Asia, and Africa. Headquartered in Spain, Zelestra has more than 1000 employees worldwide and is backed by EQT, one of three largest funds in the world with $200B in assets. One solution doesn‚Äôt fit all, especially in energy. We‚Äôre on a journey alongside our clients, assisting them in achieving their decarbonization goals. We are committed to developing tailored-made solutions by analyzing power market challenges and co-creating structured products based on customer insights. One of the top 10 sellers of clean energy to corporates in the world, according to Bloomberg NEF, we are committed to tailored solutions to meet customer needs. At Zelestra we aim to be a solid and solvent company, capable of executing quality and valuable projects for the society and the environment. Therefore, we maintain a firm commitment to contribute directly to the social development of the communities and markets in which we operate, not only through the creation of economic value, but also through the generation of quality employment and through the social projects we promote. In the efforts of supporting business with innovative digital solutions, we want to recruit an experienced and dynamic Engineer. Mission As Zelestra Digitalhub's Junior Data Engineer, you will be responsible for designing robust data pipelines, building and maintaining databases, and deploying cloud infrastructure on AWS and Microsoft Azure. You will collaborate closely with our product manager and data scientist to ensure seamless data ingestion, model deployment, and system optimization. If you want to build a career in cloud technologies, data engineering, and have a passion for renewable energy, we‚Äôd love to hear from you. Responsibilities We're looking for a Junior Data Engineer to support renewable energy operations by developing and managing data systems. You‚Äôll work on: Database Development: Build and manage databases for solar and wind plant data, ensuring efficient real-time data ingestion using OPC protocols. Data Pipelines: Design scalable, robust pipelines for real-time data processing. Cloud Infrastructure: Deploy and manage cloud solutions on AWS and Azure using services like Glue, Athena, S3, Power BI, and Synapse Analytics. Model Deployment: Support the integration of machine learning models into production pipelines. System Monitoring: Maintain system health with automated monitoring tools and rapid issue resolution. Collaboration: Work closely with data scientists and product managers to align engineering work with product goals. Documentation & Reporting: Ensure clear documentation and deliver insights on system performance and optimization. Required Job Requirements Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Information Technology, or related field. 1‚Äì2 years of experience in software or data engineering with a focus on cloud technologies, preferibly in renewable energy data (solar/wind). Proficient in Python, with experience building data pipelines and working with SQL/NoSQL databases. Familiarity with AWS and Azure cloud platforms, preferibly with cloud certifications (AWS or Azure) and/or data engineering credentials. Exposure and Hands-on expertise with OPC client protocols in cloud-integrated systems is a plus. Strong understanding of Git/version control, and experience maintaining cloud-based infrastructure and data lakes. Excellent communication and collaboration skills, with the ability to explain technical concepts clearly. Familiarity with real-time data processing, containerization (Docker/Kubernetes), and DevOps/CI-CD practices. What We Offer Career opportunities and professional development in a growing multinational company with a team highly qualified. Flexible compensation. Full working day. Remote work 2 days a week. Zelestra celebrates the diversity of thought and experience that comes from a variety of backgrounds including, among others, gender, age, ethnicity... Our mission is to contribute to a fairer and more equitable society. JR1778 Let's co-build a carbon-free tomorrow! Visit us at zelestra.energy",manager,,"Python, SQL, Power BI, Machine Learning",
4241084710,Excel her - ESW Application & Data Analysis Engineer,Volvo Group,"Bengaluru, Karnataka, India",,Full-time,,"About the job Transport is at the core of modern society. Imagine using your expertise to shape sustainable transport and infrastructure solutions for the future? If you seek to make a difference on a global scale, working with next-gen technologies and the sharpest collaborative teams, then we could be a perfect match. About ExcelHer Program Are you looking for an opportunity to restart your career? Do you want to work with an organization that would value your experience no matter when you gained them? How about working with the best minds in the transportation industry where we need more women power? We are pleased to launch the ExcelHer program ‚Äì the career returnship program at Volvo Group in India. The program is for women who have been on a career break for a year or more. This is our step towards empowering women to relaunch their professional journey after their absence from the workplace due to personal commitments. Exciting work assignments have been identified which you can refer in the list below. The assignments are for a tenure of 9 months. The participant of this program would have access to professional development programs, mentoring assistance by a business leader, apart from the experience of working with people from different functions/technologies/culture. Location : Bangalore Responsibilities BE/B.Tech/master‚Äôs degree in electrical and electronics, power electronics, computer engineering (or similar) Experience in the automotive embedded software development Excellent embedded C and C++ programming with debugging skills Excellent in analysing data and finding root cause of issues Knowledge in Classic & Adaptive AUTOSAR SW platform and AUTOSAR methodologies Knowledge of CAN, LIN, J1939, DoIP, UDS protocols Knowledge on EE architecture. Knowledge on Service Oriented Architectures Exposure to tools: Debuggers, CANalyzer, CANoe, Quality tools (MISRA) Knowledge on CI/CD with Jenkin and Klocwork etc Agile SW development. Good communication skill (written and verbal) in English Good analytical and problem-solving skills We value your data privacy and therefore do not accept applications via mail. Who We Are And What We Believe In We are committed to shaping the future landscape of efficient, safe, and sustainable transport solutions. Fulfilling our mission creates countless career opportunities for talents across the group‚Äôs leading brands and entities. Applying to this job offers you the opportunity to join Volvo Group . Every day, you will be working with some of the sharpest and most creative brains in our field to be able to leave our society in better shape for the next generation. We are passionate about what we do, and we thrive on teamwork. We are almost 100,000 people united around the world by a culture of care, inclusiveness, and empowerment. Group Trucks Technology are seeking talents to help design sustainable transportation solutions for the future. As part of our team, you‚Äôll help us by engineering exciting next-gen technologies and contribute to projects that determine new, sustainable solutions. Bring your love of developing systems, working collaboratively, and your advanced skills to a place where you can make an impact. Join our design shift that leaves society in good shape for the next generation.",,,,
4252512053,Data Engineer,"InvestCloud, Inc.","Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job About The Team You will be joining the newly formed AI, Data & Analytics team, primarily responsible as a Data Engineer leading various projects within the new Data Platform team. The new team is focused on driving increased value from the data InvestCloud captures to enable a smarter financial future for our clients, in particular focused on ‚Äúenhanced intelligence‚Äù. Ensuring we have fit-for-purpose modern capabilities is a key goal for the team. We are seeking a Data Engineer with strong expertise in building and maintaining robust data pipelines, optimizing data architecture, and integrating cloud data solutions. The ideal candidate should have hands-on experience with AWS Data services , Oracle , Snowflake and/or Databricks , and a passion for developing scalable data systems that drive business insights and innovation as well as driving data platform modernization. We will be developing and implementing a new data platform leveraging AWS and analytics tools. Helping the business to modernize its end-to-end tech stack and drive AI and analytics business objectives. Key Responsibilities Implement, manage and maintain data platforms such as Oracle, Snowflake, and/or Databricks, ensuring high availability and performance, whilst optimizing for cost. Assist in the Design, development, and maintenance of scalable data pipelines to support diverse analytics and machine learning needs. Optimize and manage data architectures for reliability, scalability, and performance. Implement and support data integration solutions from our data partners, including ETL/ELT processes, ensuring seamless data flow across platforms. Collaborate with Data Scientists, Analysts, and Product Teams to define and support data requirements. Ensure data security and compliance with company policies and relevant regulations. Monitor and troubleshoot data systems to identify and resolve performance issues. Develop and maintain datasets and data pipelines to support Machine Learning model training and deployment Analyze large datasets to identify patterns, trends, and insights that can inform business decisions. Work with 3rd party providers of Data and Data Platform products to evaluate and implement solutions achieving Investcloud‚Äôs business objectives. Required Skills Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Engineering, or a related field, or equivalent practical experience. Minimum of 5 years of professional experience in data engineering or a related role. Proficiency in database technologies, including Oracle and PostgreSQL. Hands-on experience with Snowflake and/or Databricks, with a solid understanding of their ecosystems. Expertise in programming languages such as Python or SQL. Familiarity with ETL/ELT tools and data integration frameworks. Experience with cloud platforms such as AWS, GCP, or Azure. Familiarity with containerization and CI/CD tools (e.g., Docker, Git). Excellent problem-solving skills and the ability to handle complex datasets. Outstanding communication skills to collaborate with technical and non-technical stakeholders globally. Knowledge of data preprocessing, feature engineering, and model evaluation metrics Excellent proficiency in English Ability to work in a fast-paced environment across multiple projects simultaneously Ability to collaborate effectively as a team player, fostering a culture of open communication and mutual respect. Preferred Skills Knowledge of data warehousing and data lake architectures. Familiarity with governance frameworks for data management and security. Knowledge of Machine Learning frameworks (TensorFlow, PyTorch, Scikit-learn) and LLM frameworks (e.g. Langchain) What Do We Offer Join our diverse and international cross-functional team, comprising data scientists, product managers, business analyst and software engineers. As a key member of our team, you will have the opportunity to implement cutting-edge technology to create a next-generation advisor and client experience. Location and Travel The ideal candidate will be expected to work from the London office (with some flexibility). Occasional travel may be required. Compensation The salary range will be competitive and determined based on experience and skills. Equal Opportunity Employer InvestCloud is committed to fostering an inclusive workplace and welcomes applicants from all backgrounds.",manager,,"Python, SQL, Machine Learning",
4240983867,Data Engineer I,FedEx,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job Please note that the Job will close at 12am on Posting Close date, so please submit your application prior to the Close Date Responsible for developing, optimize, and maintaining business intelligence and data warehouse systems, ensuring secure, efficient data storage and retrieval, enabling self-service data exploration, and supporting stakeholders with insightful reporting and analysis. Grade -T2 Please note that the Job will close at 12am on Posting Close date, so please submit your application prior to the Close Date Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making. Implement of business intelligence and analytics systems, ensuring alignment with business requirements. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets. Enable self-service data exploration capabilities for users to analyze and visualize data independently. Develop reporting and analysis applications to generate insights from data for business stakeholders. Design and implement data models to organize and structure data for analytical purposes. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability. Assist in training and support to users on business intelligence tools and applications. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions. Education: Bachelor's degree or equivalent in Computer Science, MIS, Mathematics, Statistics, or similar discipline. Master's degree or PhD preferred. Relevant work experience in data engineering based on the following number of years: Standard I: Two (2) years Standard II: Three (3) years Senior I: Four (4) years Senior II: Five (5) years Knowledge, Skills And Abilities Fluency in English Analytical Skills Accuracy & Attention to Detail Numerical Skills Planning & Organizing Skills Presentation Skills Data Modeling and Database Design ETL (Extract, Transform, Load) Skills Programming Skills FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone. All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances. Our Company FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World‚Äôs Most Admired Companies by ""Fortune"" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding. Our Philosophy The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company. Our Culture Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970‚Äôs. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today‚Äôs global marketplace.",,,,
4240990300,Data Engineer I,FedEx,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Please note that the Job will close at 12am on Posting Close date, so please submit your application prior to the Close Date Responsible for developing, optimize, and maintaining business intelligence and data warehouse systems, ensuring secure, efficient data storage and retrieval, enabling self-service data exploration, and supporting stakeholders with insightful reporting and analysis. Grade -T2 Please note that the Job will close at 12am on Posting Close date, so please submit your application prior to the Close Date Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making. Implement of business intelligence and analytics systems, ensuring alignment with business requirements. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets. Enable self-service data exploration capabilities for users to analyze and visualize data independently. Develop reporting and analysis applications to generate insights from data for business stakeholders. Design and implement data models to organize and structure data for analytical purposes. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability. Assist in training and support to users on business intelligence tools and applications. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions. Education: Bachelor's degree or equivalent in Computer Science, MIS, Mathematics, Statistics, or similar discipline. Master's degree or PhD preferred. Relevant work experience in data engineering based on the following number of years: Standard I: Two (2) years Standard II: Three (3) years Senior I: Four (4) years Senior II: Five (5) years Knowledge, Skills And Abilities Fluency in English Analytical Skills Accuracy & Attention to Detail Numerical Skills Planning & Organizing Skills Presentation Skills Data Modeling and Database Design ETL (Extract, Transform, Load) Skills Programming Skills FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone. All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances. Our Company FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World‚Äôs Most Admired Companies by ""Fortune"" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding. Our Philosophy The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company. Our Culture Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970‚Äôs. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today‚Äôs global marketplace.",,,,
3956025903,Data Engineer,EXO Edge,"Sahibzada Ajit Singh Nagar, Punjab, India",,Full-time,,"About the job EXO Edge is looking for experienced Data Engineer to join our Data Analytics team. As a Data Engineer, you will design, develop, evaluate, and test data infrastructures. Source system analysis, data modeling, business measure development, and report/visualization formatting. Ensure compliance with policies and procedures, safety, state, and federal laws, regulations, and standards. In your journey as a Data Engineer at EXO Edge, you will be: Primarily Focused On Consolidating separate data and reporting suites Ensuring business users are enabled with carefully curated data visualizations with accurate, reliable, and consistent data to monitor contract performance and assist in decision making Determining reporting needs, developing and building reports, training users and establishing reporting cadences and processes. Fulfilling The Below Roles And Responsibilities Translate logical designs into physical designs, taking account of business requirements, target environments, processes, performance requirements, existing systems and services, and any potential safety-related aspects Analyze and organize rawdata. Build data systems andpipelines. Evaluate business needs andobjectives. Interpret trends andpatterns. Conduct complex data analysis and report onresults. Prepare data for prescriptive and predictivemodeling. Build algorithms andprototypes. Combine raw information from differentsources. Explore ways to enhance data quality andreliability. Identify opportunities for dataacquisition. Develop analytical tools andprograms. Collaborate with data scientists and architects on multiple projects and technologies. Bringing In The Below Technical Experience 5+ years of hands-on experience in MS SQL Server writing complex T-SQL queries, stored procedures, functions etc. 5+ years of ETL development using Azure Data Factory (experience with SSIS a plus) 5+ years of experience with building data pipelines and analytics solution to stream and process datasets at low latencies. 5+ years of experience with complex data modelling, ELT/ETL design, and using large databases in a business environment. Experience ingesting data from various source formats viz. Parquet, csv, SQL databases, SharePoint, APIs etc. to Azure SQL databases. 3+ years of experience with SSAS (SQL Analysis Services) Tabular and Multi-Dimensional cubes/models. Experience writing DAX expressions a plus. Expertise in ELT/ETL optimization, designing, coding, and tuning big data processes using Azure Data Factory. Advanced experience in SQL Server relational database management systems Extensive understanding and knowledge in data analytics and reporting (TSQL, SSRS, Power BI, etc.) Strong Knowledge of data warehousing frameworks and methodologies. Knowledge of Data Engineering and Data Operational Excellence using standard methodologies. Knowledge experience with CI/CD processes. Knowledge experience with Azure DevOps. Knowledge of QA tools (manual and automated) Experience in data mining, profiling, and analysis Experience with usage of source control within a team environment (Git, TFS etc.) A strong understanding of database design best practices Experience with Azure ‚Äì setting up and monitoring databases, users access rights. Experience with Yardi system is a big plus. Based At Mohali, Punjab",Associate,,"SQL, Power BI, Data Analysis",
4237579000,Data Engineer,Accolite,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Possess over 5 years of hands-on experience in Azure based Data Engineering , specializing in the development of scalable storage solutions and robust schema layers. - the candidate should have good understanding of Azure Synapse, Azure Data factory, Datawarehouse, SQL, T-SQL - Proficient in a programming language such as Python or Java, along with their respective standard data processing libraries. - Demonstrated expertise in crafting and troubleshooting data pipelines utilizing distributed data frameworks like Apache Spark etc. - Extensive background in working with relational databases (Azure, RDS, ), adept in SQL, data warehousing, and proficient in designing ETL/streaming pipelines. - Proven track record of integrating data from core platforms into a centralized warehouse or datalake. - Adhere to rigorous standards in code quality, implement automated testing, and champion other engineering best practices. - Well-versed in establishing secure systems and access models for handling highly sensitive data. - Exhibit strong cross-functional communication skills, proficient in extracting requirements, and skilled in architecting shared datasets. - Possess a genuine passion for creating exceptional tools that provide a delightful user experience. Immediate Joiners Only",Associate,,"Python, SQL",
4250540548,Data Engineer,Nielsen,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Job Title: Principal Data Engineer (MTS4 / Principal Engineer) About the Role As a Principal Data Engineer, you will drive the strategy, architecture, and execution of large-scale data solutions across our function. This role involves tackling highly ambiguous, complex challenges where the business problem may not be fully defined at the outset. You will partner closely with cross-functional teams (Engineering, Product, Operations) to shape and deliver our data roadmap. Your work will have a profound impact on our functions' data capabilities, influencing multiple teams‚Äô technical and product direction. You should bring deep expertise in designing and developing robust data pipelines and platforms, leveraging technologies such as Spark, Airflow, Kafka, and other emerging tools. You will set standards and best practices that raise the bar for engineering excellence across the organization. Key Responsibilities Architect & Define Scope Own end-to-end design of critical data pipelines and platforms in an environment characterized by high ambiguity. Translate loosely defined business objectives into a clear technical plan, breaking down complex problems into achievable milestones. Technology Leadership & Influence Provide thought leadership in data engineering, driving the adoption of Spark, Airflow, Kafka, and other relevant technologies (e.g., Hadoop, Flink, Kubernetes, Snowflake, etc.). Lead design reviews and champion best practices for coding, system architecture, data quality, and reliability. Influence senior stakeholders (Engineers, EMs, Product Managers) on technology decisions and roadmap priorities. Execution & Delivery Spearhead strategic, multi-team projects that advance the organization‚Äôs data infrastructure and capabilities. Deconstruct complex architectures into simpler components that can be executed by various teams in parallel. Drive operational excellence, owning escalations and ensuring high availability, scalability, and cost-effectiveness of our data solutions. Mentor and develop engineering talent, fostering a culture of collaboration and continuous learning. Impact & Technical Complexity Shape how the organization operates by introducing innovative data solutions and strategic technical direction. Solve endemic, highly complex data engineering problems with robust, scalable, and cost-optimized solutions. Continuously balance short-term business needs with long-term architectural vision. Process Improvement & Best Practices Set and enforce engineering standards that elevate quality and productivity across multiple teams. Lead by example in code reviews, automation, CI/CD practices, and documentation. Champion a culture of continuous improvement, driving adoption of new tools and methodologies to keep our data ecosystem cutting-edge. Qualifications Education & Experience : Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Engineering, or related field (or equivalent experience). 5+ years of software/data engineering experience, with significant exposure to large-scale distributed systems. Technical Expertise : Demonstrated proficiency with Spark, Airflow, Kafka, and at least one major programming language (e.g., Python, Scala, Java). Experience with data ecosystem technologies such as Hadoop, Flink, Snowflake, Kubernetes, etc. Proven track record of architecting and delivering highly scalable data infrastructure solutions. Leadership & Communication : Ability to navigate and bring clarity in ambiguous situations. Strong cross-functional collaboration skills, influencing both technical and non-technical stakeholders. Experience coaching and mentoring senior engineers. Problem-Solving : History of tackling complex, ambiguous data challenges and delivering tangible results. Comfort making informed trade-offs between opportunity vs. architectural complexity.",Manager,,Python,
4232799167,Senior Data Engineer BizTech,Airbnb,"Bengaluru, Karnataka, India (Remote)",Remote,Full-time,,"About the job Airbnb was born in 2007 when two hosts welcomed three guests to their San Francisco home, and has since grown to over 5 million hosts who have welcomed over 2 billion guest arrivals in almost every country across the globe. Every day, hosts offer unique stays and experiences that make it possible for guests to connect with communities in a more authentic way. The Community You Will Join At Airbnb, we need to ensure every area of the business has trustworthy data to fuel insight and innovation. Understanding the business need, securing the right data sources, designing usable data models, and building robust & dependable data pipelines are essential skills to meet this goal. We Are Currently Hiring For The Following Teams Apps and Compliance: This team is responsible for building scalable, high quality data sets andsolutions to enable Airbnb to comply with Tax, payments and legal regulations to ensurebusiness continuity. In addition, this team is responsible for building data sets for Airbnb‚Äôsinternal applications (e.g. CRM data, projects data, workspace data) to fuel growth and driveoperational efficiencies. The Difference You Will Make Apps and Compliance: Our team charter is on enabling Airbnb to comply with Tax, Payments,and Legal regulations so that our Hosts can continue to operate in regulated geos. Ourproducts ingest, process, validate, and deliver large datasets to government authorities (oftenpartnered with tax remittance) so our data must be of the highest accuracy and quality. As youbuild and maintain key components of the critical Compliance data ecosystem, you‚Äôll have theopportunity to contribute to creating standards and best practices for Airbnb‚Äôs DataEngineering. Your work on solving complex business challenges at scale will be instrumental inshaping the tools, processes, and standards used by the broader data community. A Typical Day Design, build, and maintain robust and efficient data pipelines that collect, process, and storedata from various sources, including user interactions, listing details, and external data feeds. Develop data models that enable the efficient analysis and manipulation of data formerchandising optimization. Ensure data quality, consistency, and accuracy. Build scalable data pipelines (SparkSQL & Scala) leveraging Airflow scheduler/executorframework Collaborate with cross-functional teams, including Data Scientists, Product Managers, andSoftware Engineers, to define data requirements, and deliver data solutions that drivemerchandising and sales improvements. Contribute to the broader Data Engineering community at Airbnb to influence tooling andstandards to improve culture and productivity Improve code and data quality by leveraging and contributing to internal tools to automaticallydetect and mitigate issues Your Expertise 5-9+ years of relevant industry experience with a BS/Masters, or 2+ years with a PhD Extensive experience designing, building, and operating robust distributed data platforms (e.g., Spark, Kafka, Flink, HBase) and handling data at the petabyte scale. Strong knowledge of Java, Scala, or Python, and expertise with data processing technologies and query authoring (SQL). Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions Expertise with ETL schedulers such as Apache Airflow, Luigi, Oozie, AWS Glue or similar frameworks Solid understanding of data warehousing concepts and hands-on experience with relational databases (e.g., PostgreSQL, MySQL) and columnar databases (e.g., Redshift, BigQuery, HBase, ClickHouse) Excellent written and verbal communication skills",Manager,,"Python, SQL",
4247873701,Data Engineer - GCP,Egen,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job Job Overview: We are looking for a skilled and motivated Data Engineer with strong experience in Python programming and Google Cloud Platform (GCP) to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust and scalable ETL (Extract, Transform, Load) data pipelines. The role involves working with various GCP services, implementing data ingestion and transformation logic, and ensuring data quality and consistency across systems. Key Responsibilities: 1. Design, develop, test, and maintain scalable ETL data pipelines using Python. 2. Perform data ingestion from various sources and apply transformation & cleansing logic to ensure high-quality data delivery. 3. Implement and enforce data quality checks, validation rules, and monitoring. 4. Collaborate with data scientists, analysts, and other engineering teams to understand data needs and deliver efficient data solutions. 5. Manage version control using GitHub and participate in CI/CD pipeline deployments for data projects. 6 . Write complex SQL queries for data extraction and validation from relational databases such as SQL Server , Oracle , or PostgreSQL 7. Document pipeline designs, data flow diagrams, and operational support procedures. 8. Work extensively on Google Cloud Platform (GCP) services such as: Dataflow for real-time and batch data process. Cloud Functions for lightweight serverless compute. BigQuery for data warehousing and analytics. Cloud Composer for orchestration of data workflows ( Apache Airflow.) Google Cloud Storage (GCS) for managing data at scale. IAM for access control and secure. Cloud Run for containerized applications Required Skills: 4‚Äì6 years of hands-on experience in Python for backend or data engineering projects. Strong understanding and working experience with GCP cloud services (especially Dataflow, BigQuery, Cloud Functions, Cloud Composer, etc.). Solid understanding of data pipeline architecture , data integration , and transformation techniques . Experience in working with version control systems like GitHub and knowledge of CI/CD practices . Strong experience in SQL with at least one enterprise database (SQL Server, Oracle, PostgreSQL, etc.). Good to Have (Optional Skills): Experience working with Snowflake cloud data platform. Hands-on knowledge of Databricks for big data processing and analytics. Familiarity with Azure Data Factory (ADF) and other Azure data engineering tools.",Manager,,"Python, SQL",
4239569284,Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About Amgen Amgen harnesses the best of biology and technology to fight the world‚Äôs toughest diseases, and make people‚Äôs lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what‚Äôs known today. About The Role Role Description: Let‚Äôs do this. Let‚Äôs change the world. We are looking for highly motivated expert Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks, with deep domain knowledge of End-to-End (E2E) Regulatory Integrated Product Teams (IPTs) within the biotech or pharmaceutical industry in biotech or life sciences or pharma. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics The ideal candidate brings a strong foundation in data engineering, combined with an understanding of regulatory processes, scientific data requirements, and structured submission formats such as eCTD, IDMP, xEVMPD, and SPL. Roles & Responsibilities: Design, develop, and manage ETL/ELT pipelines to ingest, transform, and deliver regulatory data from diverse source systems. Collaborate with the Regulatory IPT to understand data requirements for global health authority submissions and ensure data readiness. Build and maintain regulatory data models and structured datasets aligned with submission standards (e.g., eCTD structure, IDMP specifications). Enable seamless data integration across regulatory, clinical, and quality platforms, ensuring consistency and traceability. Implement and maintain data validation and quality checks to ensure accuracy, integrity, and compliance of submission data. Support data lineage, metadata tagging, and audit trail capabilities to meet GxP and regulatory requirements. Proactively identify and implement opportunities to automate tasks and develop reusable frameworks Work in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value Use JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories. Support continuous improvement, test automation, and DevOps practices in the data engineering lifecycle Collaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions Must-Have Skills: Deep domain knowledge of Manufacturing and/or Process Development and/or Supply Chain in biotech or life sciences or pharma. Hands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies. Proficiency in workflow orchestration, performance tuning on big data processing. Strong understanding of AWS services Ability to quickly learn, adapt and apply new technologies Familiarity with tools such as Veeva Vault RIM, Argus, ArisGlobal, or other regulatory information systems. Experience with metadata management, data lineage tools, and data quality frameworks. Strong problem-solving and analytical skills Excellent communication and teamwork skills Experience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices. Good-to-Have Skills: Data Engineering experience in Biotechnology or pharma industry Experience in writing APIs to make the data available to the consumers Experienced with SQL/NOSQL database, vector database for large language models Experienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops Education and Professional Certifications Bachelor‚Äôs degree and 4 to 8 years of Computer Science, IT or related field experience OR Master‚Äôs degree and 1 to 4 years of Computer Science, IT or related field experience AWS Certified Data Engineer preferred Databricks Certificate preferred Scaled Agile SAFe certification preferred Soft Skills: Excellent analytical and troubleshooting skills. Strong verbal and written communication skills Ability to work effectively with global, virtual teams High degree of initiative and self-motivation. Ability to manage multiple priorities successfully. Team-oriented, with a focus on achieving team goals. Ability to learn quickly, be organized and detail oriented. Strong presentation and public speaking skills. EQUAL OPPORTUNITY STATEMENT Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.",Associate,,"Python, SQL",
4239569283,Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About Amgen Amgen harnesses the best of biology and technology to fight the world‚Äôs toughest diseases, and make people‚Äôs lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what‚Äôs known today. About The Role Role Description: Let‚Äôs do this. Let‚Äôs change the world. We are looking for highly motivated expert Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management. Roles & Responsibilities: Design, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets Understand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems Design and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments Ingest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms Ensuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring Expert in data quality, data validation and verification frameworks Innovate, explore and implement new tools and technologies to enhance efficient data processing Proactively identify and implement opportunities to automate tasks and develop reusable frameworks Work in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value Use JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories. Support continuous improvement, test automation, and DevOps practices in the data engineering lifecycle Collaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions Must-Have Skills: Hands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies. Proficiency in workflow orchestration, performance tuning on big data processing. Strong understanding of AWS services Ability to quickly learn, adapt and apply new technologies Strong problem-solving and analytical skills Excellent communication and teamwork skills Experience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices. Good-to-Have Skills: Data Engineering experience in Biotechnology or pharma industry Experience in writing APIs to make the data available to the consumers Experienced with SQL/NOSQL database, vector database for large language models Experienced with data modeling and performance tuning for both OLAP and OLTP databases Experienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops Education and Professional Certifications Master‚Äôs degree and 3 to 4 + years of Computer Science, IT or related field experience OR Bachelor‚Äôs degree and 5 to 8 + years of Computer Science, IT or related field experience AWS Certified Data Engineer preferred Databricks Certificate preferred Scaled Agile SAFe certification preferred Soft Skills: Excellent analytical and troubleshooting skills. Strong verbal and written communication skills Ability to work effectively with global, virtual teams High degree of initiative and self-motivation. Ability to manage multiple priorities successfully. Team-oriented, with a focus on achieving team goals. Ability to learn quickly, be organized and detail oriented. Strong presentation and public speaking skills. EQUAL OPPORTUNITY STATEMENT Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.",Associate,,"Python, SQL",
4230053314,Data Engineer,Uplers,"Coimbatore, Tamil Nadu, India (Remote)",Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4239568542,Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About Amgen Amgen harnesses the best of biology and technology to fight the world‚Äôs toughest diseases, and make people‚Äôs lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what‚Äôs known today. About The Role Role Description: Let‚Äôs do this. Let‚Äôs change the world. We are looking for highly motivated expert Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and maintain data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management. Roles & Responsibilities: Design, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets Understand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems Design and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments Ingest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms Ensuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring Expert in data quality, data validation and verification frameworks Innovate, explore and implement new tools and technologies to enhance efficient data processing Proactively identify and implement opportunities to automate tasks and develop reusable frameworks Work in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value Use JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories. Support continuous improvement, test automation, and DevOps practices in the data engineering lifecycle Collaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions Must-Have Skills: Hands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies. Proficiency in workflow orchestration, performance tuning on big data processing. Strong understanding of AWS services Ability to quickly learn, adapt and apply new technologies Strong problem-solving and analytical skills Excellent communication and teamwork skills Experience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices. Good-to-Have Skills: Data Engineering experience in Biotechnology or pharma industry Experience in writing APIs to make the data available to the consumers Experienced with SQL/NOSQL database, vector database for large language models Experienced with data modeling and performance tuning for both OLAP and OLTP databases Experienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops Education and Professional Certifications Minimum 5 to 8 years of Computer Science, IT or related field experience AWS Certified Data Engineer preferred Databricks Certificate preferred Scaled Agile SAFe certification preferred Soft Skills: Excellent analytical and troubleshooting skills. Strong verbal and written communication skills Ability to work effectively with global, virtual teams High degree of initiative and self-motivation. Ability to manage multiple priorities successfully. Team-oriented, with a focus on achieving team goals. Ability to learn quickly, be organized and detail oriented. Strong presentation and public speaking skills. EQUAL OPPORTUNITY STATEMENT Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.",Associate,,"Python, SQL",
4243744751,"Data Engineer, Product Analytics",Meta,"Bengaluru, Karnataka, India",,Full-time,,"About the job As a Data Engineer at Meta, you will shape the future of people-facing and business-facing products we build across our entire family of applications (Facebook, Instagram, Messenger, WhatsApp, Reality Labs, Threads). Your technical skills and analytical mindset will be utilized designing and building some of the world's most extensive data sets, helping to craft experiences for billions of people and hundreds of millions of businesses worldwide.In this role, you will collaborate with software engineering, data science, and product management teams to design/build scalable data solutions across Meta to optimize growth, strategy, and user experience for our 3 billion plus users, as well as our internal employee community.You will be at the forefront of identifying and solving some of the most interesting data challenges at a scale few companies can match. By joining Meta, you will become part of a world-class data engineering community dedicated to skill development and career growth in data engineering and beyond.Data Engineering: You will guide teams by building optimal data artifacts (including datasets and visualizations) to address key questions. You will refine our systems, design logging solutions, and create scalable data models. Ensuring data security and quality, and with a focus on efficiency, you will suggest architecture and development approaches and data management standards to address complex analytical problems.Product leadership: You will use data to shape product development, identify new opportunities, and tackle upcoming challenges. You'll ensure our products add value for users and businesses, by prioritizing projects, and driving innovative solutions to respond to challenges or opportunities.Communication and influence: You won't simply present data, but tell data-driven stories. You will convince and influence your partners using clear insights and recommendations. You will build credibility through structure and clarity, and be a trusted strategic partner. Data Engineer, Product Analytics Responsibilities: Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains Define and manage Service Level Agreements for all data sets in allocated areas of ownership Solve challenging data integration problems, utilizing optimal Extract, Transform, Load (ETL) patterns, frameworks, query techniques, sourcing from structured and unstructured data sources Improve logging Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts Influence product and cross-functional teams to identify data opportunities to drive impact Minimum Qualifications: Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent 2+ years of experience where the primary responsibility involves working with data. This could include roles such as data analyst, data scientist, data engineer, or similar positions 2+ years of experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala or others.) Preferred Qualifications: Master's or Ph.D degree in a STEM field About Meta: Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today‚Äîbeyond the constraints of screens, the limits of distance, and even the rules of physics. Individual compensation is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable. In addition to base compensation, Meta offers benefits. Learn more about benefits at Meta.",manager,,"Python, SQL",
4237602485,Data Visualization Engineer,Zoetis,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job POSITION SUMMARY Zoetis, Inc. is the world's largest producer of medicine and vaccinations for pets and livestock. Join us at Zoetis India Capability Center (ZICC) in Hyderabad, where innovation meets excellence. As part of the world's leading animal healthcare company, ZICC is at the forefront of driving transformative advancements and applying technology to solve the most complex problems. Our mission is to ensure sustainable growth and maintain a competitive edge for Zoetis globally by leveraging the exceptional talent in India. At ZICC, you'll be part of a dynamic team that partners with colleagues worldwide, embodying the true spirit of One Zoetis. Together, we ensure seamless integration and collaboration, fostering an environment where your contributions can make a real impact. Be a part of our journey to pioneer innovation and drive the future of animal healthcare. Zoetis is seeking a highly skilled and innovative Data Visualization Engineer to join our pharmaceutical R&D team. The ideal candidate will have a strong background in data science, advanced visualization techniques, and expertise in tools and technologies for creating insightful and interactive visualizations across various data types. The role requires close collaboration with scientists, analysts, and other stakeholders to translate complex datasets into impactful visual narratives that drive decision-making in drug discovery, development, and clinical research. POSITION RESPONSIBILITIES Percent of Time Design and Develop Visualizations: * Create interactive and static visualizations for exploratory, descriptive, comparative, and predictive analyses. * Develop dashboards and reports for summarizing key insights from high-throughput screening, clinical trial data, and other R&D datasets. * Implement visual representations for pathway analysis, pharmacokinetics, omics data, and time-series trends. 40% Collaborate with Cross-Functional Teams: * Partner with data scientists, bioinformaticians, pharmacologists, and clinical researchers to identify visualization needs. * Translate complex scientific data into clear, compelling, and actionable visual insights tailored to technical and non-technical audiences. 20% Maintain and Optimize Visualization Tools: * Build reusable visualization components and frameworks to support large-scale data analysis. * Evaluate and recommend tools and platforms for effective data visualization, including emerging technologies. 20% Data Processing and Integration: * Collaborate with data engineers to integrate, clean, and structure datasets for visualization purposes. * Ensure alignment of visualization solutions with pharmaceutical R&D standards, compliance, and security requirements. 10% Innovation and Expertise Development: * Stay updated on the latest trends in visualization technology and methods relevant to pharmaceutical research. * Apply advanced techniques like 3D molecular visualization, network graphs, and predictive modeling visuals. 10% ORGANIZATIONAL RELATIONSHIPS Animal Health Research & Development * Collaborate across the full spectrum of R&D functions, including pharmaceutical, biopharmaceutical, vaccine, device, and genetics R&D groups, to align technology solutions with the diverse needs of scientific disciplines and development pipelines. Zoetis Tech & Digital (ZTD) * Partner closely with ZTD teams, with a particular focus on the VMRD-ZTD Engineering group, documentation specialists, and portfolio management groups, to ensure seamless integration of IT solutions and alignment with organizational objectives. RESOURCES MANAGED Supervision No direct reports, but matrix leadership responsibilities within each project team. Managerial responsibilities for any project resources onboarded externally. EDUCATION AND EXPERIENCE * Education: Bachelor's or Master's degree in Computer Science, Data Science, Bioinformatics, or a related field. * Experience in pharmaceutical or biotech sectors is a strong plus. TECHNICAL SKILLS REQUIREMENTS * Visualization Tools: Expertise in Tableau, Power BI, Plotly, ggplot2, Matplotlib, Seaborn, D3.js, or equivalent. * Programming: Proficiency in Python, R, or JavaScript (e.g., for D3.js). * Data Handling: Experience with SQL, Pandas, NumPy, and ETL processes. * Omics and Network Tools: Familiarity with Cytoscape, BioRender, or molecular visualization platforms (e.g., PyMOL, Chimera). * Dashboarding: Building interactive dashboards with Dash, Shiny, or Streamlit. * 3D Visualization: Experience with tools for structural and spatial visualization (e.g., PyMOL, PyVista). * Soft Skills: o Strong storytelling ability to convey scientific insights visually. o Effective communication and collaboration with interdisciplinary teams. o Analytical thinking to align visualizations with research goals. PHYSICAL POSITION REQUIREMENTS Travel requirements are minimal, 0-10% About Zoetis At Zoetis , our purpose is to nurture the world and humankind by advancing care for animals. As a Fortune 500 company and the world leader in animal health, we discover, develop, manufacture and commercialize vaccines, medicines, diagnostics and other technologies for companion animals and livestock. We know our people drive our success. Our award-winning culture, built around our Core Beliefs, focuses on our colleagues' careers, connection and support. We offer competitive healthcare and retirement savings benefits, along with an array of benefits, policies and programs to support employee well-being in every sense, from health and financial wellness to family and lifestyle resources. Global Job Applicant Privacy Notice",Manager,,"Python, SQL, Tableau, Power BI, R, Data Analysis",
4244950128,Data Engineer - SQL,Veersa Technologies,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job This job is sourced from a job board. Learn More About The Company Veersa is a healthtech company that leverages emerging technology and data science to solve business problems in the US healthcare industry. Veersa has established a niche in serving small and medium entities in the US healthcare space through its tech frameworks, platforms, and tech accelerators. Veersa is known for providing innovative solutions using technology and data science to its client base and is the preferred innovation partner to its clients. Veersas rich technology expertise manifests in the various tech accelerators and frameworks developed in-house to assist in rapid solutions delivery and implementations. Its end-to-end data ingestion, curation, transformation, and augmentation framework has helped several clients quickly derive business insights and monetize data assets. Veersa teams work across all emerging technology areas such as AI/ML, IoT, and Blockchain and using tech stacks as MEAN, MERN, PYTHON, GoLang, ROR, and backend such as Java Springboot, NodeJs, and using databases as PostgreSQL, MS SQL, MySQL, Oracle on AWS and Azure cloud using serverless architecture. Veersa has two major business lines Veersalabs : an In-house R&D and product development platform and Veersa tech consulting : Technical solutions delivery for clients. Veersas customer base includes large US Healthcare software vendors, Pharmacy chains, Payers, providers, and Hospital chains. Though Veersas focus geography is North America, Veersa also provides product engineering expertise to a few clients in Australia and Singapore. About Job Position: SE/ Senior Data Engineer (with SQL, Python, Airflow, Bash) About The Role We are seeking a highly skilled and experienced Senior/Lead Data Engineer to join our growing Data Engineering Team. In this critical role, you will design, architect, and develop cutting-edge multi-tenant SaaS data solutions hosted on Azure Cloud. Your work will focus on delivering robust, scalable, and high-performance data pipelines and integrations that support our enterprise provider and payer data ecosystem. This role is ideal for someone with deep experience in ETL/ELT processes, data warehousing principles, and real-time and batch data integrations. As a senior member of the team, you will also be expected to mentor and guide junior engineers, help define best practices, and contribute to the overall data strategy. We are specifically looking for someone with strong hands-on experience in SQL, Python, and ideally Airflow and Bash scripting. Key Responsibilities Architect and implement scalable data integration and data pipeline solutions using Azure cloud services. Design, develop, and maintain ETL/ELT processes, including data extraction, transformation, loading, and quality checks using tools like SQL, Python, and Airflow. Build and automate data workflows and orchestration pipelines; knowledge of Airflow or equivalent tools is a plus. Write and maintain Bash scripts for automating system tasks and managing data jobs. Collaborate with business and technical stakeholders to understand data requirements and translate them into technical solutions. Develop and manage data flows, data mappings, and data quality & validation rules across multiple tenants and systems. Implement best practices for data modeling, metadata management, and data governance. Configure, maintain, and monitor integration jobs to ensure high availability and performance. Lead code reviews, mentor data engineers, and help shape engineering culture and standards. Stay current with emerging technologies and recommend tools or processes to improve the team's effectiveness. Required Qualifications Must have B.Tech or B.E degree in Computer Science, Information Systems, or any related field. 3+ years of experience in data engineering, with a strong focus on Azure-based solutions. Proficiency in SQL and Python for data processing and pipeline development. Experience in developing and orchestrating pipelines using Airflow (preferred) and writing automation scripts using Bash. Proven experience in designing and implementing real-time and batch data integrations. Hands-on experience with Azure Data Factory, Azure Data Lake, Azure Synapse, Databricks, or similar technologies. Strong understanding of data warehousing principles, ETL/ELT methodologies, and data pipeline architecture. Familiarity with data quality, metadata management, and data validation frameworks. Strong problem-solving skills and the ability to communicate complex technical concepts clearly. Preferred Qualifications Experience with multi-tenant SaaS data solutions. Background in healthcare data, especially provider and payer ecosystems. Familiarity with DevOps practices, CI/CD pipelines, and version control systems (e.g., Git). Experience mentoring and coaching other engineers in technical and architectural decision-making. (ref:hirist.tech)",,,"Python, SQL, R",
4249066169,Data Engineer,Inito,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job This job is sourced from a job board. Learn More As a Data Engineer at Inito, you will play a pivotal role in designing and implementing robust data pipelines that serve as the backbone for our data-driven decision-making process. Working closely with Business Analysts, Data Scientists, and Machine Learning Engineers, you will ensure the integration, transformation, and delivery of data across our platforms, creating a single source of truth that supports complex analytics and machine learning models. We are seeking a passionate and skilled individual who is adept at leveraging cutting-edge technology to solve complex data challenges and enhance our business capabilities. Responsibilities Design, develop, and maintain robust data pipelines using Airflow, DBT, and Python within a BigQuery environment. Implement and maintain data integrations with various sources and targets using tools like Airbyte, Elementary, and Metaflow, with a focus on optimizing data storage and retrieval in BigQuery. Write advanced SQL queries and scripts for data manipulation and retrieval in BigQuery. Develop and optimize data models and schemas in Preset and other visualization tools, tailored for performance in BigQuery. Utilize Pyspark for big data processing tasks, ensuring compatibility with BigQuery's infrastructure. Work with APIs, Docker, and GCP services to enhance and automate data workflows, particularly focusing on integration with BigQuery. Apply best practices in continuous integration and continuous deployment (CI/CD) environments using testing and pipeline validation techniques. Employ asynchronous architectures, queues, caching strategies, and distributed computing to enhance system performance. Write custom code in Python and utilize Jinja within DBT for dynamic pipeline generation, specifically designed for efficiency in BigQuery. Adopt design patterns and engineering best practices to ensure code scalability and maintainability. Key Outcomes Develop a Unified Data Foundation - Spearheaded the creation of a single source of truth that centralizes our data assets, ensuring consistency and reliability across the board. Expertise in Cutting-Edge Technology - Utilize the state-of-the-art DBT stack and automate reports, optimizing our data processing workflows for efficiency and accuracy. Drive Impact Across Teams - Your work will directly influence the success of our Business Analysts, Data Scientists, and Machine Learning Engineers by providing them with the crucial data they need to excel. Optimize Business Logic - Design and manage data tables that support and enhance business logic functions, delivering tailored data solutions that drive strategic decision-making. This job was posted by Manikanta Reddy from Inito. Desired Skills and Experience Data Analysis,Data Streaming,Data Warehousing,ETL,Search,BigQuery,Kafka,Spark,SQL",,,"Python, SQL, Excel, Machine Learning, Data Analysis",
4219961576,Senior Data Engineer,Atlassian,Greater Kolkata Area (Remote),Remote,Full-time,,"About the job Overview Working at Atlassian Atlassians can choose where they work ‚Äì whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company. Responsibilities Atlassian is looking for a Senior Data Engineer to join our Data Engineering team which is responsible for building our data lake, maintaining our big data pipelines / services and facilitating the movement of billions of messages each day. We work directly with the business stakeholders and plenty of platform and engineering teams to enable growth and retention strategies at Atlassian. We are looking for an open-minded, structured thinker who is passionate about building services that scale. On a typical day you will help our stakeholder teams ingest data faster into our data lake, you‚Äôll find ways to make our data pipelines more efficient, or even come up ideas to help instigate self-serve data engineering within the company. You‚Äôll get the opportunity to work on a AWS based data lake backed by the full suite of open source projects such as Spark and Airflow. We are a team with little legacy in our tech stack and as a result you‚Äôll spend less time paying off technical debt and more time identifying ways to make our platform better and improve our users experience. Qualifications As a Senior Data Engineer in the DE team, you will have the opportunity to apply your strong technical experience building highly reliable services on managing and orchestrating a multi-petabyte scale data lake. You enjoy working in a fast paced environment and you are able to take vague requirements and transform them into solid solutions. You are motivated by solving challenging problems, where creativity is as crucial as your ability to write code and test cases. On Your First Day, We'll Expect You To Have A BS in Computer Science or equivalent experience At least 7+ years professional experience as a Sr. Software Engineer or Sr. Data Engineer Strong programming skills (Python, Java or Scala preferred) Experience writing SQL, structuring data, and data storage practices Experience with data modeling Knowledge of data warehousing concepts Experience building data pipelines, platforms Experience with Databricks, Spark, Hive, Airflow and other streaming technologies to process incredible volumes of streaming data Experience in modern software development practices (Agile, TDD, CICD) Strong focus on data quality and experience with internal/external tools/frameworks to automatically detect data issues, anomalies. A willingness to accept failure, learn and try again An open mind to try solutions that may seem crazy at first Experience working on Amazon Web Services (in particular using EMR, Kinesis, RDS, S3, SQS and the like) It's Preferred That You Have Experience building self-service tooling and platforms Built and designed Kappa architecture platforms Contributed to open source projects (Ex: Operators in Airflow) Experience with Data Build Tool (DBT) Our Perks & Benefits Atlassian offers a wide range of perks and benefits designed to support you, your family and to help you engage with your local community. Our offerings include health and wellbeing resources, paid volunteer days, and so much more. To learn more, visit go.atlassian.com/perksandbenefits . About Atlassian At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together. We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines. To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them. To learn more about our culture and hiring process, visit go.atlassian.com/crh .",,,"Python, SQL",
4219150258,Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About Amgen Amgen harnesses the best of biology and technology to fight the world‚Äôs toughest diseases, and make people‚Äôs lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what‚Äôs known today. About The Role Role Description: Let‚Äôs do this. Let‚Äôs change the world. We are looking for highly motivated expert Data Engineer who can own the design, development & maintenance of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management. Roles & Responsibilities: Design, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets Understand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems Design and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments Ingest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms Ensuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring Expert in data quality, data validation and verification frameworks Innovate, explore and implement new tools and technologies to enhance efficient data processing Proactively identify and implement opportunities to automate tasks and develop reusable frameworks Work in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value Use JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories. Support continuous improvement, test automation, and DevOps practices in the data engineering lifecycle Collaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions Must-Have Skills: Hands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies. Proficiency in workflow orchestration, performance tuning on big data processing. Strong understanding of AWS services Ability to quickly learn, adapt and apply new technologies Strong problem-solving and analytical skills Excellent communication and teamwork skills Experience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices. Good-to-Have Skills: Data Engineering experience in Biotechnology or pharma industry Experience in writing APIs to make the data available to the consumers Experienced with SQL/NOSQL database, vector database for large language models Experienced with data modeling and performance tuning for both OLAP and OLTP databases Experienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops Education and Professional Certifications Minimum 5 to 8 years of Computer Science, IT or related field experience AWS Certified Data Engineer preferred Databricks Certificate preferred Scaled Agile SAFe certification preferred Soft Skills: Excellent analytical and troubleshooting skills. Strong verbal and written communication skills Ability to work effectively with global, virtual teams High degree of initiative and self-motivation. Ability to manage multiple priorities successfully. Team-oriented, with a focus on achieving team goals. Ability to learn quickly, be organized and detail oriented. Strong presentation and public speaking skills. EQUAL OPPORTUNITY STATEMENT Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.",Associate,,"Python, SQL",
4214879731,"Associate Staff Engineer, Data Engineer (Snowflake)",Nagarro,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job Company Description üëãüèºWe're Nagarro. We are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in Job Description REQUIREMENTS: Total experience 5+ years. Hands on working experience in Data engineering. Strong working experience in SQL, Python or Scala. Deep understanding of Cloud Design Patterns and their implementation. Experience working with Snowflake as a data warehouse solution. Experience with Power BI data integration. Design, develop, and maintain scalable data pipelines and ETL processes. Work with structured and unstructured data from multiple sources (APIs, databases, flat files, cloud platforms). Strong understanding of data modelling, warehousing (e.g., Star/Snowflake schema), and relational database systems (PostgreSQL, MySQL, etc.) Hands-on experience with ETL tools such as Apache Airflow, Talend, Informatica, or similar. Strong problem-solving skills and a passion for continuous improvement. Strong communication skills and the ability to collaborate effectively with cross-functional teams. RESPONSIBILITIES: Writing and reviewing great quality code. Understanding the client's business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements. Mapping decisions with requirements and be able to translate the same to developers. Identifying different solutions and being able to narrow down the best option that meets the clients' requirements. Defining guidelines and benchmarks for NFR considerations during project implementation Writing and reviewing design documents explaining overall architecture, framework, and high-level design of the application for the developers. Reviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed. Developing and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it. Understanding and relating technology integration scenarios and applying these learnings in projects. Resolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken. Carrying out POCs to make sure that suggested design/technologies meet the requirements. Qualifications Bachelor‚Äôs or master‚Äôs degree in computer science, Information Technology, or a related field.",Associate,,"Python, SQL, Power BI",
4204068455,Data Engineer,Accenture in India,"Coimbatore, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job Project Role : Data Engineer Project Role Description : Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems. Must have skills : Talend ETL Good to have skills : NA Minimum 3 Year(s) Of Experience Is Required Educational Qualification : 15 years full time education Summary: As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Be involved in the end-to-end data management process. Roles & Responsibilities: - Expected to perform independently and become an SME. - Required active participation/contribution in team discussions. - Contribute in providing solutions to work-related problems. - Develop and maintain data pipelines for efficient data processing. - Ensure data quality and integrity throughout the data lifecycle. - Implement ETL processes to extract, transform, and load data. - Collaborate with cross-functional teams to optimize data solutions. - Conduct data analysis to identify trends and insights. Professional & Technical Skills: - Must To Have Skills: Proficiency in Talend ETL. - Strong understanding of data integration and ETL processes. - Experience with data modeling and database design. - Knowledge of SQL and database querying languages. - Hands-on experience with data warehousing concepts. Additional Information: - The candidate should have a minimum of 3 years of experience in Talend ETL. - This position is based at our Hyderabad office. - A 15 years full-time education is required. 15 years full time education",,,"SQL, Data Analysis",
4192841478,Data Engineer,Jade Global,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Data Engineer1 Job Description Job Description Common Skils - SQL, GCP BQ, ETL pipelines using Pythin/Airflow, Experience on Spark/Hive/HDFS, Data modeling for Data conversion Resources (4) Prior experience working on a conv/migration HR project is additional skill needed along with above mentioned skills Common Skils - SQL, GCP BQ, ETL pipelines using Pythin/Airflow, Experience on Spark/Hive/HDFS, Data modeling for Data conversion Resources (4) Prior experience working on a conv/migration HR project is additional skill needed along with above mentioned skills Data Engineer - Knows HR Knowledge , all other requirement from Functional Area given by UBER Customer Name Customer Name uber",,,SQL,
4159493649,Big Data Engineer - P50,Adobe,"Bengaluru, Karnataka, India",,Full-time,,"About the job Our Company Changing the world through digital experiences is what Adobe‚Äôs all about. We give everyone‚Äîfrom emerging artists to global brands‚Äîeverything they need to design and deliver exceptional digital experiences! We‚Äôre passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen. We‚Äôre on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours! The Opportunity Join Adobe in the heart of Bengaluru, where brand-new engineering meets outstanding innovation. As a Data Engineer, you will play a pivotal role in crafting the future of digital experiences. This is an outstanding opportunity to be part of a dynamic and ambitious team dedicated to developing a data platform, data pipelines, reports, and analysis. Your work will be instrumental in delivering powerful technology that empowers users globally. What You Will Do Build a data platform using big data processing and warehousing technologies. Build, maintain, and ideate on frameworks and tools required to ingest, store, process and serve data for global scale, data-intensive systems. Build data pipelines to publish reports and to power dashboards for ALM clients. Collaborate with cross-functional teams to determine key interfaces for your projects and build them with quality and agility. Do analysis and present key findings, insights and concepts to key influencers and leaders to help them make data driven decisions on business and product roadmap. Support Machine Learning engineers and Data Scientists with their dataset needs for training the ML or statistical models and evaluating them. Be aware of the latest open-source projects and technological breakthroughs in the data engineering space and explore them to build better and more effective solutions. Drive innovation through research and experimentation, encouraging an environment where new ideas can thrive. Mentor junior engineers on the team What you need to succeed A bachelor's degree in computer science or related fields. 8-12 years of experience as a server-side developer, with at least 4 years in Data Engineering, with a consistent track record of designing, implementing, and delivering large scale, high-quality solutions. Solid understanding and programming skill in languages such as Java or Python and frameworks and libraries based on them with a focus on data engineering. Strong expertise and familiarity with various Big Data technologies and frameworks including Delta Lake, Apache Hive, Datahub, Apache Spark, Apache Flink, Apache Storm, Trino, and AWS EMR. Strong familiarity with the concepts of RDBMS, Data Lake, Data Warehouse, Data Lakehouse, and Medallion Architecture. Knowledge of at least one workflow orchestration tools such as Airflow or Oozie. Experience working with ML engineers, Data Scientists, product and engineering leadership on their data and analysis needs. Experience working with the DevOps teams to drive operationally excellent infrastructure. Excellent problem-solving skills, with a consistent track record of delivering innovative solutions. Ability to thrive in a collaborative, inclusive, and diverse workplace, embracing different perspectives and ideas. Being comfortable with the ambiguity and having the ability to adapt to evolving priorities. Be a role model and a mentor for the team by doing diligent code and design reviews, by doing knowledge sharing sessions and showcasing sound technical decision making in day-to-day work. Being ahead of the curve by familiarizing yourself with the latest and the best in the technological landscape and turning them into effective solutions for our customers. Adobe is proud to be an Equal Employment Opportunity employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more about our vision here. Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.",,,"Python, Machine Learning",
4246671077,Software Engineer II - Data Engineer,JPMorganChase,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Job Description You thrive on diversity and creativity, and we welcome individuals who share our vision of making a lasting impact. Your unique combination of design thinking and experience will help us achieve new heights. As a Data Engineer II at JPMorgan Chase within the Consumer & Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As an emerging member of a data engineering team, you execute data solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role. Job Responsibilities Executes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems Supports review of controls to ensure sufficient protection of enterprise data Advises and makes custom configuration changes in one to two tools to generate a product at the business or customer request Updates logical or physical data models based on new use cases Frequently uses SQL and understands NoSQL databases and their niche in the marketplace Adds to team culture of diversity, equity, inclusion, and respect Contributes to software and data engineering communities of practice and events that explore new and emerging technologies Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems Required Qualifications, Capabilities, And Skills Formal training or certification on Data Engineering concepts and 3+ years applied experience in AWS and Kubernetes Proficiency in one or more large-scale data processing distributions such as JavaSpark/PySpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on. Experience across the data lifecycle along with expertise with consuming data in any of: batch (file), near real-time (IBM MQ, Apache Kafka), streaming (AWS kinesis, MSK) Good at SQL (e.g., joins and aggregations) Working understanding of NoSQL databases Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages. Solid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis Experience customizing changes in a tool to generate product Preferred Qualifications, Capabilities, And Skills Familiarity with modern front-end technologies Experience designing and building REST API services using Java Exposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable. AWS Developer/Solutions Architect Certification is highly desired About Us JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world‚Äôs most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management. We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants‚Äô and employees‚Äô religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation. About The Team Our Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We‚Äôre proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions ‚Äì all while ranking first in customer satisfaction.",,,"SQL, Data Analysis",
4240034738,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You‚Äôll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you‚Äôll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you‚Äôll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour‚Äôs. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization. Strong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources. Proficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities. Hands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements Preferred Technical And Professional Experience Understanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling. Ability to implement robust data validation, cleansing, and governance frameworks within ETL processes. Proficiency in SQL and/or Shell scripting for custom transformations and automation tasks",,,"SQL, Machine Learning",
4084435905,Data Engineer,Indium,"Hyderabad, Telangana, India (On-site)",On-site,‚Çπ100K/yr - ‚Çπ1.5M/yr,,"About the job This job is sourced from a job board. Learn More Responsibilities 3+ years of Data Engineering Experience - Design, develop, deliver and maintain data infrastructures. SQL Specialist ‚Äì Strong knowledge and Seasoned experience with SQL Queries Languages: Python Good communicator, shows initiative, works well with stakeholders. Experience working closely with Data Analysts and provide the data they need and guide them on the issues. Solid ETL experience and Hadoop/Hive/Pyspark/Presto/ SparkSQL Solid communication and articulation skills Able to handle stakeholders independently with less interventions of reporting manager. Develop strategies to solve problems in logical yet creative ways. Create custom reports and presentations accompanied by strong data visualization and storytelling We Would Be Excited If You Have Excellent communication and interpersonal skills Ability to meet deadlines and manage project delivery Excellent report-writing and presentation skills Critical thinking and problem-solving capabilities Skills:- SQL, Python, Hadoop, HiveQL, Spark and PySpark",manager,,"Python, SQL",
4225796066,Data Engineer,Accenture in India,"Indore, Madhya Pradesh, India (On-site)",On-site,Full-time,,"About the job Project Role : Data Engineer Project Role Description : Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems. Must have skills : Informatica MDM Good to have skills : NA Minimum 3 Year(s) Of Experience Is Required Educational Qualification : 15 years full time education Summary: As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Expand upon the provided project role description and add more details to showcase creativity in your work. Roles & Responsibilities: - Expected to perform independently and become an SME. - Required active participation/contribution in team discussions. - Contribute in providing solutions to work-related problems. - Develop and maintain data pipelines for efficient data processing. - Implement ETL processes to migrate and deploy data across systems. - Ensure data quality and integrity throughout the data lifecycle. - Collaborate with cross-functional teams to optimize data solutions. - Stay updated with industry trends and best practices in data engineering. Professional & Technical Skills: - Must To Have Skills: Proficiency in Informatica MDM. - Strong understanding of data modeling and database concepts. - Experience with data integration tools and technologies. - Hands-on experience in designing and implementing data solutions. - Knowledge of data governance and data security practices. Additional Information: - The candidate should have a minimum of 3 years of experience in Informatica MDM. - This position is based at our Indore office. - A 15 years full-time education is required. 15 years full time education",,,,
4226400700,Data QA Engineer,Dario,"Gurugram, Haryana, India (Hybrid)",Hybrid,Full-time,,"About the job At Dario, Every Day is a New Opportunity to Make a Difference. We are on a mission to make better health easy. Every day our employees contribute to this mission and help hundreds of thousands of people around the globe improve their health. How cool is that? We are looking for passionate, smart, and collaborative people who have a desire to do something meaningful and impactful in their career. Responsibilities Design and execute test plans to validate data pipelines, transformations, and business logic Perform data validation and reconciliation across multiple systems (Snowflake, DBT, etc.) Develop automated data quality checks using Python, SQL, and testing frameworks Collaborate with Data Engineering and Analytics teams to ensure accuracy and relevance of data used for reporting and KPIs Validate business rules and metrics implemented in dashboards (e.g., Tableau) Ensure alignment between data definitions, business expectations, and actual outputs Identify data quality issues, report defects using JIRA, and follow through to resolution Maintain test documentation such as STP, STD, STR for data validation processes Monitor scheduled jobs and alerting systems for data pipeline failures Requirements: 5+ years of experience in QA or data quality roles Strong proficiency in Python and SQL Experience testing ETL/ELT processes, preferably in Snowflake or similar DWH platforms Familiarity with data pipeline tools (Airflow, DBT, Fivetran, etc.) Strong understanding of QA methodologies (STP/STD/STR) Excellent communication skills Fast learner of new tools and systems Business-minded with the ability to understand and validate KPIs and reporting logic Ability to work independently and proactively Preferred Qualifications: Experience with test automation tools (e.g., PyTest, Great Expectations) Experience validating dashboards in Tableau or similar tools Familiarity with Git, JIRA, and CI/CD processes Knowledge of healthcare data or analytics is a plus",,,"Python, SQL, Tableau",
4248487522,Data Engineer,Takeda,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job By clicking the ‚ÄúApply‚Äù button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda‚Äôs Privacy Notice and Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge. Job Description: The Future Begins Here: At Takeda, we are leading digital evolution and global transformation. By building innovative solutions and future-ready capabilities, we are meeting the need of patients, our people, and the planet. Bangalore, the city, which is India‚Äôs epicenter of Innovation, has been selected to be home to Takeda‚Äôs recently launched Innovation Capability Center. We invite you to join our digital transformation journey. In this role, you will have the opportunity to boost your skills and become the heart of an innovative engine that is contributing to global impact and improvement. At Takeda‚Äôs ICC we Unite in Diversity : Takeda is committed to creating an inclusive and collaborative workplace, where individuals are recognized for their backgrounds and abilities they bring to our company. We are continuously improving our collaborators journey in Takeda, and we welcome applications from all qualified candidates. Here, you will feel welcomed, respected, and valued as an important contributor to our diverse team. Job Summary : As a Data Engineer, you will be building and maintaining data systems and construct datasets that are easy to analyze and support Business Intelligence requirements as well as downstream systems. Develops and maintains scalable data pipelines and builds out new integrations using AWS native technologies to support continuing increases in data source, volume, and complexity. Collaborates with analytics and business teams to improve data models that feed business intelligence tools and dashboards, increasing data accessibility and fostering data-driven decision making across the organization. Implements processes and systems to drive data reconciliation, monitor data quality, ensuring production data is always accurate and available for key stakeholders, downstream systems, and business processes that depend on it. Writes unit/integration/performance test scripts, contributes to engineering wiki, and documents work. Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Works closely with a team of frontend and backend engineers, product managers, and analysts. Works with DevOps and Cloud Center of Excellence to deploy data pipeline solutions in Takeda AWS environments meeting security and performance requirements. Skills and Qualifications : Bachelors‚Äô Degree, from an accredited institution in Engineering, Computer Science, or related field. 3+ years of experience in software, data, data warehouse, data lake, and analytics reporting development. Strong experience in data/Big Data, data integration, data model, modern database (Graph, SQL, No-SQL, etc.) query languages and AWS cloud technologies including DMS, Lambda, Databricks, SQS, Step Functions, Data Streaming, Visualization, etc. Solid experience in DBA, dimensional modeling, SQL optimization - Aurora is preferred. Experience designing, building, maintaining data integrations using SOAP/REST web services/API, as well as schema design and dimensional data modeling. Excellent written and verbal communication skills including the What Takeda Can Offer You: Takeda is certified as a Top Employer, not only in India, but also globally. No investment we make pays greater dividends than taking good care of our people. At Takeda, you take the lead on building and shaping your own career. Joining the ICC in Bangalore will give you access to high-end technology, continuous training and a diverse and inclusive network of colleagues who will support your career growth. Benefits: It is our priority to provide competitive compensation and a benefit package that bridges your personal life with your professional career. Amongst our benefits are Competitive Salary + Performance Annual Bonus Flexible work environment, including hybrid working Comprehensive Healthcare Insurance Plans for self, spouse, and children Group Term Life Insurance and Group Accident Insurance programs Health & Wellness programs including annual health screening, weekly health sessions for employees. Employee Assistance Program Broad Variety of learning platforms Diversity, Equity, and Inclusion Programs No Meeting Days Reimbursements ‚Äì Home Internet & Mobile Phone Employee Referral Program Leaves ‚Äì Paternity Leave (4 Weeks) , Maternity Leave (up to 26 weeks), Bereavement Leave (5 days) About ICC in Takeda: Takeda is leading a digital revolution. We‚Äôre not just transforming our company; we‚Äôre improving the lives of millions of patients who rely on our medicines every day. As an organization, we are committed to our cloud-driven business transformation and believe the ICCs are the catalysts of change for our global organization. Locations: IND - Bengaluru Worker Type: Employee Worker Sub-Type: Regular Time Type: Full time",manager,,"SQL, Data Analysis",
4248304501,"Senior Data Engineer, ITC",Nike,"Karnataka, India (On-site)",On-site,Full-time,,"About the job Who You‚Äôll Work With This role is part of the Nike‚Äôs Content Technology team within Consumer Product and Innovation (CP&I) organization, working very closely with the globally distributed Engineering and Product teams. This role will roll up to the Director Software Engineering based out of Nike India Tech Centre. Who We Are Looking For We are looking for experienced Technology focused and hands on Lead Engineer to join our team in Bengaluru, India. As a Senior Data Engineer, you will play a key role in ensuring that our data products are robust and capable of supporting our Data Engineering and Business Intelligence initiatives. A data engineer with 5+ years of experience working with cloud-native platforms. Advanced skills in SQL, PySpark, Apache Airflow (or similar workflow management tools), Databricks, and Snowflake. Deep understanding of Spark optimization, Delta Lake, and Medallion architecture. Strong experience in data modeling and data quality practices. Experience with Tableau for data validation and monitoring. Exposure to DevOps practices, CI/CD, Git, and security aspects. Effective mentorship and team collaboration skills. Strong communication skills, able to explain technical concepts clearly. Experience with Kafka or other real-time systems, Preferred: Familiarity with ML/GenAI integration into pipelines. Databricks Data Engineer certification. What You‚Äôll Work On Own and optimize large-scale ETL/ELT pipelines and reusable frameworks. Collaborate with cross-functional teams to translate business requirements into technical solutions. Guide junior engineers through code reviews and design discussions. Monitor data quality, availability, and system performance. Lead CI/CD implementation and improve workflow automation.",Director,,"SQL, Tableau",
4205831569,Data Engineer || 5 to 6 || Pune,Capgemini,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Job Description The AI/Data Engineer will be responsible for designing, implementing, and maintaining scalable data solutions. This role will involve working with various data tools and technologies to ensure efficient data processing, integration, and visualization. Key Responsibilities: Develop and maintain data pipelines and ETL processes to ingest and transform data from multiple sources. Design, implement, and manage data models and databases using SQL. Utilize Python for data manipulation, analysis, and automation tasks. Administer and automate processes on Linux systems using shell scripting and tools like Putty. Schedule and monitor jobs using Control-M or similar scheduling tools. Create interactive dashboards and reports using Tableau and Power BI to support data-driven decision-making. Collaborate with data scientists and analysts to support AI/ML model deployment and integration. Ensure data quality, integrity, and security across all data processes. Utilize version control software, such as Git and Bitbucket, to manage and track code changes effectively. Qualifications: Bachelor‚Äôs degree in computer science, Information Technology, or a related field. At least 3 years of proven experience as a Data Engineer, AI Engineer, or similar role. Proficiency in Python and SQL.",,,"Python, SQL, Tableau, Power BI",
4248771849,PySpark Data Engineer,Viraaj HR Solutions Private Limited,"Bhubaneswar, Odisha, India (On-site)",On-site,‚Çπ1.2M/yr - ‚Çπ2M/yr,,"About the job Company Overview Viraaj HR Solutions is a leading recruitment firm in India, dedicated to connecting top talent with industry-leading companies. We focus on understanding the unique needs of each client, providing tailored HR solutions that enhance their workforce capabilities. Our mission is to empower organizations by bridging the gap between talent and opportunity. We value integrity, collaboration, and excellence in service delivery, ensuring a seamless experience for both candidates and employers. Job Title: PySpark Data Engineer Work Mode: On-Site Location: India Role Responsibilities Design, develop, and maintain data pipelines using PySpark. Collaborate with data scientists and analysts to gather data requirements. Optimize data processing workflows for efficiency and performance. Implement ETL processes to integrate data from various sources. Create and maintain data models that support analytical reporting. Ensure data quality and accuracy through rigorous testing and validation. Monitor and troubleshoot production data pipelines to resolve issues. Work with SQL databases to extract and manipulate data as needed. Utilize cloud technologies for data storage and processing solutions. Participate in code reviews and provide constructive feedback. Document technical specifications and processes clearly for team reference. Stay updated with industry trends and emerging technologies in big data. Collaborate with cross-functional teams to deliver data solutions. Support the data governance initiatives to ensure compliance. Provide training and mentorship to junior data engineers. Qualifications Bachelor's degree in Computer Science, Information Technology, or related field. Proven experience as a Data Engineer, preferably with PySpark. Strong understanding of data warehousing concepts and architecture. Hands-on experience with ETL tools and frameworks. Proficiency in SQL and NoSQL databases. Familiarity with cloud platforms like AWS, Azure, or Google Cloud. Experience with Python programming for data manipulation. Knowledge of data modeling techniques and best practices. Ability to work in a fast-paced environment and juggle multiple tasks. Excellent problem-solving skills and attention to detail. Strong communication and interpersonal skills. Ability to work independently and as part of a team. Experience in Agile methodologies and practices. Knowledge of data governance and compliance standards. Familiarity with BI tools such as Tableau or Power BI is a plus. Skills: data modeling,python programming,pyspark,bi tools,sql proficiency,sql,cloud technologies,nosql databases,etl processes,data warehousing,agile methodologies,cloud computing,data engineer",,,"Python, SQL, Tableau, Power BI",
4233592152,Data Engineer,Infogain,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Looking for Data Engineers Location: Bangalore/Pune/Delhi/NCR Experience: 3-12 years (Individual Contributors) Skill: Databricks plus strong knowledge of SQL We will consider only Immediate Joiners Please send your resume with the following details to jeena.cherian@infogain.com Total Exp: Relevant Exp: Current CTC: Expected CTC: Notice Period: Any offer in hand: If yes, what‚Äôs the offer amount: Current Location: Preferred Location: Date of Birth:",,,SQL,
4004024913,Staff Data Engineer - Denado  [T500-13483],Talent500,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job JOB DUTIES: Translate business requirements into data requirements, data warehouse design and sustaining data management strategies on enterprise data platforms like Snowflake. Work with project leads, stakeholders, and business SMEs to define technical specifications to develop data modeling requirements and maintain data infrastructure to provide business users with the tools and data needed. Solution architect, design and develop large scale and optimized analytics solutions. Requirements gathering and analysis, development planning and coordination in collaboration with stakeholder teams. Understand data architecture & solution design, design & develop dimensional models in an enterprise data warehouse environment. Development and automation of enterprise data transformation pipelines, ELT/ ETL processes. Work with cross functional teams and process owners on the development of test cases and scripts, test models and solutions to verify that requirements are met and ensuring high levels of data quality. Develop and apply quality assurance best practices. Design and apply data engineering best practices for an enterprise data warehouse. Analyze data and data behaviors to support business user queries. Excellent understanding of impact due to changes in data platforms, data models and data behaviors. Excellent problem-solving skills and ability to troubleshoot complex data engineering issues. Benchmark application operational performance periodically, track (metrics) and fix issues. Understand and comply with data governance and compliance practices as defined for risk management. This includes data encryption practices, RBAC and security policies. Promote and apply metadata management best practices supporting enterprise data catalogs. Support change and release management processes. Support incident and response management including problem solving and root cause analysis, documentation. Support automation and on-call processes (Tier 1 / Tier 2). Specific Skills Or Other Requirements: Requires 8 + years of experience with: Primary Experience Data Warehouses and Data Transformations Data Platforms: Required: Snowflake cloud data platform Preferred: Denodo data virtualization, SAP HANA analytics Data Engineering: Snowflake: 5+ years of required expertise with Snowflake SnowSQL, Snowpipe (integrated with AWS S3), Streams and Tasks, Stored Procedures, Merge statements, Functions, RBAC, Security Policies, Compute and Storage usage optimization techniques, Performance optimization techniques. Dbt Cloud: 3+ years of expertise with dbt cloud platform, very good understanding of data models (views, data materializations, incremental data loads, snapshots), cross functional references, DAGs and its impact, job scheduling, audit and monitoring, working with code repositories, deployments. SnapLogic: Data integrations with SnapLogic integration platform is a plus. Certifications: Snowflake and dbt Cloud data engineering certifications is a plus. Data Orchestration: Required: Control-M, Apache Airflow Cloud Storage Platforms: Required: Amazon Web Services Preferred: Microsoft Azure Programming/ Scripting: Snowflake: SQL Scripting (Snowpipes, Tasks, Streams, Merge Statements, Stored Procedures, Functions, Security Policies ‚Äì DDM, Row-Access), SQL Scripting, Python Code Management: Required: Excellent understanding of working with code repositories like GitHub, GitLab ,code version management, branching and merging patterns in a central repository managing cross functional code deployments. Data Operations: Excellent understanding of Data Ops practices for data management. Solution Design: Good understanding of end-to-end solution architecture and design practices, ability to document solutions (maintain diagrams) Stakeholder Engagement: Ability to take the lead and drive project activities collaborating with Analytics stakeholders and ensure the requirement is completed. Data Warehousing: Excellent on fundamental concepts of dimensional modeling, experience working on data warehouse solutions, requirement gathering, design & build, data analysis, data quality. data validations, developing data transformations using ELT/ ETL patterns. Source Systems: Required : Knowledge and experience maintaining dimensional data model built on data from source systems like SAP ERP (On premises, Cloud), Salesforce CRM, Workday, Planisware, Team Center PLM. Other source systems are a plus. Data As-A-Product: Preferred knowledge and experience working with data treated as data products. is following a hybrid data mesh architecture which promotes data as a product for data lifecycle management. Governance: Good understanding of working with companies having regulated systems and processes for data. Adherence to data protection practices using tagging, security policies and data security (object-level, column-level, row-level). Promoting and applying best practices for data catalogs, following data classification practices and metadata management for data products within your scope. Operating Systems: Windows, Linux Education & Experience: Bachelor‚Äôs degree equivalent in Computer Science/Engineering or equivalent degree.",,,"Python, SQL, Data Analysis",
4246672060,Software Engineer II - Data Engineer,JPMorganChase,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Job Description You thrive on diversity and creativity, and we welcome individuals who share our vision of making a lasting impact. Your unique combination of design thinking and experience will help us achieve new heights. As a Data Engineer II at JPMorgan Chase within the Consumer & Community Banking Team, you are part of an agile team that works to enhance, design, and deliver the data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As an emerging member of a data engineering team, you execute data solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role. Job Responsibilities Executes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems Supports review of controls to ensure sufficient protection of enterprise data Advises and makes custom configuration changes in one to two tools to generate a product at the business or customer request Updates logical or physical data models based on new use cases Frequently uses SQL and understands NoSQL databases and their niche in the marketplace Adds to team culture of diversity, equity, inclusion, and respect Contributes to software and data engineering communities of practice and events that explore new and emerging technologies Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems Required Qualifications, Capabilities, And Skills Formal training or certification on Data Engineering concepts and 3+ years applied experience in AWS and Kubernetes Proficiency in one or more large-scale data processing distributions such as JavaSpark/PySpark along with knowledge on Data Pipeline (DPL), Data Modeling, Data warehouse, Data Migration and so-on. Experience across the data lifecycle along with expertise with consuming data in any of: batch (file), near real-time (IBM MQ, Apache Kafka), streaming (AWS kinesis, MSK) Good at SQL (e.g., joins and aggregations) Working understanding of NoSQL databases Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages. Solid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis Experience customizing changes in a tool to generate product Preferred Qualifications, Capabilities, And Skills Familiarity with modern front-end technologies Experience designing and building REST API services using Java Exposure to cloud technologies - knowledge on Hybrid cloud architectures is highly desirable. AWS Developer/Solutions Architect Certification is highly desired About Us JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world‚Äôs most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management. We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants‚Äô and employees‚Äô religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation. About The Team Our Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We‚Äôre proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions ‚Äì all while ranking first in customer satisfaction.",,,"SQL, Data Analysis",
4247901284,Azure Data Engineer,Deloitte,"Bangalore Urban, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Technology & Transformation: EAD: Azure Data Engineer-Consultant/Senior Consultant/Manager Your potential, unleashed. India‚Äôs impact on the global economy has increased at an exponential rate and Deloitte presents an opportunity to unleash and realize your potential amongst cutting edge leaders, and organizations shaping the future of the region, and indeed, the world beyond. At Deloitte, your whole self to work, every day. Combine that with our drive to propel with purpose and you have the perfect playground to collaborate, innovate, grow, and make an impact that matters. The Team Deloitte‚Äôs Technology & Transformation practice can help you uncover and unlock the value buried deep inside vast amounts of data. Our global network provides strategic guidance and implementation services to help companies manage data from disparate sources and convert it into accurate, actionable information that can support fact-driven decision-making and generate an insight-driven advantage. Our practice addresses the continuum of opportunities in business intelligence & visualization, data management, performance management and next-generation analytics and technologies, including big data, cloud, cognitive and machine learning. Your work profile: As a Consultant/Senior Consultant/Manager in our T&T Team you‚Äôll build and nurture positive working relationships with teams and clients with the intention to exceed client expectations: - Design, develop and deploy solutions using different tools, design principles and conventions. Configure robotics processes and objects using core workflow principles in an efficient way; ensure they are easily maintainable and easy to understand. Understand existing processes and facilitate change requirements as part of a structured change control process. Solve day to day issues arising while running robotics processes and provide timely resolutions. Maintain proper documentation for the solutions, test procedures and scenarios during UAT and Production phase. Coordinate with process owners and business to understand the as-is process and design the automation process flow. Desired Qualifications 3-15 Years of hands-on experience Implementing Azure Cloud data warehouses, Azure and No-SQL databases and hybrid data scenarios. Experience developing Azure Data Factory (covering Azure Functions, LogicApps, Triggers, IR), Databricks (pySpark, Scala), Stream Analytics, Event Hub & HD Insight Components Experience in working on data lake & DW solutions on Azure. Experience managing Azure DevOps pipelines (CI/CD) Experience managing source data access security, using Vault, configuring authentication and authorization, enforcing data policies and standards. UG: B. Tech /B.E. in Any Specialization . Location and way of working: Base location: Pan India This profile involves occasional travelling to client locations. Hybrid is our default way of working. Each domain has customized the hybrid approach to their unique needs. Your role as a Consultant/Senior Consultant/Manager: We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society. In addition to living our purpose, Consultant/Senior Consultant/Manager across our organization must strive to be: Inspiring - Leading with integrity to build inclusion and motivation. Committed to creating purpose - Creating a sense of vision and purpose. Agile - Achieving high-quality results through collaboration and Team unity. Skilled at building diverse capability - Developing diverse capabilities for the future. Persuasive / Influencing - Persuading and influencing stakeholders. Collaborating - Partnering to build new solutions. Delivering value - Showing commercial acumen Committed to expanding business - Leveraging new business opportunities. Analytical Acumen - Leveraging data to recommend impactful approach and solutions through the power of analysis and visualization. Effective communication ‚Äì Must be well abled to have well-structured and well-articulated conversations to achieve win-win possibilities. Engagement Management / Delivery Excellence - Effectively managing engagement(s) to ensure timely and proactive execution as well as course correction for the success of engagement(s) Managing change - Responding to changing environment with resilience Managing Quality & Risk - Delivering high quality results and mitigating risks with utmost integrity and precision Strategic Thinking & Problem Solving - Applying strategic mindset to solve business issues and complex problems. Tech Savvy - Leveraging ethical technology practices to deliver high impact for clients and for Deloitte Empathetic leadership and inclusivity - creating a safe and thriving environment where everyone's valued for who they are, use empathy to understand others to adapt our behaviours and attitudes to become more inclusive. How you‚Äôll grow Connect for impact Our exceptional team of professionals across the globe are solving some of the world‚Äôs most complex business problems, as well as directly supporting our communities, the planet, and each other. Know more in our Global Impact Report and our India Impact Report . Empower to lead You can be a leader irrespective of your career level. Our colleagues are characterised by their ability to inspire, support, and provide opportunities for people to deliver their best and grow both as professionals and human beings. Know more about Deloitte and our One Young World partnership. Inclusion for all At Deloitte, people are valued and respected for who they are and are trusted to add value to their clients, teams and communities in a way that reflects their own unique capabilities. Know more about everyday steps that you can take to be more inclusive. At Deloitte, we believe in the unique skills, attitude and potential each and every one of us brings to the table to make an impact that matters. Drive your career At Deloitte, you are encouraged to take ownership of your career. We recognise there is no one size fits all career path, and global, cross-business mobility and up / re-skilling are all within the range of possibilities to shape a unique and fulfilling career. Know more about Life at Deloitte. Everyone‚Äôs welcome‚Ä¶ entrust your happiness to us Our workspaces and initiatives are geared towards your 360-degree happiness. This includes specific needs you may have in terms of accessibility, flexibility, safety and security, and caregiving. Here‚Äôs a glimpse of things that are in store for you. Interview tips We want job seekers exploring opportunities at Deloitte to feel prepared, confident and comfortable. To help you with your interview, we suggest that you do your research, know some background about the organisation and the business area you‚Äôre applying to. Check out recruiting tips from Deloitte professionals.",Manager,,"SQL, Machine Learning",
4252299536,Python Developer,ClearTrail Technologies,"Indore, Madhya Pradesh, India (On-site)",On-site,Full-time,,"About the job About Company: At ClearTrail, work is more than ‚Äòjust a job‚Äô. Our calling is to develop solutions that empower those dedicated to keeping their people, places and communities safe. For over 23 years, law enforcement & federal agencies across the globe have trusted ClearTrail as their committed partner in safeguarding nations & enriching lives. We are envisioning the future of intelligence gathering by developing artificial intelligence and machine learning-based lawful interception & communication analytics solutions that solve the world‚Äôs most challenging problems. Role ‚Äì Python Developer Years of Experience Required- 2-4 year Location ‚Äì Indore (Work from office 5 days) Qualification : BE/B.Tech/MCA/M.Tech Job Description : Proficiency in Python: Strong grasp of basic concepts. Requirement Understanding: Ability to analyze and interpret business needs, ensuring precise implementation. Solution-Oriented Thinking: Aptitude for troubleshooting issues and developing efficient solutions. Roles & Responsibility : Write clean, efficient, and maintainable code. Test and troubleshoot Python-based applications for optimal performance. Integrate Python solutions with databases, APIs, or external services",,,"Python, Machine Learning",
4250519771,Data Engineer,Tanla Platforms Limited,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Job Overview: The person will be responsible for expanding and optimizing our data and data pipeline architecture. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. You‚Äôll be Responsible for ? Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Cloud technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems. You‚Äôd have? We are looking for a candidate with 3+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: Experience with data pipeline and workflow management tools: Apache Airflow, NiFi, Talend etc. ‚Ä¢ Experience with relational SQL and NoSQL databases, including Clickhouse, Postgres and MySQL. Experience with stream-processing systems: Storm, Spark-Streaming, Kafka etc. Experience with object-oriented/object function scripting languages: Python, Scala, etc. Experience building and optimizing data pipelines, architectures and data sets. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. Working knowledge of message queuing, stream processing, and highly scalable data stores Why Join us? Impactful Work: Play a pivotal role in safeguarding Tanla's assets, data, and reputation in the industry. Tremendous Growth Opportunities: Be part of a rapidly growing company in the telecom and CPaaS space, with opportunities for professional development. Innovative Environment: Work alongside a world-class team in a challenging and fun environment, where innovation is celebrated. Tanla is an equal opportunity employer. We champion diversity and are committed to creating an inclusive environment for all employees. www.tanla.com",,,"Python, SQL",
4238449223,Data Engineer,Encora Inc.,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Job Details Location: Pune Experience: 7 to 9 years Job Mode: Full-time Notice Period: 30 days or less Working Mode: Hybrid (2 days a week ) Job Summary As a Lead Data Engineer, you will be a key member of our data team, responsible for developing and maintaining infrastructure and pipelines. You will collaborate with cross-functional teams to support data-driven decision-making and contribute to the growth and success of our data-driven initiatives. This is a key role for an experienced engineer who thrives in developing high-performance, scalable, secure, and resilient data systems using AWS cloud-native services, Python, and a mix of relational and non-relational databases. This position offers the opportunity to work with the latest cloud technologies, while also supporting and modernizing legacy on-prem systems built with Microsoft technologies. You should be passionate about crafting modern data architectures and working across the full data lifecycle‚Äîfrom ingestion to modeling to storage and access. Key Responsibilities: ‚Ä¢ Architect, develop, and maintain high-throughput, fault-tolerant data pipelines using Python and AWS services such as AWS Glue, Lambda, Kinesis, Step Functions, and S3. ‚Ä¢ Design and optimize data models for both structured (relational) and semi/unstructured (non-relational) data, using databases such as Amazon RDS, Aurora, DynamoDB, and Microsoft SQL Server (on-prem). ‚Ä¢ Develop and integrate robust data ingestion solutions using REST APIs, messaging systems (SNS/SQS/Kinesis), and file-based ingestion. ‚Ä¢ Collaborate with product and analytics teams to gather requirements and build data solutions that are scalable and maintainable. ‚Ä¢ Implement data quality, lineage, and validation checks to ensure reliable data delivery. ‚Ä¢ Lead efforts to migrate on-prem workloads to cloud-native solutions on AWS. ‚Ä¢ Provide technical mentorship and guidance to junior team members and foster best practices in coding, testing, and operations. Qualifications: ‚Ä¢ Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Engineering, or a related field. ‚Ä¢ 5+ years of experience in Data Engineering or similar roles, with a strong emphasis on cloud-native architectures. ‚Ä¢ Deep expertise in Python for backend data processing, scripting, and automation. ‚Ä¢ Strong hands-on experience with a broad range of AWS data services, such as: o AWS Glue, Lambda, Step Functions, Kinesis, Athena, Redshift, S3, RDS, DynamoDB ‚Ä¢ Solid understanding of relational databases (e.g., PostgreSQL, MySQL, SQL Server) and non-relational databases (e.g., DynamoDB, MongoDB, Elasticsearch). ‚Ä¢ Proficiency in REST API development and integration, including security, authentication, and performance tuning. ‚Ä¢ Strong knowledge of CI/CD pipelines, infrastructure-as-code (IaC), and version control (e.g., Git). ‚Ä¢ Excellent debugging, analytical, and communication skills. Preferred Skills: ‚Ä¢ Experience with big data tools such as Apache Spark, Hadoop, or EMR. ‚Ä¢ Familiarity with data cataloging, data governance, and data lake patterns. ‚Ä¢ Exposure to containerized environments (Docker, ECS, EKS) and orchestration. ‚Ä¢ Experience working with monitoring and alerting tools such as CloudWatch, Datadog, or Prometheus. Nice to Have: ‚Ä¢ Certifications in AWS (e.g., AWS Certified Data Analytics, Solutions Architect) or related disciplines. ‚Ä¢ Familiarity with data visualization and reporting tools such as QuickSight, Power BI, or Tableau. About Encora Encora is the preferred digital engineering and modernization partner of some of the world‚Äôs leading enterprises and digital native companies. With over 9,000 experts in 47+ offices and innovation labs worldwide, Encora‚Äôs technology practices include Product Engineering & Development, Cloud Services, Quality Engineering, DevSecOps, Data & Analytics, Digital Experience, Cybersecurity, and AI & LLM Engineering. At Encora, we hire professionals based solely on their skills and qualifications, and do not discriminate based on age, disability, religion, gender, sexual orientation, socioeconomic status, or nationality.",associate,,"Python, SQL, Tableau, Power BI",
4240983868,Data Engineer I,FedEx,Mumbai Metropolitan Region,,Full-time,,"About the job Responsible for developing, optimize, and maintaining business intelligence and data warehouse systems, ensuring secure, efficient data storage and retrieval, enabling self-service data exploration, and supporting stakeholders with insightful reporting and analysis. Grade - T2 Please note that the Job will close at 12am on Posting Close date, so please submit your application prior to the Close Date What Your Main Responsibilities Are Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making. Implement of business intelligence and analytics systems, ensuring alignment with business requirements. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets. Enable self-service data exploration capabilities for users to analyze and visualize data independently. Develop reporting and analysis applications to generate insights from data for business stakeholders. Design and implement data models to organize and structure data for analytical purposes. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability. Assist in training and support to users on business intelligence tools and applications. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions. What We Are Looking For Education: Bachelor's degree or equivalent in Computer Science, MIS, Mathematics, Statistics, or similar discipline. Master's degree or PhD preferred. Relevant work experience in data engineering based on the following number of years: Standard I: Two (2) years Standard II: Three (3) years Senior I: Four (4) years Senior II: Five (5) years Knowledge, Skills And Abilities Fluency in English Analytical Skills Accuracy & Attention to Detail Numerical Skills Planning & Organizing Skills Presentation Skills Data Modeling and Database Design ETL (Extract, Transform, Load) Skills Programming Skills FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone. All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances. Our Company FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World‚Äôs Most Admired Companies by ""Fortune"" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding. Our Philosophy The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company. Our Culture Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970‚Äôs. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today‚Äôs global marketplace.",,,,
4035452311,Data Science/ML Engineer,VPlayed,"Chennai, Tamil Nadu, India",,Full-time,,"About the job We are seeking an experienced Data Science/ML Engineer with 6-8 years of expertise in AI/ML to design and deploy machine learning models. You will work with tools like TensorFlow, PyTorch, and AWS services such as Sagemaker. Experience in developing data pipelines, deploying models using MLOps, and building visualizations is required. Key Responsibilities Comprehend business issues: Propose valuable business solutions using statistical and AI/ML models. Model Design: Design statistical models and machine learning/deep learning models to address complex business challenges. Model Deployment: Develop and deploy ML/DL models into production environments. Data Insights: Formulate available data sources, augment them, and extract valuable insights using SQL and efficient query writing techniques. Data Visualization: Create innovative data visualizations and graphs using tools like d3js, dashplotly, and neo4j. Integration: Ensure seamless integration of AI/ML models into business processes using cloud platforms like AWS or Azure. Cross-Functional Collaboration: Work closely with cross-functional teams, translating business needs into actionable AI solutions. Agile Participation: Participate in agile project delivery, ensuring the timely execution of deliverables. Required Technical Skills Python Programming: Proficient in Python programming. Statistical Knowledge: Practical knowledge of Statistics and Operations Research methods. Frameworks and Tools: Hands-on experience with frameworks and tools such as Flask, PySpark, PyTorch, TensorFlow, Keras, Databricks, OpenCV, Pillow/PIL, Streamlit, and neo4j. Analytics/AI-ML Services: Expertise in Analytics/AI-ML AWS services, including SageMaker, Canvas, Bedrock. Predictive Modeling: Strong understanding of predictive modeling techniques like regression models, XGBoost, random forests, GBM, Neural Nets, SVM, etc. NLP Techniques: Proficient in NLP techniques like RNN, LSTM, and Attention-based models, with knowledge of Stanford NLP, IBM, Azure, and OpenAI NLP models. MLOps: Experience in MLOps: deploying models into production on AWS or Azure cloud platforms. Version Control: Working knowledge of version control tools such as GitHub and Bitbucket. Soft Skills Analytical Abilities: Strong analytical and problem-solving abilities. Communication Skills: Excellent communication, listening, and probing skills. Interpersonal Skills: Strong interpersonal skills, with the ability to collaborate and work effectively within a team. Good To Have AWS Specialty Certification in Data Analytics or Machine Learning.",,,"Python, SQL, Machine Learning",
4088113604,Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About Amgen Amgen harnesses the best of biology and technology to fight the world‚Äôs toughest diseases, and make people‚Äôs lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what‚Äôs known today. About The Role Role Description: Let‚Äôs do this. Let‚Äôs change the world. In this vital role you will be responsible for designing, building, maintaining, analyzing, and interpreting data to provide actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and executing data initiatives and, visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture and ETL processes Design, develop, and maintain data solutions for data generation, collection, and processing Be a key team member that assists in design and development of the data pipeline Create data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems Contribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions Develop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency Implement data security and privacy measures to protect sensitive data Leverage cloud platforms (AWS preferred) to build scalable and efficient data solutions Collaborate and communicate effectively with product teams Collaborate with Data Architects, Business SMEs, and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions Identify and resolve complex data-related challenges Adhere to best practices for coding, testing, and designing reusable code/component Explore new tools and technologies that will help to improve ETL platform performance Participate in sprint planning meetings and provide estimations on technical implementation What We Expect Of You Master‚Äôs degree and 1 to 3 years of Computer Science, IT or related field experience OR Bachelor‚Äôs degree and 3 to 5 years of Computer Science, IT or related field experience OR Diploma and 7 to 9 years of Computer Science, IT or related field experience Basic Qualifications: Hands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL),Snowflake, workflow orchestration, performance tuning on big data processing Proficiency in data analysis tools (eg. SQL) Proficient in SQL for extracting, transforming, and analyzing complex datasets from relational data stores Experience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development Strong understanding of data modeling, data warehousing, and data integration concepts Proven ability to optimize query performance on big data platforms Preferred Qualifications: Experience with Software engineering best-practices, including but not limited to version control, infrastructure-as-code, CI/CD, and automated testing Knowledge of Python/R, Databricks, SageMaker, cloud data platforms Strong understanding of data governance frameworks, tools, and best practices. Knowledge of data protection regulations and compliance requirements (e.g., GDPR, CCPA) Professional Certifications: AWS Certified Data Engineer preferred Databricks Certificate preferred Soft Skills: Excellent critical-thinking and problem-solving skills Strong communication and collaboration skills Demonstrated awareness of how to function in a team setting Demonstrated presentation skills EQUAL OPPORTUNITY STATEMENT Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.",Associate,,"Python, SQL, R, Machine Learning, Data Analysis",
4148215137,Data Engineer,Komodo Health,India (Remote),Remote,Full-time,,"About the job We Breathe Life Into Data At Komodo Health, our mission is to reduce the global burden of disease. And we believe that smarter use of data is essential to this mission. That‚Äôs why we built the Healthcare Map ‚Äî the industry‚Äôs largest, most complete, precise view of the U.S. healthcare system ‚Äî by combining de-identified, real-world patient data with innovative algorithms and decades of clinical experience. The Healthcare Map serves as our foundation for a powerful suite of software applications, helping us answer healthcare‚Äôs most complex questions for our partners. Across the healthcare ecosystem, we‚Äôre helping our clients unlock critical insights to track detailed patient behaviors and treatment patterns, identify gaps in care, address unmet patient needs, and reduce the global burden of disease. As we pursue these goals, it remains essential to us that we stay grounded in our values: be awesome, seek growth, deliver ‚Äúwow,‚Äù and enjoy the ride. At Komodo, you will be joining a team of ambitious, supportive Dragons with diverse backgrounds but a shared passion to deliver on our mission to reduce the burden of disease ‚Äî and enjoy the journey along the way. The Opportunity at Komodo Health Komodo Health leverages the latest data engineering technology such as Spark, Airflow, and Snowflake to tackle some of healthcare‚Äôs biggest challenges by transforming extraordinary amounts of data into rich and meaningful insights. As a Data Engineer on Komodo Health‚Äôs team, you will be solving complex data challenges while helping build and scale our data platform that powers state-of-the-art interactive product experiences. You will enable smarter, more innovative uses of healthcare datasets by designing robust data pipelines and implementing data best practices. This role will collaborate with Product Managers, Customer Success and Sales to understand data requirements and to develop the set of necessary data processing steps for creating said data products and insights. This role will be a key contributor to the scalability of the data systems across our suite of product offerings, including the design and implementation of data pipelines. This person will be a part of a team delivering data products and insights to our customers either via our suite of product applications or any other delivery mechanisms. Looking back on your first 12 months at Komodo Health, you will have‚Ä¶ Gained an understanding of the broader Komodo Health data landscape, Sentinel and MapLab platform. Built and improved the foundational pieces of Sentinel data infrastructure, pipelines, and data services Led and implemented new data product offerings on Sentinel platform with complex business requirements across different stakeholders including engineering, product and customers. Designed and implemented data and system migrations for existing customers to new platform Ensured non-functional requirements are met, such as costs, developer experience, reliability, maintainability and operations/support. You Will Accomplish These Outcomes Through The Following Responsibilities‚Ä¶ Partnering with Engineering team members, Product Managers, and customer-facing teams to understand complex health data use cases and business logic Being curious about our data Building foundational pieces of our data platform architecture, pipelines, analytics, and services underlying our platform Designing and developing reliable data pipelines that transform data at scale, orchestrated jobs via Airflow/Temporal, using SQL and Python in Snowflake Contributing to python packages in Github and APIs, using current best practices What You Bring To Komodo Health Demonstrated proficiency in designing and developing with distributed data processing platforms like Spark and pipeline orchestration tools like Airflow and/or Temporal Experience with modern data warehouses such as Snowflake; Experience with SQL and query design on large, complex datasets Solid computer science skills and proficiency in programming languages like Python. Able to leverage industry standard engineering best practices, like design patterns and/or testing Capable of quickly building expertise on an as-need basis on a new tech stack Experience with product engineering software development, in an agile environment Demonstrated track-record of delivering products and features with varying degrees of complexity, and through several iterations of product development Understand and design for non-functional concerns such as performance, cost optimization, maintainability and developer experience A thirst for knowledge, willingness to learn, and a growth-oriented mindset Excellent cross-team communication and collaboration skills Additional skills and experience we‚Äôd prioritize (nice to have)‚Ä¶ Experience in building containerized API services to serve both internal and external clients Experience enhancing CI/CD build tooling in a containerized environment, from deployment pipelines (Jenkins, etc), infrastructure as code (Terraform, Cloudformation), and configuration management via Docker and Kubernetes US health care data experience is not required but it is a strong plus Where You‚Äôll Work Komodo Health has a hybrid work model ; we recognize the power of choice and importance of flexibility for the well-being of both our company and our individual Dragons. Roles may be completely remote based anywhere in the country listed, remote but based in a specific region, or local (commuting distance) to one of our hubs in San Francisco, New York City, or Chicago with remote work options. What We Offer Positions may be eligible for company benefits in accordance with Company policy. We offer a competitive total rewards package including medical, dental and vision coverage along with a broad range of supplemental benefits including 401k Retirement Plan, prepaid legal assistance, and more. We also offer paid time off for vacation, sickness, holiday, and bereavement. We are pleased to be able to provide 100% company-paid life insurance and long-term disability insurance. This information is intended to be a general overview and may be modified by the Company due to business-related factors. Equal Opportunity Statement Komodo Health provides equal employment opportunities to all applicants and employees. We prohibit discrimination and harassment of any type with regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.",Manager,,"Python, SQL",
4242004491,Data Engineer I,FedEx ACC,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job We are seeking an experienced IBM ITX Developer with a strong background in IBM Integration Tool (ITX) to join our team. The ideal candidate will have hands-on expertise in developing, implementing, and debugging ITX maps, along with managing EDI transactions in the logistics and supply chain domain. Primary Skills IBM ITX (WebSphere Transformation Extender) Development: Proficiency in designing, developing, and debugging ITX maps. Data Formats: Experience with various data formats, including ANSI X12, EDIFACT, XML, JSON, FF, and CSV. Experience or strong knowledge of communication and data handling protocols (AS2, EDI, HTTPS, FTPS/SFTP/SCP/OFTP, POP3/SMTP, Web Services, etc.) Communication Protocols: Experience with communication protocols like AS2, SFTP, and others relevant to logistics. Technical Problem-Solving: Ability to identify, analyze, and resolve technical issues independently. Teamwork and Collaboration: Strong interpersonal skills and the ability to work effectively in a team environment. Education: Bachelors‚Äô degree or equivalent in Computer Science, MIS, or similar discipline. Accreditation: Specific business accreditation for Business Intelligence. Experience: Relevant work experience in data engineering based on the following number of years: Associate: Prior experience not required Standard I: Two (2) years Standard II: Three (3) years Senior I: Four (4) years Senior II: Five (5) years Knowledge, Skills And Abilities Fluency in English Analytical Skills Accuracy & Attention to Detail Numerical Skills Planning & Organizing Skills Presentation Skills Preferred Qualifications Pay Transparency: Pay Additional Details: FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone. All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances. Our Company FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World‚Äôs Most Admired Companies by ""Fortune"" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding. Our Philosophy The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company. Our Culture Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970‚Äôs. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today‚Äôs global marketplace.",Associate,,,
4238832516,Data Engineer,PINKERTON,"Gurgaon, Haryana, India (On-site)",On-site,Full-time,"Leader. Global Impact. At Pinkerton, the mission is to protect our clients. To do this, we provide enterprise risk management services and programs specifically designed for each client. Pinkerton employees are one of our most important assets and critical to the delivery of world-class solutions. Bonded together, we share a commitment to integrity, vigilance, and excellence. Pinkerton is an inclusive employer who seeks candidates with diverse backgrounds, experiences, and perspectives to join our family of industry subject matter experts. The Data Engineer will be part of a high-performing and international team with the goal to expand Data & Analytics solutions for our CRM application which is live in all Securitas countries. Together with the dedicated Frontend- & BI Developer you will be responsible for managing and maintaining the Databricks based BI Platform including the processes from data model changes, implementation and development of pipelines are part of the daily focus, but ETL will get most of your attention. Continuous development to do better will need the ability to think bigger and work closely with the whole team. The Data Engineer (ETL Specialist) will collaborate with the Frontend- & BI Developer to align on possibilities to improve the BI Platform deliverables specifically for the CEP organization. Cooperation with other departments such as integrations or specific IT/IS projects and business specialists is part of the job. The expectation is to always take data privacy into consideration when talking about moving data or sharing data with others. For that purpose, there is a need to develop the security layer as agreed with the legal department. Represent Pinkerton‚Äôs core values of integrity, vigilance, and excellence. Maintain & Develop the Databricks workspace used to host the BI CEP solution Active in advising needed changes the data model to accommodate new BI requirements Develop and implement new ETL scripts and improve the current ones Ownership on resolving the incoming tickets for both incidents and requests Plan activities to stay close to the Frontend- & BI Developer to foresee coming changes to the backend Through working with different team members improve the teamwork using the DevOps tool to keep track of the status of the deliverable from start to end Ensure understanding and visible implementation of the company‚Äôs core values of Integrity, Vigilance and Helpfulness. Knowledge about skills and experience available and required in your area today and tomorrow to drive liaison with other departments if needed. All other duties, as assigned. At least 3+ years of experience in Data Engineering Understanding of designing and implementing data processing architectures in Azure environments Experience with different SSAS - modelling techniques (preferable Azure, databricks - Microsoft related) Understanding of data management and - treatment to secure data governance & security (Platform management and administration) An analytical mindset with clear communication and problem-solving skills Experience in working with SCRUM set up Fluent in English both spoken and written Bonus knowledge of additional language(s) Ability to communicate, present and influence credibly at all levels both internally and externally Business Acumen & Commercial Awareness Working Conditions With or without reasonable accommodation, requires the physical and mental capacity to effectively perform all essential functions; Regular computer usage. Occasional reaching and lifting of small objects and operating office equipment. Frequent sitting, standing, and/or walking. Travel, as required. Pinkerton is an equal opportunity employer to all applicants and positions without regard to race/ethnicity, color, national origin, ancestry, sex/gender, gender identity/expression, sexual orientation, marital/prenatal status, pregnancy/childbirth or related conditions, religion, creed, age, disability, genetic information, veteran status, or any protected status by local, state, federal or country-specific law. Job search faster with Premium Access company insights like strategic priorities, headcount trends, and more Sivaraman and millions of other members use Premium Retry Premium for ‚Çπ0 1-month free trial. Cancel whenever. We‚Äôll remind you 7 days before your trial ends. About the company PINKERTON 1,654 followers Follow Security and Investigations 501-1,000 employees 112 on LinkedIn Protecting your Personal. In today's demanding business ambiance, there is an unprecedented market of espionage and Pilferage, in order to gain disloyal advantage. The competitors may use any weakness of your security to their advantage. Our organization will make you conquer your insecurity by our security professionals who work round the clock towards the common objective of protecting your valuable assets and property. Thank you for your interest in the extensive range of available security offered by our organization. We have the pleasure to invite you to the varied services offered by our organization. Our organization provides the highest quality of security services with a solitary focus on security in view of the fact that there is a great demand for a professional security organization with governmental standard in the commercial market and we are proud to avow that we meet those standards. Our organization services are coupled with a corporate wide desire for perfection and engender a customer service second to none. ‚Ä¶ show more Show more","About the job 170+ Years Strong. Industry Leader. Global Impact. At Pinkerton, the mission is to protect our clients. To do this, we provide enterprise risk management services and programs specifically designed for each client. Pinkerton employees are one of our most important assets and critical to the delivery of world-class solutions. Bonded together, we share a commitment to integrity, vigilance, and excellence. Pinkerton is an inclusive employer who seeks candidates with diverse backgrounds, experiences, and perspectives to join our family of industry subject matter experts. The Data Engineer will be part of a high-performing and international team with the goal to expand Data & Analytics solutions for our CRM application which is live in all Securitas countries. Together with the dedicated Frontend- & BI Developer you will be responsible for managing and maintaining the Databricks based BI Platform including the processes from data model changes, implementation and development of pipelines are part of the daily focus, but ETL will get most of your attention. Continuous development to do better will need the ability to think bigger and work closely with the whole team. The Data Engineer (ETL Specialist) will collaborate with the Frontend- & BI Developer to align on possibilities to improve the BI Platform deliverables specifically for the CEP organization. Cooperation with other departments such as integrations or specific IT/IS projects and business specialists is part of the job. The expectation is to always take data privacy into consideration when talking about moving data or sharing data with others. For that purpose, there is a need to develop the security layer as agreed with the legal department. Represent Pinkerton‚Äôs core values of integrity, vigilance, and excellence. Maintain & Develop the Databricks workspace used to host the BI CEP solution Active in advising needed changes the data model to accommodate new BI requirements Develop and implement new ETL scripts and improve the current ones Ownership on resolving the incoming tickets for both incidents and requests Plan activities to stay close to the Frontend- & BI Developer to foresee coming changes to the backend Through working with different team members improve the teamwork using the DevOps tool to keep track of the status of the deliverable from start to end Ensure understanding and visible implementation of the company‚Äôs core values of Integrity, Vigilance and Helpfulness. Knowledge about skills and experience available and required in your area today and tomorrow to drive liaison with other departments if needed. All other duties, as assigned. At least 3+ years of experience in Data Engineering Understanding of designing and implementing data processing architectures in Azure environments Experience with different SSAS - modelling techniques (preferable Azure, databricks - Microsoft related) Understanding of data management and - treatment to secure data governance & security (Platform management and administration) An analytical mindset with clear communication and problem-solving skills Experience in working with SCRUM set up Fluent in English both spoken and written Bonus knowledge of additional language(s) Ability to communicate, present and influence credibly at all levels both internally and externally Business Acumen & Commercial Awareness Working Conditions With or without reasonable accommodation, requires the physical and mental capacity to effectively perform all essential functions; Regular computer usage. Occasional reaching and lifting of small objects and operating office equipment. Frequent sitting, standing, and/or walking. Travel, as required. Pinkerton is an equal opportunity employer to all applicants and positions without regard to race/ethnicity, color, national origin, ancestry, sex/gender, gender identity/expression, sexual orientation, marital/prenatal status, pregnancy/childbirth or related conditions, religion, creed, age, disability, genetic information, veteran status, or any protected status by local, state, federal or country-specific law.",,,,
4228900901,Lead Data Engineer,MakeMyTrip,"Bangalore Urban, Karnataka, India (On-site)",On-site,Full-time,"Preference: E-Commerce Job search faster with Premium Access company insights like strategic priorities, headcount trends, and more Sivaraman and millions of other members use Premium Retry Premium for ‚Çπ0 1-month free trial. Cancel whenever. We‚Äôll remind you 7 days before your trial ends. About the company MakeMyTrip 897,987 followers Follow Technology, Information and Internet 1,001-5,000 employees 6,810 on LinkedIn With our three powerhouse brands‚ÄîMakeMyTrip, Goibibo, and Redbus‚Äîwe are proud pioneers of online travel in India. We empower millions of travelers with easy and instant travel solutions, offering a wide range of services, including flights, hotels, homestays, holiday packages, cabs, buses, and trains. Our recent expansions include successful corporate travel solutions, a travel suite for agents, and our entry into the Gulf market with tailored flight and hotel offerings. Launched in 2005 as India‚Äôs first online travel portal, MakeMyTrip revolutionized travel planning with user-friendly platforms and value-driven pricing. This innovation led to our NASDAQ listing in 2010, making us the first Indian travel company to achieve this milestone. Our commitment to excellence extends to our people, earning us recognition as one of India‚Äôs ‚ÄòTop 10 Best Companies to Work For‚Äô for four consecutive years (2010-2013). In 2016, we were named the Best E-commerce Company by the Great Place to Work Institute. Today, with millions of online visitors and an offline presence of 60+ stores, MakeMyTrip continues to redefine travel by connecting people and places, one journey at a time. ‚Ä¶ show more Interested in working with us in the future? Privately share your profile with our recruiters ‚Äì you‚Äôll be noted as expressing interest for up to a year and be notified about jobs and updates. Learn more Learn more about Interested in working for our company I‚Äôm interested Company photos Page 1 of 5 Previous Next May 30, 2017 May 30, 2017 June 20, 2017 Show more","About the job We at MakeMyTrip understand that every traveller is unique and being the leading OTA in India we have the leverage to redefine the travel booking experience to meet their need. If you love to travel and want to be a part of a dynamic team that works on personalizing every user's journey, then look no further. We are looking for a brilliant mind like yours to join our Data Platform team to build exciting data products at a scale where we solve for industry best and fault-tolerant feature stores, real-time data pipelines, catalogs, and much more. Hands-on: Spark, Scala Technologies: Spark, Aerospike, DataBricks, Kafka, Debezium, EMR, Athena, Glue, RocksDB, Redis, Airflow, MySQL, and any other data sources (e.g. Mongo, Neo4J, etc) used by other teams. Location: Gurgaon/Bengaluru Experience: 6+ years Industry Preference: E-Commerce",,,,
4250883077,Python Developer Intern,Unified Mentor Private Limited,India (Remote),Remote,Internship,,"About the job Job Title: Python Developer Intern Company: Unified Mentor Location: Remote Duration: 3 months Opportunity: Full-time based on performance, with a Certificate of Internship Application Deadline: 18th June 2025 About Unified Mentor Unified Mentor provides students and graduates with hands-on experience, professional training, and career-building opportunities in software development , helping you enhance your coding skills and prepare for a successful career. Role Overview As a Python Developer Intern , you will work on real-world projects , enhance your coding skills, and gain practical experience in software development . Responsibilities ‚úÖ Develop, test, and debug Python applications. ‚úÖ Collaborate on software projects and API integrations. ‚úÖ Learn and apply Python frameworks like Django/Flask . Qualifications üéì Enrolled in or completed a Computer Science or related program. üêç Proficient in Python programming . üåê Familiarity with web frameworks (Django/Flask). üß© Strong problem-solving and time-management skills . Perks üí∞ Stipend: ‚Çπ7,500 - ‚Çπ15,000 (Performance-Based) (Paid) ‚úî Certificate of Internship & Letter of Recommendation . ‚úî Hands-on experience with real-world projects for your portfolio . How to Apply üì© Submit your application with the subject: ""Python Developer Intern Application"" . Equal Opportunity Unified Mentor is an equal opportunity employer , welcoming applicants from all backgrounds.",,,Python,"‚Çπ7,500 - ‚Çπ15,000"
4247307211,Data Engineer,Mphasis,"Chennai, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job Job Description: Lead Data Engineer Experience Level: 7-10 years WFO-Chennai Required Skills: AI /ML-skill mandatory ¬∑ AWS Glue, EMR (Spark), Redshift, Step Functions: Deep experience in using AWS tools for data processing and pipeline orchestration. ¬∑ Python, SQL, PySpark/Scala: Advanced skills in Python and SQL, as well as experience in Spark-based data processing frameworks. ¬∑ Data Pipeline Performance: Expertise in tuning and optimizing data pipelines for performance and scalability. ¬∑ Monitoring & Troubleshooting: Experience with monitoring data systems and troubleshooting performance bottlenecks. Job Description (JD): The Lead Data Engineer will oversee the design, development, and optimization of data pipelines that serve as the backbone for data processing and analytics. The candidate should have a strong command of cloud-based data engineering tools and the ability to lead teams in building scalable, high-performance data systems. You will collaborate closely with data scientists, analysts, and product teams to deliver reliable and efficient data architectures on AWS, ensuring they meet both current and future business needs. Roles and Responsibilities: ¬∑ Lead Data Pipeline Development: Architect and develop scalable, secure, and optimized data pipelines using AWS Glue, Redshift, EMR (Spark), and Step Functions. ¬∑ Data Performance Tuning: Optimize data pipelines for performance, ensuring minimal latency and high throughput. ¬∑ Collaborate with Stakeholders: Work with data scientists, business analysts, and other engineers to understand requirements and deliver effective solutions. ¬∑ Data Storage & Management: Ensure efficient management of data storage in Redshift, Glue, and other AWS services. ¬∑ Team Leadership & Mentorship: Guide and mentor a team of data engineers, ensuring best practices for data engineering are followed. ¬∑ System Monitoring & Troubleshooting: Set up monitoring for all data pipelines and perform proactive troubleshooting to minimize downtime. ¬∑ Continuous Improvement: Stay up-to-date with emerging technologies and improve existing data pipelines to enhance performance and scalability. Qualifications: ¬∑ Bachelor‚Äôs degree in Computer Science, Engineering, or a related field. ¬∑ 7+ years of experience in data engineering with expertise in AWS technologies. ¬∑ Advanced skills in Python, SQL, and Spark. ¬∑ Solid understanding of data engineering principles, including ETL processes, performance optimization, and scalability. ¬∑ Experience in leading teams and mentoring junior engineers. ¬∑ Excellent communication skills and ability to collaborate with cross-functional teams. Preferred Skills: ¬∑ Experience with containerization (Docker, Kubernetes). ¬∑ Familiarity with Apache Kafka, Kinesis, or other streaming technologies. ¬∑ Knowledge of machine learning frameworks and their integration into data pipelines. ¬∑ Familiarity with infrastructure as code tools such as Terraform. Skills PRIMARY COMPETENCY : Data Engineering PRIMARY SKILL : DBA / Data Modeling / Data Engineering PRIMARY SKILL PERCENTAGE : 100",,,"Python, SQL, Machine Learning",
4239890610,Data Engineer l,Barracuda,"Bengaluru East, Karnataka, India (On-site)",On-site,Full-time,,"About the job Job ID 26-107 Come join our passionate team! Barracuda is a leading cybersecurity company providing complete protection against complex threats. Our platform protects email, data, applications, and networks with innovative solutions, and a managed XDR service, to strengthen cyber resilience. Hundreds of thousands of IT professionals and managed service providers worldwide trust us to protect and support them with solutions that are easy to buy, deploy, and use. We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an employer that complies with all applicable national, state and local laws pertaining to nondiscrimination and equal opportunity regardless of race, gender, religion, sex, sexual orientation, national origin, or disability. Envision yourself at Barracuda As a Data Engineer l at Barracuda, you will be part of a dynamic team responsible for the enterprise data and data governance. You will assist in sourcing, modeling, and warehousing data from various sources, including telemetry and usage data from over a dozen products, CRM systems like Salesforce, ERP systems like NetSuite, and various marketing systems and services. Your work will support data activation, BI, and AI use cases across departments such as finance, marketing, and customer success, positively impacting the top line of the company. Tech Stack SQL, Python, PySpark Databricks AWS What You‚Äôll Be Working On Assist in the development and maintenance of ETL pipelines to reliably ingest, cleanse, transform and store data. Support the implementation of data models to organize data logically and efficiently for various analytics and ML use cases. Implement data quality checks and enforce data governance practices to ensure data integrity and reliability. Assist in building reverse ETL pipelines to activate data in external systems. Implement data infrastructure using infrastructure-as-code. What You Bring To The Role Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Information Technology, or related field 3+ years of industry experience as a Data Engineer working on large data sets. Strong programming skills in Python. Excellent knowledge of SQL. Experience with public cloud, preferably AWS and services such as Redshift, S3, ECS, EC, Kinesis etc. Excellent communication skills and the ability to work collaboratively with cross-functional teams. What You‚Äôll Get From Us A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility ‚Äì there are opportunities for cross training and the ability to attain your next career step within Barracuda. In addition, you will receive equity, in the form of non-qualifying options.",,,"Python, SQL",
4249420738,Data Engineer-1,Zelis,"Hyderabad, Telangana, India (Hybrid)",Hybrid,Full-time,,"About the job About Us Zelis is modernizing the healthcare financial experience in the United States (U.S.) by providing a connected platform that bridges the gaps and aligns interests across payers, providers, and healthcare consumers. This platform serves more than 750 payers, including the top 5 health plans, BCBS insurers, regional health plans, TPAs and self-insured employers, and millions of healthcare providers and consumers in the U.S. Zelis sees across the system to identify, optimize, and solve problems holistically with technology built by healthcare experts‚Äîdriving real, measurable results for clients. Why We Do What We Do In the U.S., consumers, payers, and providers face significant challenges throughout the healthcare financial journey. Zelis helps streamline the process by offering solutions that improve transparency, efficiency, and communication among all parties involved. By addressing the obstacles that patients face in accessing care, navigating the intricacies of insurance claims, and the logistical challenges healthcare providers encounter with processing payments, Zelis aims to create a more seamless and effective healthcare financial system. Zelis India plays a crucial role in this mission by supporting various initiatives that enhance the healthcare financial experience. The local team contributes to the development and implementation of innovative solutions, ensuring that technology and processes are optimized for efficiency and effectiveness. Beyond operational expertise, Zelis India cultivates a collaborative work culture, leadership development, and global exposure, creating a dynamic environment for professional growth. With hybrid work flexibility, comprehensive healthcare benefits, financial wellness programs, and cultural celebrations, we foster a‚ÄØholistic workplace experience. Additionally, the team plays a vital role in maintaining high standards of service delivery and contributes to Zelis‚Äô award-winning culture. Summary Position Overview Responsible for operating and enhancing the performance and functionality of existing data management systems, as well as participating in the design and delivery of new database solutions. Essential Duties & Functions Manages data activities such as data requirements gathering, data analysis/modeling, and data issues resolution using standard approved technology Manages standardization, migration, transformation, validation, and quality assurance of data within multi-database platforms Creates and maintains code through github repository for change control Builds Jenkins pipelines using groovy Leverages internal and external ETL tools for data processing and publishing Identifies and maintains company databases, including data sources, data structures, data organization, and data optimization Identifies complex issues proactively and is responsible to see them through resolution, including identifying trends through data analysis and manipulation Responsible for specific client data life-cycles from discovery to implementation to maintenance Formulates and monitors policies, procedures, and standards relating to database management Responds to production defects and relays information back to the Operations Manager to communicate to clients Contribute in all phases of the data and software development lifecycle when needed Ability to support off hours data processing and emergency requests Experience, Qualifications, Knowledge And Skills Bachelor's degree (B. A. / B. S.) from four-year college or university; and two to four years related experience and/or training; or equivalent combination of education and experience. 2+ years Healthcare industry experience preferred 3+ years of experience with SQL, database design, optimization, and tuning 3+ years of experience with open source relational (e.g. Postgresql) 3+ years of experience using Github 3+ years of experience in Shell Scripting and one other object oriented language such as Python, or PhP. 3+ years of experience in continuous integration and development methodologies tools such as Jenkins 3+ years of experience in an Agile development environment Time management skills Professionalism Programming skills particularly SQL, Shell Scripting, and Python Detail oriented Conscientious Team player Oral and written communication skills",Manager,,"Python, SQL, Data Analysis",
4247590104,Data Engineer,Accenture in India,"Chennai, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job Project Role : Data Engineer Project Role Description : Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems. Must have skills : Databricks Unified Data Analytics Platform Good to have skills : Business Agility Minimum 3 Year(s) Of Experience Is Required Educational Qualification : 15 years full time education Summary: As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders. Roles & Responsibilities: - Need Databricks resource with Azure cloud experience - Expected to perform independently and become an SME. - Required active participation/contribution in team discussions. - Contribute in providing solutions to work related problems. - Collaborate with data architects and analysts to design scalable data solutions. - Implement best practices for data governance and security throughout the data lifecycle. Professional & Technical Skills: - Must To Have Skills: Proficiency in Databricks Unified Data Analytics Platform. - Good To Have Skills: Experience with Business Agility. - Strong understanding of data modeling and database design principles. - Experience with data integration tools and ETL processes. - Familiarity with cloud platforms and services related to data storage and processing. Additional Information: - The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform. - This position is based at our Pune office. - A 15 years full time education is required. 15 years full time education",,,,
4247467306,Data Engineer II,Uplers,Greater Bengaluru Area (Hybrid),Hybrid,Full-time,"experience as a Big Data Engineer and working along cross functional teams such as Software Engineering, Analytics, Data Science with a track record of manipulating, processing, and extracting value from large datasets. Strong business acumen. Experience leading large-scale data warehousing and analytics projects, including using GCP technologies ‚Äì Big Query, Dataproc, GCS, Cloud Composer, Dataflow or related big data technologies in other cloud platforms like AWS, Azure etc. Be a team player and introduce/follow the best practices on the data engineering space. Ability to effectively communicate (both written and verbally) technical information and the results of engineering design at all levels of the organization. Good to have : Understanding of NoSQL Database exposure and Pub-Sub architecture setup. Familiarity with Bl tools like Looker, Tableau, AtScale, PowerBI, or any similar tools. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Engineering, Big Data Technologies, Hadoop, Spark, Hive, Presto, Airflow, Data Modeling, Etl development, Data Lake Architecture, Python, Scala, GCP Big Query, Dataproc, Dataflow, Cloud Composer, AWS, Big Data Stack, Azure, GCP Job search faster with Premium Access company insights like strategic priorities, headcount trends, and more Sivaraman and millions of other members use Premium Retry Premium for ‚Çπ0 1-month free trial. Cancel whenever. We‚Äôll remind you 7 days before your trial ends. About the company Uplers 1,150,786 followers Follow Technology, Information and Internet 1,001-5,000 employees 1,059 on LinkedIn Uplers is the #1 hiring platform for tech companies, designed to help you hire top product and engineering talent quickly and efficiently. Our end-to-end AI-powered platform combines artificial intelligence with human expertise to connect you with the best engineering talent from India. With over 1M deeply vetted professionals, Uplers streamlines the hiring process, reducing lengthy screening times and ensuring you find the perfect fit. Companies like GitLab, Twilio, TripAdvisor, and Airbnb trust Uplers to scale their tech and digital teams effectively and cost-efficiently. Experience a simpler, faster, and more reliable hiring process with Uplers today. ‚Ä¶ show more Interested in working with us in the future? Privately share your profile with our recruiters ‚Äì you‚Äôll be noted as expressing interest for up to a year and be notified about jobs and updates. Learn more Learn more about Interested in working for our company I‚Äôm interested Show more","About the job Experience : 3.00 + years Salary : Confidential (based on experience) Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Hybrid (Bengaluru) Placement Type : Full time Permanent Position (*Note: This is a requirement for one of Uplers' client - Wayfair) What do you need for this opportunity? Must have skills required: Data Engineering, Big Data Technologies, Hadoop, Spark, Hive, Presto, Airflow, Data Modeling, Etl development, Data Lake Architecture, Python, Scala, GCP Big Query, Dataproc, Dataflow, Cloud Composer, AWS, Big Data Stack, Azure, GCP Wayfair is Looking for: About The Job The Data Engineering team within the SMART org supports development of large-scale data pipelines for machine learning and analytical solutions related to unstructured and structured data. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct impact on all applications that our millions of customers interact with every day: search results, homepage content, emails, auto-complete searches, browse pages and product carousels and build and scale data platforms that enable to measure the effectiveness of wayfair‚Äôs ad-costs , media attribution that helps to decide on day to day or major marketing spends. About the Role: As a Data Engineer, you will be part of the Data Engineering team with this role being inherently multi-functional, and the ideal candidate will work with Data Scientist, Analysts, Application teams across the company, as well as all other Data Engineering squads at Wayfair. We are looking for someone with a love for data, understanding requirements clearly and the ability to iterate quickly. Successful candidates will have strong engineering skills and communication and a belief that data-driven processes lead to phenomenal products. What you'll do: Build and launch data pipelines, and data products focussed on SMART Org. Helping teams push the boundaries of insights, creating new product features using data, and powering machine learning models. Build cross-functional relationships to understand data needs, build key metrics and standardize their usage across the organization. Utilize current and leading edge technologies in software engineering, big data, streaming, and cloud infrastructure What You'll Need: Bachelor/Master degree in Computer Science or related technical subject area or equivalent combination of education and experience 3+ years relevant work experience in the Data Engineering field with web scale data sets. Demonstrated strength in data modeling, ETL development and data lake architecture. Data Warehousing Experience with Big Data Technologies (Hadoop, Spark, Hive, Presto, Airflow etc.). Coding proficiency in at least one modern programming language (Python, Scala, etc) Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing and query performance tuning skills of large data sets. Industry experience as a Big Data Engineer and working along cross functional teams such as Software Engineering, Analytics, Data Science with a track record of manipulating, processing, and extracting value from large datasets. Strong business acumen. Experience leading large-scale data warehousing and analytics projects, including using GCP technologies ‚Äì Big Query, Dataproc, GCS, Cloud Composer, Dataflow or related big data technologies in other cloud platforms like AWS, Azure etc. Be a team player and introduce/follow the best practices on the data engineering space. Ability to effectively communicate (both written and verbally) technical information and the results of engineering design at all levels of the organization. Good to have : Understanding of NoSQL Database exposure and Pub-Sub architecture setup. Familiarity with Bl tools like Looker, Tableau, AtScale, PowerBI, or any similar tools. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Engineering, Big Data Technologies, Hadoop, Spark, Hive, Presto, Airflow, Data Modeling, Etl development, Data Lake Architecture, Python, Scala, GCP Big Query, Dataproc, Dataflow, Cloud Composer, AWS, Big Data Stack, Azure, GCP",,,"Python, Tableau, Machine Learning",
4247452398,Data Engineer [T500-18490],ANSR,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job About Albertsons Companies India: At Albertsons Companies India, we're not just pushing the boundaries of technology and retail innovation, we're cultivating a space where ideas flourish and careers thrive. Our workplace in India is a vital extension of the Albertsons Companies Inc. workforce and important to the next phase in the company‚Äôs technology journey to support millions of customers‚Äô lives every day. At the Albertsons Companies India, we are raising the bar to grow across Technology & Engineering, AI, Digital and other company functions, and transform a 165-year-old American retailer. At Albertsons Companies India associates collaborate directly with international teams, enhancing decision-making processes and organizational agility through exciting and pivotal projects. Your work will make history and help millions of lives each day come together around the joys of food and inspire their well-being. Position Title: Engineer Data The Data Engineering team at Albertsons Companies is looking for an experienced Engineer Data to work for the most transformational food and drug retailers in the United States. Albertsons operates over 2,300 stores under 19 well-known banners including Albertsons, Safeway, Vons, Jewel-Osco, Shaw's, Acme, Tom Thumb, Randalls, United Supermarkets, Pavilions, Star Market, Haggen and Carrs. The company reported revenue of over $60billion from over 34 million weekly shoppers and is the third largest private company in the country. Position Purpose: We are seeking a highly skilled and motivated Engineer Data to join our dynamic and growing data team. This role is ideal for someone who thrives in a fast-paced, cloud-native environment and is passionate about building scalable, efficient data solutions. You will play a key role in designing, developing, and maintaining our data infrastructure, with a strong focus on Google BigQuery and other Google Cloud Platform (GCP) services. Key Responsibilities: Design, develop, and optimize BigQuery stored procedures and complex SQL queries. Build and maintain robust ETL pipelines using Python and Google Cloud Dataflow. Manage data ingestion and transformation workflows using Google Cloud Composer or Stonebranch. Utilize Google Cloud Storage (GCS) for data staging and archival. Implement and maintain scalable data models and data warehousing solutions. Collaborate with cross-functional teams using Agile methodologies (Epics, Stories, Standups, Retrospectives). Document data processes and flows using Confluence. Track and manage tasks using Jira. Required Qualifications: Advanced proficiency in SQL Experience with Python (or another scripting language) and automation. Hands-on expertise with GCP services: BigQuery, GCS, Dataflow, and Cloud Composer Strong understanding of data modeling, ETL processes, and data warehousing concepts. Experience with streaming systems such as Kafka. Familiarity with version control systems like Git. Proven experience working in Agile environments. Nice to Have: Experience with Power BI or ThoughtSpot for data visualization and analytics. Exposure to other cloud platforms, especially Microsoft Azure. Awareness of Snowflake, Oracle, and DB2. Expertise in AI, preferably in the context of data engineering. Experience with Stonebranch for job scheduling. Knowledge of CI/CD pipelines and DevOps practices.",associate,,"Python, SQL, Power BI",
4247426083,Data Engineer,AXA XL,"Bengaluru East, Karnataka, India",,Full-time,,"About the job Senior Engineer, Data Modeling Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities. This data should not only be high quality, but also actionable - enabling AXA XL‚Äôs executive leadership team to maximize benefits and facilitate sustained industrious advantage. Our Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market. As we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking a Data Engineer. The role will support the team‚Äôs efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines. This role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner. What You‚Äôll Be Doing What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate. Understand current and future data consumption patterns, architecture (granular level), partner with Architects to make sure optimal design of data layers. Apply best practices in Data architecture. For example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning. Leading and hands-on execution of research into new technologies. Formulating frameworks for assessment of new technology vs business benefit, implications for data consumers. Act as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform. Design prototypes and work in a fast-paced iterative solution delivery model. Design, Develop and maintain ETL pipelines using Py spark in Azure Databricks using delta tables. Use Harness for deployment pipeline. Monitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed. Diagnose system performance issue related to data processing and implement solution to address them. Collaborate with other teams to make sure successful integration of data pipelines into larger system architecture requirement. Maintain integrity and quality across all pipelines and environments. Understand and follow secure coding practice to make sure code is not vulnerable. You will report to the Application Manager. What You Will BRING We‚Äôre looking for someone who has these abilities and skills: Required Skills And Abilities Effective Communication skills. Bachelor‚Äôs degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience. Relevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc.), application development, advanced data querying skills. Relevant years of programming experience using Databricks. Relevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS). Solid knowledge on network and firewall concepts. Solid experience writing, optimizing and analyzing SQL. Relevant years of experience with Python. Ability to break complex data requirements and architect solutions into achievable targets. Robust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile. Experience using Harness. Technical lead responsible for both individual and team deliveries. Desired Skills And Abilities Worked in big data migration projects. Worked on performance tuning both at database and big data platforms. Ability to interpret complex data requirements and architect solutions. Distinctive problem-solving and analytical skills combined with robust business acumen. Excellent basics on parquet files and delta files. Effective Knowledge of Azure cloud computing platform. Familiarity with Reporting software - Power BI is a plus. Familiarity with DBT is a plus. Passion for data and experience working within a data-driven organization. You care about what you do, and what we do. Who WE Are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks. For mid-sized companies, multinationals and even some inspirational individuals we don‚Äôt just provide re/insurance, we reinvent it. How? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business ‚àí property, casualty, professional, financial lines and specialty. With an innovative and flexible approach to risk solutions, we partner with those who move the world forward. Learn more at axaxl.com What We OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic. At AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success. That‚Äôs why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential. It‚Äôs about helping one another ‚Äî and our business ‚Äî to move forward and succeed. Five Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl.com/about-us/inclusion-and-diversity . AXA XL is an Equal Opportunity Employer. Total Rewards AXA XL‚Äôs Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security. It provides dynamic compensation and personalized, inclusive benefits that evolve as you do. We‚Äôre committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence. Sustainability At AXA XL, Sustainability is integral to our business strategy. In an ever-changing world, AXA XL protects what matters most for our clients and communities. We know that sustainability is at the root of a more resilient future. Our 2023-26 Sustainability strategy, called ‚ÄúRoots of resilience‚Äù, focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations. Our Pillars Valuing nature: How we impact nature affects how nature impacts us. Resilient ecosystems - the foundation of a sustainable planet and society - are essential to our future. We‚Äôre committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans. Addressing climate change: The effects of a changing climate are far reaching and significant. Unpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption. We're building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions. Integrating ESG: All companies have a role to play in building a more resilient future. Incorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business. We‚Äôre training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting. AXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL‚Äôs ‚ÄúHearts in Action‚Äù programs. These include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving. For more information, please see axaxl.com/sustainability.",executive,,"Python, SQL, Power BI",
4247366386,Data Engineer,ALIQAN Technologies,"Pune, Maharashtra, India (On-site)",On-site,‚Çπ600K/yr - ‚Çπ1.2M/yr,,"About the job Skills: Azure Databricks, ETL Tools, Azure Data Factory, python, SQL, etl processes, Data Integration, ms pbi, Job Responsibilities Responsible for designing, developing, and maintaining data pipelines and ETL processes using Databricks and Azure Data Factory. Good experience in using SQL, Python and Spark within Databricks notebooks for data processing and transformations. Integrate and orchestrate data from various sources, ensuring data quality and accuracy. Build reusable rules for Data Quality and required transformations. Optimize, monitor data pipelines for performance and scalability and debug in case of issues. Proficiency in Databricks, Azure Data Factory, Python, SQL is must. Familiarity with ETL principles, data warehousing, and big data technologies. Evaluate and define technical specs using BRD for BI and DW solutions. Build conceptual and logical models based on the functional flow of business in scalable way. Work with Business functions/ leadership and Application SMEs to understand the requirement and analysing the source to fulfil the requirements. Be compliant to data quality, governance, compliance, and other legal requirements. Act as key P.O.C between data analyst, data scientists, and the business/application teams. Must have Azure Databricks experience & Data Integrations. Good work experience on Data bricks, ADF, and other Azure and Databricks objects. Experience on creating frameworks towards building the optimized data pipelines is a must. Working knowledge of different self-hosted and other runtimes along with SSIS. Strong work experience of SQL, Scala, Python and Spark. Performance Tuning of Spark SQL running on DELTA LAKE storage and Strong Knowledge on Databricks and Cluster Configurations. Working knowledge of Data Warehousing, Modelling, Data Validation, Data Migration. Documentation of all project resources and follow best practices for development. Knowledge of front-end tools like Qlik Sense, MS PBI Minimum bachelors degree in computer science or related field is required. Desired Skills and Experience Azure Databricks, ETL Tools, Azure Data Factory, python, SQL, etl processes, Data Integration, ms pbi",,,"Python, SQL",
4247344435,Data Engineer IRC262774,GlobalLogic,"Nagpur, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Description We are looking for a skilled Data Engineer to join our existing team. Candidates will be working on Big Data applications using cutting edge technologies including Google Cloud Platform. The position offers 1st hand exposure to build data pipelines that can process peta-byte scale of data solving complex business problems Requirements Mandatory Skills: 4-6 years of hands-on experience in Data Engineering. Experience in writing and optimizing SQL queries in HIVE/Spark. Excellent coding and/or scripting skills in Python. Good experience in deploying spark applications in Kubernetes cluster. Good experience in development, deployment & troubleshooting of Spark applications. Exposure to any cloud environment (AWS/GCP preferred) Job responsibilities Role Description: Candidate will be part of an agile team. Development/Migration of new data pipelines. Optimizing/Fine-tuning existing workflows. Deploying Spark tasks on K8 clusters. Bringing new ideas for performance enhancement of data pipelines running on K8s.. Mandatory Skills: 4-6 years of hands-on experience in Data Engineering. Experience in writing and optimizing SQL queries in HIVE/Spark. Excellent coding and/or scripting skills in Python. Good experience in deploying spark applications in Kubernetes cluster. Good experience in development, deployment & troubleshooting of Spark applications. Exposure to any cloud environment (AWS/GCP preferred) What we offer Culture of caring. At GlobalLogic, we prioritize a culture of caring. Across every region and department, at every level, we consistently put people first. From day one, you‚Äôll experience an inclusive culture of acceptance and belonging, where you‚Äôll have the chance to build meaningful connections with collaborative teammates, supportive managers, and compassionate leaders. Learning and development. We are committed to your continuous learning and development. You‚Äôll learn and grow daily in an environment with many opportunities to try new things, sharpen your skills, and advance your career at GlobalLogic. With our Career Navigator tool as just one example, GlobalLogic offers a rich array of programs, training curricula, and hands-on opportunities to grow personally and professionally. Interesting & meaningful work. GlobalLogic is known for engineering impact for and with clients around the world. As part of our team, you‚Äôll have the chance to work on projects that matter. Each is a unique opportunity to engage your curiosity and creative problem-solving skills as you help clients reimagine what‚Äôs possible and bring new solutions to market. In the process, you‚Äôll have the privilege of working on some of the most cutting-edge and impactful solutions shaping the world today. Balance and flexibility. We believe in the importance of balance and flexibility. With many functional career areas, roles, and work arrangements, you can explore ways of achieving the perfect balance between your work and life. Your life extends beyond the office, and we always do our best to help you integrate and balance the best of work and life, having fun along the way! High-trust organization. We are a high-trust organization where integrity is key. By joining GlobalLogic, you‚Äôre placing your trust in a safe, reliable, and ethical global company. Integrity and trust are a cornerstone of our value proposition to our employees and clients. You will find truthfulness, candor, and integrity in everything we do. About GlobalLogic GlobalLogic, a Hitachi Group Company, is a trusted digital engineering partner to the world‚Äôs largest and most forward-thinking companies. Since 2000, we‚Äôve been at the forefront of the digital revolution ‚Äì helping create some of the most innovative and widely used digital products and experiences. Today we continue to collaborate with clients in transforming businesses and redefining industries through intelligent products, platforms, and services.",manager,,"Python, SQL",
4247344270,Data Engineer ‚Äì ETL Developer IRC266727,GlobalLogic,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Description Join GlobalLogic, to be a valid part of the team working on a huge software project for the world-class company providing M2M / IoT 4G/5G modules e.g. to the automotive, healthcare and logistics industries. Through our engagement, we contribute to our customer in developing the end-user modules‚Äô firmware, implementing new features, maintaining compatibility with the newest telecommunication and industry standards, as well as performing analysis and estimations of the customer requirements. ‚Äî Requirements 6+ Years of experience in Data engineering with experience in Guidewire products Data Pipelines & ETL Development: Design, build, and maintain scalable and efficient ETL pipelines that integrate Guidewire data with data warehouses, data lakes, and other enterprise systems. Data Modeling & Architecture: Work closely with Data Architects to develop, optimize, and manage Guidewire data models and schema, ensuring high performance and scalability. Cloud Integration: Implement cloud-based data engineering solutions using platforms like AWS, Azure, or GCP, ensuring smooth integration of Guidewire data with cloud services Data Quality & Governance: Ensure data integrity, accuracy, and compliance with data governance standards across all Guidewire-related data pipelines and integrations. Performance Tuning & Optimization: Optimize data processing workflows and queries to ensure high performance, minimizing delays in data availability. Collaboration: Collaborate with business analysts, data architects, and other IT teams to translate business requirements into effective data engineering solutions. Automation: Build and maintain automation processes for regular data loads, ensuring reliable data ingestion and processing with minimal manual intervention. Documentation & Best Practices: Maintain clear documentation of data engineering processes, data flows, and pipeline architecture while adhering to industry best practices. Technical Skills 6+ years of experience in data engineering or a similar role. 4+ years of experience with Guidewire Insurance Suite (PolicyCenter, BillingCenter, ClaimCenter). 2+ years of hands-on experience with Guidewire Cloud Data Access (CDA) for data extraction and integration. Proven experience in building ETL pipelines and integrating Guidewire data with cloud-based and on-premises systems. Strong SQL and PL/SQL skills for querying and transforming Guidewire data. Proficiency with data integration and ETL tools such as Informatica, PySpark etc. Experience with Azure cloud platforms for data storage, processing, and integration. Familiarity with big data technologies and modern data architecture. Hands-on experience with APIs and microservices for data integration. Knowledge of version control systems (e.g., Git) and CI/CD practices for automating data workflows Job responsibilities Design, build, and maintains data pipelines and infrastructure to ensure efficient data processing, storage, and accessibility. What we offer Culture of caring. At GlobalLogic, we prioritize a culture of caring. Across every region and department, at every level, we consistently put people first. From day one, you‚Äôll experience an inclusive culture of acceptance and belonging, where you‚Äôll have the chance to build meaningful connections with collaborative teammates, supportive managers, and compassionate leaders. Learning and development. We are committed to your continuous learning and development. You‚Äôll learn and grow daily in an environment with many opportunities to try new things, sharpen your skills, and advance your career at GlobalLogic. With our Career Navigator tool as just one example, GlobalLogic offers a rich array of programs, training curricula, and hands-on opportunities to grow personally and professionally. Interesting & meaningful work. GlobalLogic is known for engineering impact for and with clients around the world. As part of our team, you‚Äôll have the chance to work on projects that matter. Each is a unique opportunity to engage your curiosity and creative problem-solving skills as you help clients reimagine what‚Äôs possible and bring new solutions to market. In the process, you‚Äôll have the privilege of working on some of the most cutting-edge and impactful solutions shaping the world today. Balance and flexibility. We believe in the importance of balance and flexibility. With many functional career areas, roles, and work arrangements, you can explore ways of achieving the perfect balance between your work and life. Your life extends beyond the office, and we always do our best to help you integrate and balance the best of work and life, having fun along the way! High-trust organization. We are a high-trust organization where integrity is key. By joining GlobalLogic, you‚Äôre placing your trust in a safe, reliable, and ethical global company. Integrity and trust are a cornerstone of our value proposition to our employees and clients. You will find truthfulness, candor, and integrity in everything we do. About GlobalLogic GlobalLogic, a Hitachi Group Company, is a trusted digital engineering partner to the world‚Äôs largest and most forward-thinking companies. Since 2000, we‚Äôve been at the forefront of the digital revolution ‚Äì helping create some of the most innovative and widely used digital products and experiences. Today we continue to collaborate with clients in transforming businesses and redefining industries through intelligent products, platforms, and services.",manager,,SQL,
4228152299,Junior Data Engineer work location : PAN India,Capgemini Engineering,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Job Title: Data Engineer Company Overview: Capgemini Engineering is a forward-thinking organization dedicated to leveraging data-driven insights to propel our business strategies. We are currently seeking a motivated Junior Data Engineer with proficiency in Python and a keen interest in working with Cognite Data Fusion to join our dynamic team. Position Overview: As a Junior Data Engineer, you will be instrumental in designing, developing, and maintaining scalable data pipelines and infrastructure. Your primary focus will be on integrating and optimizing data workflows using Cognite Data Fusion and Python, thereby facilitating seamless data accessibility and analysis across the organization. Key Responsibilities: Data Pipeline Development: Design, implement, and optimize end-to-end data pipelines for ingesting, processing, and transforming large volumes of structured and unstructured data. Data Integration: Utilize Cognite Data Fusion to automate and scale the contextualization of various data sources, ensuring efficient data integration and accessibility. Programming: Develop robust ETL processes and data workflows using Python, ensuring code quality, scalability, and maintainability. Collaboration: Work closely with cross-functional teams, including data scientists, analysts, and business stakeholders, to understand data requirements and deliver effective solutions. Data Quality Assurance: Implement data validation and quality checks to ensure accuracy, consistency, and reliability of data. Documentation: Maintain comprehensive documentation of data processes, workflows, and system architectures. Bachelor‚Äôs degree in Computer Science, Information Technology, Engineering, or a related field. Proficiency in Python programming language. Familiarity with data integration platforms Understanding of data modelling, database design, and data warehousing concepts. Experience with SQL and working with relational databases. Basic knowledge of cloud platforms such as AWS or Azure is a plus. Strong problem-solving skills and attention to detail. Excellent communication and collaboration abilities. Preferred Qualifications: Experience with data pipeline and workflow management tools. Knowledge of big data technologies and frameworks. Familiarity with data visualization tools and techniques (Grafana is a plus).",,,"Python, SQL",
4247275244,Data Engineer_Bangalore_5+ Years_CTC 30-35 LPA,Andor Tech,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Contract,,"About the job Required Qualifications: 8+ years of software engineering experience with strong focus on big data platforms. Proven hands-on experience with Hadoop, Hive, Spark, Scala, and large-scale distributed systems. Track record of building and maintaining high-throughput, low-latency platforms. Experience in agile software development methodologies. Excellent verbal and written communication skills, especially with leadership audiences. Proven ability to mentor developers and conduct in-depth code reviews. Willingness and capability to support 24/7 production environments and handle critical escalations. Nice to Have: Experience with Kafka, messaging platforms, and stream processing. Background in retail, eCommerce, or other data-intensive industries. Experience with RESTful APIs, microservices architecture, and backend technologies like Java and Kotlin. Familiarity with cloud platforms (AWS, GCP, Azure) and containerization (Docker, Kubernetes). Passion for learning, adapting to new challenges, and contributing to a collaborative team environment. #Bigdata #dataengineer #hadoop #hive",Manager,,,
4247045246,Risk Data Engineer/ Leads,EY,"Chennai, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job At EY, you‚Äôll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we‚Äôre counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. RCE - Risk Data Engineer/Leads Job Description: Our Technology team builds innovative digital solutions rapidly and at scale to deliver the next generation of Financial and Non- Financial services across the globe. The Position is a senior technical, hands-on delivery role, requiring the knowledge of data engineering, cloud infrastructure and platform engineering, platform operations and production support using ground-breaking cloud and big data technologies. The ideal candidate with 3-6 years of relevant experience, will possess strong technical skills, an eagerness to learn, a keen interest on 3 keys pillars that our team support i.e. Financial Crime, Financial Risk and Compliance technology transformation, the ability to work collaboratively in fast-paced environment, and an aptitude for picking up new tools and techniques on the job, building on existing skillsets as a foundation. Senior Data & BI Analyst Job Summary: The Senior Data & BI Analyst is a key contributor to the Enterprise Analytics Center of Excellence (COE), responsible for promoting and supporting self-service analytics across the firm using tools such as ThoughtSpot and Tableau. This role requires a strong technical foundation in data analytics and BI tools, combined with strategic consulting skills to guide users in developing scalable, high-quality analytics. The ideal candidate has a passion for data, an understanding of financial services, and the ability to collaborate across teams to enable data-driven decision-making. Responsibilities: Contribute to the buildout and ongoing evolution of the Enterprise Analytics COE, shaping best practices and governance standards. Act as a subject matter expert (SME) in enterprise lakehouse data, guiding end users in identifying and leveraging key data sources. Work closely with business users to understand their analytics needs, providing consultation on BI tool selection and data strategies. Lead training sessions for ThoughtSpot (and Tableau) users to enhance adoption and proficiency in self-service analytics. Perform hands-on data analysis against enterprise lakehouse data to uncover insights and demonstrate tool capabilities. Develop reusable analytics templates, dashboards, and frameworks that drive consistency and efficiency across the organization. Support initial dashboard and analytics development efforts, assisting teams with prototyping and best practices implementation. Stay up to date on emerging BI trends, ThoughtSpot and Tableau advancements, and AI-driven analytics approaches to drive innovation. Required Qualifications: Experience working as a Technical Data Analyst, BI Analyst, or Business Analyst in an enterprise environment. Strong SQL skills and experience handling large datasets. Proficiency in data warehousing concepts, ETL processes, and BI reporting tools such as Tableau, Power BI, Qlik, ThoughtSpot. Excellent analytical and problem-solving skills with the ability to collaborate across cross-functional teams. Strong stakeholder management abilities, with the ability to effectively communicate technical insights to non-technical audiences. Preferred Qualifications: Hands-on experience with ThoughtSpot BI Platform. Financial services experience and understanding of financial data structures. Familiarity with cloud-based BI platforms (AWS, Azure, etc.). Knowledge of Natural Language Processing (NLP), AI-driven data aggregation, and automated reporting technologies. EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",,,"SQL, Tableau, Power BI, Data Analysis",
4246518360,Data Engineer-Data Platforms-AWS,IBM,"Kochi, Kerala, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and AWS Cloud Data Platform Responsibilities: Experienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases on Cloud Data Platforms (AWS) or HDFS Experienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform Experience in developing streaming pipelines Experience to work with Hadoop / AWS eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc Preferred Education Master's Degree Required Technical And Professional Expertise Minimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala ; Minimum 3 years of experience on Cloud Data Platforms on AWS; Experience in AWS EMR / AWS Glue / Data Bricks, AWS RedShift, DynamoDB Good to excellent SQL skills Exposure to streaming solutions and message brokers like Kafka technologies Preferred Technical And Professional Experience Certification in AWS and Data Bricks or Cloudera Spark Certified developers",,,"Python, SQL",
4246299482,Data Engineer ‚ÄìC11/Officer Pune,Citi,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job The Role The Data Engineer is accountable for developing high quality data products to support the Bank‚Äôs regulatory requirements and data driven decision making. A Mantas Scenario Developer will serve as an example to other team members, work closely with customers, and remove or escalate roadblocks. By applying their knowledge of data architecture standards, data warehousing, data structures, and business intelligence they will contribute to business outcomes on an agile team. Responsibilities Developing and supporting scalable, extensible, and highly available data solutions Deliver on critical business priorities while ensuring alignment with the wider architectural vision Identify and help address potential risks in the data supply chain Follow and contribute to technical standards Design and develop analytical data models Required Qualifications & Work Experience First Class Degree in Engineering/Technology (4-year graduate course) 5 to 8 years‚Äô experience implementing data-intensive solutions using agile methodologies Experience of relational databases and using SQL for data querying, transformation and manipulation Experience of modelling data for analytical consumers Hands on Mantas ( Oracle FCCM ) expert throughout the full development life cycle, including: requirements analysis, functional design, technical design, programming, testing, documentation, implementation, and on-going technical support Translate business needs (BRD) into effective technical solutions and documents (FRD/TSD) Ability to automate and streamline the build, test and deployment of data pipelines Experience in cloud native technologies and patterns A passion for learning new technologies, and a desire for personal growth, through self-study, formal classes, or on-the-job training Excellent communication and problem-solving skills T echnical Skills (Must Have) ETL: Hands on experience of building data pipelines. Proficiency in two or more data integration platforms such as Ab Initio, Apache Spark, Talend and Informatica Mantas: Expert in Oracle Mantas/FCCM, Scenario Manager, Scenario Development, thorough knowledge and hands on experience in Mantas FSDM, DIS, Batch Scenario Manager Big Data: Experience of ‚Äòbig data‚Äô platforms such as Hadoop, Hive or Snowflake for data storage and processing Data Warehousing & Database Management: Understanding of Data Warehousing concepts, Relational (Oracle, MSSQL, MySQL) and NoSQL (MongoDB, DynamoDB) database design Data Modeling & Design: Good exposure to data modeling techniques; design, optimization and maintenance of data models and data structures Languages: Proficient in one or more programming languages commonly used in data engineering such as Python, Java or Scala DevOps: Exposure to concepts and enablers - CI/CD platforms, version control, automated quality control management Technical Skills (Valuable) Ab Initio: Experience developing Co>Op graphs; ability to tune for performance. Demonstrable knowledge across full suite of Ab Initio toolsets e.g., GDE, Express>IT, Data Profiler and Conduct>IT, Control>Center, Continuous>Flows Cloud: Good exposure to public cloud data platforms such as S3, Snowflake, Redshift, Databricks, BigQuery, etc. Demonstratable understanding of underlying architectures and trade-offs Data Quality & Controls: Exposure to data validation, cleansing, enrichment and data controls Containerization: Fair understanding of containerization platforms like Docker, Kubernetes File Formats: Exposure in working on Event/File/Table Formats such as Avro, Parquet, Protobuf, Iceberg, Delta Others: Basics of Job scheduler like Autosys. Basics of Entitlement management Certification on any of the above topics would be an advantage. ------------------------------------------------------ Job Family Group: Technology ------------------------------------------------------ Job Family: Digital Software Engineering ------------------------------------------------------ Time Type: Full time ------------------------------------------------------ Most Relevant Skills Please see the requirements listed above. ------------------------------------------------------ Other Relevant Skills For complementary skills, please see above and/or contact the recruiter. ------------------------------------------------------ Citi is an equal opportunity employer, and qualified candidates will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other characteristic protected by law. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi . View Citi‚Äôs EEO Policy Statement and the Know Your Rights poster.",Manager,,"Python, SQL",
4243739777,Data Engineer-Data Platforms,IBM,"Navi Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution. Your Primary Responsibilities Include Lead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements. Strive for continuous improvements by testing the build solution and working under an agile framework. Discover and implement the latest technologies trends to maximize and build creative solutions Preferred Education Master's Degree Required Technical And Professional Expertise Experience with Apache Spark (PySpark): In-depth knowledge of Spark‚Äôs architecture, core APIs, and PySpark for distributed data processing. Big Data Technologies: Familiarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering Skills: Strong understanding of ETL pipelines, data modeling, and data warehousing concepts. Strong proficiency in Python: Expertise in Python programming with a focus on data processing and manipulation. Data Processing Frameworks: Knowledge of data processing libraries such as Pandas, NumPy. SQL Proficiency: Experience writing optimized SQL queries for large-scale data analysis and transformation. Cloud Platforms: Experience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems Preferred Technical And Professional Experience Define, drive, and implement an architecture strategy and standards for end-to-end monitoring. Partner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering, Good to have detection and prevention tools for Company products and Platform and customer-facing",,,"Python, SQL, Data Analysis",
4246298175,Data Engineer,Droisys,India (Remote),Remote,Full-time,,"About the job About the job Droisys is an innovation technology company focused on helping companies accelerate their digital initiatives from strategy and planning through execution. We leverage deep technical expertise, Agile methodologies, and data-driven intelligence to modernize systems of engagement and simplify human/tech interaction. Amazing things happen when we work in environments where everyone feels a true sense of belonging and when candidates have the requisite skills and opportunities to succeed. At Droisys, we invest in our talent and support career growth, and we are always on the lookout for amazing talent who can contribute to our growth by delivering top results for our clients. Join us to challenge yourself and accomplish work that matters. Job Title: Data Engineer with Cortex ( or Similar AI experience) Location: India / Remote Key Responsibilities: Design and implement ETL/ELT workflows using Snowflake and Python. Build scalable and secure data pipelines using AWS services such as S3, Lambda, Glue, Redshift, and EMR. Develop, manage, and optimize data models and warehouse architecture in Snowflake. Leverage Cortex for deploying and managing machine learning workflows or real-time data processing (based on your use case). Utilize Docker and container orchestration tools (e.g., Kubernetes) for deploying data applications. Collaborate with cross-functional teams to understand data requirements and ensure high data quality and availability. Monitor and troubleshoot performance issues across data infrastructure. Droisys is an equal opportunity employer. We do not discriminate based on race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law. Droisys believes in diversity, inclusion, and belonging, and we are committed to fostering a diverse work environment.",,,"Python, Machine Learning",
4236631734,Data Engineer,Ubique Systems,India (Remote),Remote,Full-time,,"About the job Ubique Systems is hiring. Location- Work From Home (Remote) Experience: 3-5 years Role: Data Engineer Type: Contract Job Description: Data Pipelines: Proven experience in building scalable and reliable data pipelines BigQuery: Expertise in writing complex SQL transformations; hands-on with indexing and performance optimization Ingestion: Skilled in data scraping and ingestion through RESTful APIs and file-based sources Orchestration: Familiarity with orchestration tools like Prefect, Apache Airflow (nice to have) Tech Stack: Proficient in Python, FastAPI, and PostgreSQL End-to-End Workflows: Capable of owning ingestion, transformation, and delivery processes Interested? Kindly share your CV with siddhi.divekar@ubique-systems.com",Manager,,"Python, SQL",
4242618520,Data Engineer,LTIMindtree,"Maharashtra, India (On-site)",On-site,Full-time,,"About the job Greetings from LTIMindtree! We are thrilled to announce an exciting opportunity for the role of Informatica IDMC/IICS Developer! We are on the lookout for exceptional candidates who can join us within an immediate to 30 days' time. If you are passionate about this field and eager to take on new challenges, we would love to hear from you! This is a fantastic chance to become a part of our dynamic team and contribute to our projects with your expertise and skills Experience: - 3years to 8 years Location- Any LTIM location Notice Period- Immediate 30 Days Job Description: - 3 to 5 Years of experience in IDMC ETL tool Must have experience in CICD pipeline creation maintenance good hand on experience in connecting various data sources and APIs must have knowledge on Azure cloud experience in creation of mappings mapplets task flows in IDMC monitoring the production jobs analyzing the failures and resolving them as per SLA Migration of code to higher environments and validation Document creation and maintain as per the design good knowledge on performance improvement of existing jobs excellent client communication skill Skills Mandatory Skills : ANSI-SQL, IICS - IDMC, Informatica PowerCenter Good to Have Skills : Dimensional Data Modeling, Informatica DVO Interested candidate can share their profile to Pooja.Jawalekar2@ltimindtree.com",,,SQL,
4240005585,Data Engineer,TresVista,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Overview: TresVista is looking to hire an Associate in its Data Intelligence Group team, who will be primarily responsible for managing clients as well as monitor/execute projects both for the clients as well as internal teams. The Associate may be directly managing a team of up to 3-4 Data Engineers & Analysts across multiple data engineering efforts for our clients with varied technologies. They would be joining the current team of 70+ members, which is a mix of Data Engineers, Data Visualization Experts, and Data Scientists. Roles and Responsibilities: ‚Ä¢ Expertise and experience with Analytics, SQL, Python and handling large datasets is mandatory. ‚Ä¢ Use technical skills to ensure the accuracy of large analytical data sets, automate processes with scripts and macros and efficiently query information from a vast database. ‚Ä¢ Have a good understanding of Fixed Income, Equity, Derivatives and Alternatives products and how they are modeled and traded ‚Ä¢ Exhibit attention to detail when quality checking Package analytics and be accountable of timely delivery of reports to clients in accordance with Service Level Agreements. ‚Ä¢ Engage in meetings with end-users of the Package product from all levels within the company from Portfolio and Risk Managers to Operations teams and also with our external Clients. ‚Ä¢ Support client requests related to the Package analytics. ‚Ä¢ Project work: engaging with other internal teams to think creatively and deliver innovative solutions to our complex client demands. Prerequisites: ‚Ä¢ Excellent problem-solving and critical-thinking skills and an ability to identify problems, design and articulate solutions and implement change. ‚Ä¢ Expertise and experience with Analytics, SQL, Python and handling large datasets is mandatory. ‚Ä¢ Experience with UNIX, PERL and developing an Asset Management platform is highly desirable. ‚Ä¢ Knowledge and understanding of Fixed Income, Equity, Derivatives and Alternatives products is preferred. ‚Ä¢ Experience with Risk analytics such as Durations, Spreads, Beta and VaR is preferable. ‚Ä¢ Must possess strong verbal and written communication skills and be able to develop good working relationships with stakeholders. Must be detail orientated, possess initiative and work well under pressure. ‚Ä¢ Work experience with BFSI will be an added advantage. ‚Ä¢ A Degree in Engineering or Technology is required. Experience 3-7 years Education BTech/B.E/MCA Compensation The compensation structure will be as per industry standard",Manager,,"Python, SQL",
4207928464,Senior Data Engineer II,MakeMyTrip,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Position: Senior Data Engineer II Experience: Must have 4+ years of experience About Role: We are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences. About us: Nurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India‚Äôs online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010. Post-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018. GO-MMT is the corporate entity of three giants in the Online Travel Industry‚ÄîGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry. About the team: MakeMyTrip as India‚Äôs leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs. Data Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement. There are two key responsibilities of Data Engineering team: One to develop the platform for data capture, storage, processing, serving and querying. Second is to develop data products starting from; o personalization & recommendation platform o customer segmentation & intelligence o data insights engine for persuasions and o the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users We developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences. Technology experience : Extensive experience working with large data sets with hands-on technology skills to design and build robust data architecture Extensive experience in data modeling and database design At least 4+ years of hands-on experience in PySpark/BigData Tech stack Stream processing engines ‚Äì Spark Structured Streaming Analytical processing on Big Data using Spark At least 4+ years of experience in Python/Scala Hands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems At least 4+ years of cloud deployment experience ‚Äì AWS | Azure | Google Cloud Platform At least 4+ product deployments of big data technologies ‚Äì Business Data Lake, NoSQL databases etc Awareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies Should have experience in architecting and implementing domain centric big data solutions Ability to frame architectural decisions and provide technology leadership & direction Excellent problem solving, hands-on engineering, and communication skills",,,"Python, SQL, Machine Learning",
4251256943,Data Engineer (Research),micro1,APAC (Remote),Remote,Full-time,,"About the job Job Title : Data Engineer Job Type : Full-time, Contractor Location : Remote Job Summary We are seeking a skilled Data Engineer with strong analytical thinking and a passion for solving research-driven data challenges. This exciting role involves building data pipelines, performing exploratory data analysis (EDA), and working with both structured and unstructured data. Your exposure to AI/ML techniques will be advantageous as you collaborate with data scientists and researchers to derive insights and support model development. Key Responsibilities - Design, develop, and maintain robust and scalable ETL pipelines for ingesting and transforming raw data from diverse sources. - Conduct exploratory data analysis (EDA) to identify patterns, anomalies, and valuable insights. - Collaborate with researchers and data scientists to prepare datasets for AI/ML modeling and experimentation. - Develop and manage data models, schemas, and databases for efficient storage and querying of large datasets. - Write optimized SQL queries and scripts for data extraction and aggregation. - Ensure data quality, integrity, and security across all pipelines and storage systems. - Automate data validation and reporting workflows to facilitate ongoing research tasks. Required Skills and Qualifications - Proficiency in Python and SQL. - Experience in building and managing ETL pipelines. - Expertise in using Pandas and NumPy for data manipulation. - Familiarity with AI/ML techniques and tools such as scikit-learn, Hugging Face Transformers, and OpenAI API. - Strong written and verbal communication skills. - Experience with databases like PostgreSQL and MySQL. - Knowledge in exploratory data analysis tools including Jupyter Notebooks, VS Code, or PyCharm. Preferred Qualifications - Experience with vector search tools like Qdrant, FAISS, or Pinecone. - Familiarity with data visualization tools such as Matplotlib, Seaborn, or Plotly.",,,"Python, SQL, Data Analysis",
4232799167,Senior Data Engineer BizTech,Airbnb,"Bengaluru, Karnataka, India (Remote)",Remote,Full-time,,"About the job Airbnb was born in 2007 when two hosts welcomed three guests to their San Francisco home, and has since grown to over 5 million hosts who have welcomed over 2 billion guest arrivals in almost every country across the globe. Every day, hosts offer unique stays and experiences that make it possible for guests to connect with communities in a more authentic way. The Community You Will Join At Airbnb, we need to ensure every area of the business has trustworthy data to fuel insight and innovation. Understanding the business need, securing the right data sources, designing usable data models, and building robust & dependable data pipelines are essential skills to meet this goal. We Are Currently Hiring For The Following Teams Apps and Compliance: This team is responsible for building scalable, high quality data sets andsolutions to enable Airbnb to comply with Tax, payments and legal regulations to ensurebusiness continuity. In addition, this team is responsible for building data sets for Airbnb‚Äôsinternal applications (e.g. CRM data, projects data, workspace data) to fuel growth and driveoperational efficiencies. The Difference You Will Make Apps and Compliance: Our team charter is on enabling Airbnb to comply with Tax, Payments,and Legal regulations so that our Hosts can continue to operate in regulated geos. Ourproducts ingest, process, validate, and deliver large datasets to government authorities (oftenpartnered with tax remittance) so our data must be of the highest accuracy and quality. As youbuild and maintain key components of the critical Compliance data ecosystem, you‚Äôll have theopportunity to contribute to creating standards and best practices for Airbnb‚Äôs DataEngineering. Your work on solving complex business challenges at scale will be instrumental inshaping the tools, processes, and standards used by the broader data community. A Typical Day Design, build, and maintain robust and efficient data pipelines that collect, process, and storedata from various sources, including user interactions, listing details, and external data feeds. Develop data models that enable the efficient analysis and manipulation of data formerchandising optimization. Ensure data quality, consistency, and accuracy. Build scalable data pipelines (SparkSQL & Scala) leveraging Airflow scheduler/executorframework Collaborate with cross-functional teams, including Data Scientists, Product Managers, andSoftware Engineers, to define data requirements, and deliver data solutions that drivemerchandising and sales improvements. Contribute to the broader Data Engineering community at Airbnb to influence tooling andstandards to improve culture and productivity Improve code and data quality by leveraging and contributing to internal tools to automaticallydetect and mitigate issues Your Expertise 5-9+ years of relevant industry experience with a BS/Masters, or 2+ years with a PhD Extensive experience designing, building, and operating robust distributed data platforms (e.g., Spark, Kafka, Flink, HBase) and handling data at the petabyte scale. Strong knowledge of Java, Scala, or Python, and expertise with data processing technologies and query authoring (SQL). Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions Expertise with ETL schedulers such as Apache Airflow, Luigi, Oozie, AWS Glue or similar frameworks Solid understanding of data warehousing concepts and hands-on experience with relational databases (e.g., PostgreSQL, MySQL) and columnar databases (e.g., Redshift, BigQuery, HBase, ClickHouse) Excellent written and verbal communication skills",Manager,,"Python, SQL",
4248695144,Senior Data Engineer,Delta Air Lines,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job About the Delta Technology Hub (DTH), Bangalore Delta has fast emerged as a customer-focused, innovation-led, technology-enabled business. The Delta Technology Hub will contribute directly to these objectives. It will sustain our long-term aspirations of delivering niche, IP-intensive, high-value, and innovative solutions. It supports various teams and functions across Delta and is an integral part of our transformation agenda, working seamlessly with a distributed team to create memorable experiences for customers. ‚ÄúWhy join?‚Äù: Technology is a key enabler of the differentiated services that Delta provides. At the DTH, you get the opportunity to work on projects with a significant impact on business outcomes and customer experience. Deepen your knowledge by taking part in multifaceted learning and development programs ‚Äìexposure to extensive internal and partner repositories, institutional affiliations, and industry SIG (Special Interest Groups) partnerships. Collaborate with research, innovation and IP co-development partners. Immerse yourself in an employee-centric culture. Develop deep and broad business acumen on airline operations while retaining focus on cutting-edge technology driven solutions. Avail a full range of benefits that support you and your family: Insurance, Commute, Meals, Retirement, and Special Travel opportunities. Avail a full range of benefits that support you and your family: Insurance, Commute, Meals, Retirement, and special travel opportunities. How you‚Äôll help us Keep Climbing (overview & key responsibilities): Delta Air Lines is seeking a talented and motivated Data Engineer to join their Marketing Science team. This role is perfect for someone with a deep understanding of data engineering best practices, strong technical skills, and business acumen, as well as the ability to learn complex data models and apply that knowledge to solve real-world business challenges. You will lead the development of data pipelines to integrate, aggregate, and harmonize marketing performance data from various sources, while driving the migration to modern platforms like Python and AWS. Additionally, you will collaborate closely with cross-functional teams, including analysts, data scientists, and marketing stakeholders, to ensure the seamless delivery of reliable and scalable data solutions. Responsibilities: Data Pipeline Development and Maintenance: Design, build, and optimize scalable ETL/ELT pipelines to ingest data from diverse sources such as APIs, cloud platforms, and databases. Ensure pipelines are robust, efficient, and capable of handling large volumes of data. Data Integration and Harmonization: Aggregate and normalize marketing performance data from multiple platforms (e.g., Adobe Analytics, Salesforce Marketing Cloud, Delta‚Äôs EDW, ad platforms). Implement data transformation and enrichment processes to support analytics and reporting needs. Data Quality and Monitoring: Develop and implement data validation and monitoring frameworks to ensure data accuracy and consistency. Troubleshoot and resolve issues related to data quality, latency, or performance. Collaboration with Stakeholders: Partner with marketing teams, analysts, and data scientists to understand data requirements and translate them into technical solutions. Provide technical support and guidance on data-related issues or projects. Tooling and Automation: Leverage cloud-based solutions and frameworks (e.g., AWS) to streamline processes and enhance automation. Maintain and optimize existing workflows while continuously identifying opportunities for improvement. Documentation and Best Practices: Document pipeline architecture, data workflows, and processes for both technical and non-technical audiences. Follow industry best practices for version control, security, and data governance. Continuous Learning and Innovation: Stay current with industry trends, tools, and technologies in data engineering and marketing analytics. Recommend and implement innovative solutions to improve the scalability and efficiency of data systems. What you need to succeed (minimum qualifications): Bachelor of Science degree in Computer Science or equivalent in required At least 4+ years of post-degree professional experience as a data engineer developing and maintaining data pipelines Extensive experience with databases and data platforms (Teradata and AWS preferred) 5+ Hands-on experience in designing, implementing, managing large scale data and ETL solutions utilizing AWS Compute, Storage and database services (S3, Lambda, Redshift, Glue, Athena) Proficiency in Python, SQL, PySpark, Glue, Lambda, S3 and other AWS tool sets Strong knowledge of relational and non-relational databases Good understanding of data warehouses, ETL, AWS architecture, Airflow and Redshift Ability to create clean, well-designed code and systems Proven ability to work with large and complex datasets Strong analytical and programming skills with the ability to solve data-related challenges efficiently Strong attention to detail and a commitment to data accuracy. Proven ability to learn new data models quickly and apply them effectively in a fast-paced environment. Excellent communication skills with the ability to present complex data findings to both technical and non-technical audiences. What will give you a competitive edge (preferred qualifications): Desired Airline industry experience Desired experience working with marketing and media data Experience working with SAS to develop data pipelines AWS certifications: Solution Architect (SAA/SAP) or Data Analytics Specialty (DAS) Experience migrating data pipelines and systems to modern cloud-based solutions Familiarity with marketing data platforms",,,"Python, SQL",
4249653741,DATA ENGINEER (ML/AI EXPERIENCE),"Svitla Systems, Inc.",Greater Kolkata Area (Remote),Remote,Full-time,,"About the job Svitla Systems Inc. is looking for a Data Engineer (with ML/AI Experience) for a full-time position (40 hours per week) in India . Our client is the world‚Äôs largest travel guidance platform, helping hundreds of millions each month become better travelers, from planning to booking to taking a trip. Travelers across the globe use the site and app to discover where to stay, what to do, and where to eat based on guidance from those who have been there before. With more than 1 billion reviews and opinions from nearly 8 million businesses, travelers turn to clients to find deals on accommodations, book experiences, and reserve tables at delicious restaurants. They discover great places nearby as a travel guide company, available in 43 markets and 22 languages. As a member of the Data Platform Enterprise Services Team, you will collaborate with engineering and business stakeholders to build, optimize, maintain, and secure the full data vertical, including tracking instrumentation, information architecture, ETL pipelines, and tooling that provide key analytics insights for business-critical decisions at the highest levels of product, finance, sales, CRM, marketing, data science, and more, all in a dynamic environment of continuously modernizing tech stack including highly scalable architecture, cloud-based infrastructure, and real-time responsiveness. Requirements BS/MS in Computer Science or related field. 4+ years of experience in data engineering or software development. Proven data design and modeling with large datasets (star/snowflake schema, SCDs, etc.). Strong SQL skills and ability to query large datasets. Experience with modern cloud data warehouses: Snowflake, BigQuery, etc. ETL development experience: SLA, performance, and monitoring. Familiarity with BI tools and semantic layer principles (e.g., Looker, Tableau). Understanding of CI/CD, testing, documentation practices. Comfortable in a fast-paced, dynamic environment. Ability to collaborate cross-functionally and communicate with technical/non-technical peers. Strong data investigation and problem-solving abilities. Comfortable with ambiguity and focused on clean, maintainable data architecture. Detail-oriented with a strong sense of ownership. Nice to Have Experience with data governance, data transformation tools. Prior work with e-commerce platforms. Experience with Airflow, Dagster, Monte Carlo, or Knowledge Graphs. Responsibilities Collaborate with stakeholders from multiple teams to collect business requirements and translate them into technical data model solutions. Design, build, and maintain efficient, scalable, and reusable data models in cloud data warehouses (e.g., Snowflake, BigQuery). Transform data from many sources into clean, curated, standardized, and trustworthy data products. Build data pipelines and ETL processes handling terabytes of data. Analyze data using SQL and dashboards; ensure models align with business needs. Ensure data quality through testing, observability tools, and monitoring. Troubleshoot complex data issues, validate assumptions, and trace anomalies. Participate in code reviews and help improve data development standards. We offer US and EU projects based on advanced technologies. Competitive compensation based on skills and experience. Annual performance appraisals. Remote-friendly culture and no micromanagement. Personalized learning program tailored to your interests and skill development. Bonuses for article writing, public talks, other activities. 15 PTO days, 10 national holidays. Free webinars, meetups and conferences organized by Svitla. Fun corporate celebrations and activities. Awesome team, friendly and supportive community!",,,"SQL, Tableau",
4241539418,"Data Engineer II, Data & AI, Customer Engagement Technology",Amazon,"Hyderabad, Telangana, India",,Full-time,,"About the job Description As a Data Engineer on the Data and AI team, you will design and implement robust data pipelines and infrastructure that power our organization's data-driven decisions and AI capabilities. This role is critical in developing and maintaining our enterprise-scale data processing systems that handle high-volume transactions while ensuring data security, privacy compliance, and optimal performance. You'll be part of a dynamic team that designs and implements comprehensive data solutions, from real-time processing architectures to secure storage solutions and privacy-compliant data access layers. The role involves close collaboration with cross-functional teams, including software development engineers, product managers, and scientists, to create data products that power critical business capabilities. You'll have the opportunity to work with leading technologies in cloud computing, big data processing, and machine learning infrastructure, while contributing to the development of robust data governance frameworks. If you're passionate about solving complex technical challenges in high-scale environments, thrive in a collaborative team setting, and want to make a lasting impact on our organization's data infrastructure, this role offers an exciting opportunity to shape the future of our data and AI capabilities. Key job responsibilities Design and implement ETL/ELT frameworks that handle large-scale data operations, while building reusable components for data ingestion, transformation, and orchestration while ensuring data quality and reliability. Establish and maintain robust data governance standards by implementing comprehensive security controls, access management frameworks, and privacy-compliant architectures that safeguard sensitive information. Drive the implementation of data solutions, both real-time and batch, optimizing them for both analytical workloads and AI/ML applications. Lead technical design reviews and provide mentorship on data engineering best practices, identifying opportunities for architectural improvements and guiding the implementation of enhanced solutions. Build data quality frameworks with robust monitoring systems and validation processes to ensure data accuracy and reliability throughout the data lifecycle. Drive continuous improvement initiatives by evaluating and implementing new technologies and methodologies that enhance data infrastructure capabilities and operational efficiency. A day in the life The day often begins with a team stand-up to align priorities, followed by a review of data pipeline monitoring alarms to address any processing issues and ensure data quality standards are maintained across systems. Throughout the day, you'll find yourself immersed in various technical tasks, including developing and optimizing ETL/ELT processes, implementing data governance controls, and reviewing code for data processing systems. You'll work closely with software engineers, scientists, and product managers, participating in technical design discussions and sharing your expertise in data architecture and engineering best practices. Your responsibilities extend to communicating with non-technical stakeholders, explaining data-related projects and their business impact. You'll also mentor junior engineers and contribute to maintaining comprehensive technical documentation. You'll troubleshoot issues that arise in the data infrastructure, optimize the performance of data pipelines, and ensure data security and compliance with relevant regulations. Staying updated on the latest data engineering technologies and best practices is crucial, as you'll be expected to incorporate new learnings into your work. By the end of a typical day, you'll have advanced key data infrastructure initiatives, solved complex technical challenges, and improved the reliability, efficiency, and security of data systems. Whether it's implementing new data governance controls, optimizing data processing workflows, or enhancing data platforms to support new AI models, your work directly impacts the organization's ability to leverage data for critical business decisions and AI capabilities. If you are not sure that every qualification on the list above describes you exactly, we'd still love to hear from you! At Amazon, we value people with unique backgrounds, experiences, and skillsets. If you‚Äôre passionate about this role and want to make an impact on a global scale, please apply! About The Team The Data and Artificial Intelligence (AI) team is a new function within Customer Engagement Technology. We own the end-to-end process of defining, building, implementing, and monitoring a comprehensive data strategy. We also develop and apply Generative Artificial Intelligence (GenAI), Machine Learning (ML), Ontology, and Natural Language Processing (NLP) to customer and associate experiences. Basic Qualifications 3+ years of data engineering experience Bachelor‚Äôs degree in Computer Science, Engineering, or a related technical discipline Preferred Qualifications Experience with AWS data services (Redshift, S3, Glue, EMR, Kinesis, Lambda, RDS) and understanding of IAM security frameworks Proficiency in designing and implementing logical data models that drive physical designs Hands-on experience working with large language models, including understanding of data infrastructure requirements for AI model training Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - Amazon Dev Center India - Hyderabad Job ID: A2996966",manager,,Machine Learning,
4246279105,Data Engineer,Snapmint,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job About Snapmint: India‚Äôs booming consumer market has over 300 million credit-eligible consumers, yet only 35million actively use credit cards. At Snapmint, we are building a better alternative to credit cards that lets consumers buy now and pay later for a wide variety of products, be it shoes, clothes, fashion accessories, clothes or mobile phones. We firmly believe that an enduring financial services business must be built on the bedrock of providing honest, transparent and fair terms. Founded in 2017, today we are the leading online zero-cost EMI provider in India. We have served over 10M consumers across 2,200 cities and are doubling year on year. Our founders are serial entrepreneurs and alumni of IIT Bombay and ISB with over two decades of experience across leading organizations like Swiggy, Oyo, Maruti Suzuki and ZS Associates before successfully scaling and exiting businesses in patent analytics, ad-tech and bank-tech software services. Key Responsibilities: Design, build, and manage real-time data pipelines using tools like Apache Kafka, Apache Flink, Apache Spark Streaming Optimize data pipelines for performance, scalability, and fault-tolerance. Perform real-time transformations, aggregations, and joins on streaming data Collaborate with data scientists to onboard new features and ensure they‚Äôre discoverable, documented, and versioned. Optimize feature retrieval latency for real-time inference use cases. Ensure strong data governance: lineage, auditing, schema evolution, and quality checks using tools such as as dbt, and OpenLineage Requirements: Bachelor‚Äôs degree in Engineering Strong programming skills in Python, Java, or Scala and proficient in SQL. Hands-on experience with Kafka, Flink, Spark Streaming Proficiency with data pipeline orchestration tools Exposure to event-driven microservices architecture. 2+ years of experience in an Indian startup/ tech company Strong written and verbal communication skills Good to have - familiarity with data warehouse/lake systems like BigQuery, Snowflake or Delta Lake. Good to have - familiarity with designing, building, and maintaining feature store infrastructure to support machine learning use cases. Location: Bangalore (Marathahalli) Working days: 5 days/ week",Associate,,"Python, SQL, Machine Learning",
4183474213,Azure Data Engineer (Walkin drive on 22-Mar @ Cochin),Cognisol,"Kochi, Kerala, India (On-site)",On-site,Full-time,,"About the job We are looking for immediate joiners or someone with short notice. If you are interested in pursuing this opportunity, kindly share your updated resume to divya.s@cognisolglobal.com Azure Data Engineer Proficiency In Python, SQL, Databricks, Snowflake, Data Modelling, ETL Process, Apache Spark, Pyspark, Data Integration and workflow orchestration , real time data processing experience and frameworks, Azure cloud experience Experience with Azure services like Azure Data Lake, Azure Blob Storage, and Azure SQL Database Knowledge of data processing languages, including SQL, Python, and Scala Understanding of Azure data services Experience with data storage and processing frameworks Experience with data warehousing concepts and tools Experience with Python/Pyspark frameworks/libraries; Expertise of Azure Databricks; Experienced of SQL; Experience in designing and implementing complex ETL and process flows Expertise across data transformation frameworks; Good to have Azure/Azure DevOps knowledge; Configuration Management, CI/CD and Cloud Skills: ci/cd,databricks,python,azure cloud,azure,azure sql database,snowflake,etl',data modelling,etl process,sql,scala,real-time data processing,workflow orchestration,data integration,pyspark,azure/azure devops,apache spark,configuration management,azure data lake,azure blob storage,data warehousing",,,"Python, SQL",
4233540154,Data Engineer,Nykaa,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job Job Description: Data Platform Engineer Location: Gurgaon (5days, work from office) The Nykaa‚Äôs Data Platform team manages a multi-terabyte, state-of-the-art, self-service data platform built on AWS, serving diverse analytics, data science, and operational use cases. We‚Äôre looking for a skilled and passionate Software Development Engineer 2 (SDE 2) to join our Data Platform team. In this hands-on role, you‚Äôll be responsible for designing, building, and scaling data pipelines, platform services, and tools that ensure fast, reliable, and secure access to data across the organization. Roles & Responsibilities: Technically Hands-on, prior experience with scalable Architecture Bring 4-6 years of software engineering and product delivery experience Good command over Data Structures and Algorithms Good coding skills in an Object Oriented programming language Exceptional problem solving and analytical skills Experience with Linux, SQL, Setting up Data Pipelines is a must have Experience with AWS S3, Boto API, Redshift, Postgresql, ELK, Python - Good to have Experience with Athena or Presto or Hive, Spark, Pyspark or Scala or Java is a must have Experience in data processing in large scale applications Education Qualifications: Minimum 4 years of relevant work experience with a Bachelor‚Äôs Degree in BS or MS in computer science or related field.",,,"Python, SQL",
4243620224,Data Engineer (Python and SQL),Infraveo Technologies,India (Remote),Remote,Full-time,,"About the job This is a remote position. We are seeking a Data Engineer (Python and SQL) to join our team. Responsibilties: AI patterns recognition: You'll be developing multiple AI features, from user categorization to autofilled descriptions, market and conversations sentiment analysis and AI insights to help crypto marketers. SDK, User Graph improvement and reliability: We're constantly improving our proprietary user graph by matching wallets to social profiles. Your mission will be supporting new crypto wallets and networks, automating a system to match more users to their identities, and applying data checks to ensure reliability. Integrations: As a customer data platform, we're expanding the number of data sources our customers can import from Web2 and Web3. You'll manage multiple API endpoints and integrate new third-party tools like Mixpanel, Amplitude, Segment, Dune Analytics, and DeFi Llama. This involves not only integration work but also data modeling and architectural design. Social Data analysis: You'll work with social data APIs like Twitter to analyze Key Opinion Leaders' performance and trends. Requirements Proven track record as a Data Engineer delivering complex data solutions. Advanced SQL skills and expertise with complex queries. Mastery in Python development and strong experience with PySpark. Extensive proficiency managing cloud services, including AWS Redshift, RDS Postgres, S3, Lambda, Kinesis, SQS, ECS, EC2. Strong competency implementing and supporting various data models, such as highly normalized, star schema and Data Vault. Practical experience with orchestration tools like Airflow, Dagster or Prefect. Demonstrated proficiency consuming and automating interactions with APIs. Hands-on experience creating data pipelines using dbt for various platforms (ideally AWS Redshift and Postgres). Experience in SaaS analytics, marketing, or crypto companies is a plus. Benefits Work Location: Remote 5 days working Desired Skills and Experience Proven track record as a Data Engineer delivering complex data solutions. Advanced SQL skills and expertise with complex queries. Mastery in Python development and strong experience with PySpark. Extensive proficiency managing cloud services, including AWS Redshift, RDS Postgres, S3, Lambda, Kinesis, SQS, ECS, EC2. Strong competency implementing and supporting various data models, such as highly normalized, star schema and Data Vault. Practical experience with orchestration tools like Airflow, Dagster or Prefect. Demonstrated proficiency consuming and automating interactions with APIs. Hands-on experience creating data pipelines using dbt for various platforms (ideally AWS Redshift and Postgres). Experience in SaaS analytics, marketing, or crypto companies is a plus.",,,"Python, SQL, Data Analysis",
4249622799,Data Engineer,Impetus,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Location: Pune / Indore / Bangalore / Noida Job Title: Big Data Engineer Job Description: Key Technical Skills (Must-Have & Good-to-Have): 2-4 years of professional experience Expertise and hands-on experience with Python ‚Äì Must Have In-depth knowledge of SparkQL / Spark DataFrame ‚Äì Must Have Strong understanding of SQL ‚Äì Good to Have Experience with Shell scripting ‚Äì Good to Have Knowledge of workflow engines such as Oozie, Autosys ‚Äì Good to Have Familiarity with Agile development methodologies ‚Äì Good to Have Understanding of Cloud technologies ‚Äì Good to Have Passion for exploring new technologies ‚Äì Good to Have Approach to Automation ‚Äì Good to Have Roles & Responsibilities: The selected candidate will be responsible for Data Warehouse modernization projects, including: Developing programs/scripts using Python/Java combined with SparkSQL/Spark DataFrame or Python/Java + Cloud-native SQL (e.g., RedshiftSQL, SnowSQL). Script validation and ensuring optimal performance. Performance tuning of data processes. Data ingestion from source to target platforms. Job orchestration. Experience Required: 2 to 4 years Education/Qualification: BE / B.Tech / MCA / M.Tech / M.Com",,,"Python, SQL",
4243629935,Azure Data Engineer,Tata Consultancy Services,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job Azure Data Engineer Primary Skills: SQL(Azure), Data Vault, Azure Data Factory, Blob Storage, Azure Synapse, Pipelines, Delta Lake Secondary Skills: Python, Apache Spark, MS Fabric (Trained Knowledge or experience), Power BI Proficiency in SQL and Python. SQL being far more important. Experience with Apache Spark for data processing (Python version) Understanding of Delta files and Lakehouse architecture Data warehouse basic knowledge or experience Hands-on skills (even gotten only by training) are essential for effectively using Microsoft Fabric. Practical experience in setting up end-to-end analytics, managing Lakehouse and Medallion Architecture, using Apache Spark and Delta Lake tables, handling data ingestion with Dataflows Gen2, creating pipelines with Data Factory, and setting up data warehouses is crucial. Understand the capabilities of Microsoft Fabric for complete analytics solutions, including data ingestion, transformation, storage, and visualization. Familiarity with features such as Direct Lake access for Power BI reports, is important. Utilize Apache Spark for large-scale data processing and work with Delta Lake tables for advanced data analytics. Ingest data using Dataflows Gen2 and create pipelines with Data Factory capabilities for multi-step data ingestion and transformation tasks. Set up and query data warehouses in Microsoft Fabric, integrating them with other analytics components. Learn how to secure a Microsoft Fabric data warehouse and administer the platform effectively.",,,"Python, SQL, Power BI",
4243578954,"Data Engineer, WW Returns & ReComm Tech& Inn",Amazon,"Hyderabad, Telangana, India",,Full-time,,"About the job Description The Data Engineer will own the data infrastructure for the Reverse Logistics Team which includes collaboration with software development teams to build the data infrastructure and maintain a highly scalable, reliable and efficient data system to support the fast growing business. You will work with analytic tools, can write excellent SQL scripts, optimize performance of SQL queries and can partner with internal customers to answer key business questions. We look for candidates who are self-motivated, flexible, hardworking and who like to have fun. About The Team Reverse Logistics team at Amazon Hyderabad Development Center is an agile team whose charter is to deliver the next generation of Reverse Logistics platform. As a member of this team, your mission will be to design, develop, document and support massively scalable, distributed data warehousing, querying and reporting system. Basic Qualifications 2+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Knowledge of AWS Infrastructure Knowledge of writing and optimizing SQL queries in a business environment with large-scale, complex datasets Strong analytical and problem solving skills. Curious, self-motivated & a self-starter with a ‚Äòcan do attitude‚Äô. Comfortable working in fast paced dynamic environment Preferred Qualifications Bachelor's degree in a quantitative/technical field such as computer science, engineering, statistics Proven track record of strong interpersonal and communication (verbal and written) skills. Experience developing insights across various areas of customer-related data: financial, product, and marketing Proven problem solving skills, attention to detail, and exceptional organizational skills Ability to deal with ambiguity and competing objectives in a fast paced environment Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing and operations Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - ADCI HYD 13 SEZ Job ID: A2998295",,,"Python, SQL",
4235656159,Data Engineer,HCLTech,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Greetings from HCLTECH ! Role: Data Engineer Hackathon location: Hilton, Manyata Tech Park Block A, Hotel Building-1, Hebbal, Nagawara, Bengaluru Hackathon Date and Time: 30th May (Friday at 9.00 AM) Experience: 6 to 12 Years Job Description: Primary skill : Databricks, Pyspark , SQL, Experience working on Data Warehouse project, Cloud (Azure or AWS) Secondary Skills: Hands on experience on data modelling. Data Vault is good to have, Data Security on data bricks, Hadoop, Python, Java Interested can drop your CV to nandhinia@hcltech.com",,,"Python, SQL",
4237419133,Big Data Engineer,Kresta Softech Private Limited,India (Remote),Remote,Full-time,,"About the job We‚Äôre looking for a skilled Big Data Engineer. Role Highlights: Position: Big Data Engineer Experience: 4+ years Location- Remote Work mode- WFH Notice Period: Immediate/15 days joiners Mandatory. Key Skills:- Big Data, AWS, CI/CD Pipelines, Scala , Python or Java or C++",,,Python,
4249677792,Cloud Data Developer/ Data Engineer,TELUS Digital,"Noida, Uttar Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Senior Cloud Data Developer We are seeking an exceptional Cloud Data Developer who can bridge the gap between data engineering and cloud-native application development. This role combines strong programming skills with data engineering expertise to build and maintain scalable data solutions in the cloud. Position Overview: Work with cutting-edge cloud technologies to develop data-intensive applications, create efficient data pipelines, and build robust data processing systems using AWS services and modern development practices. Core Responsibilities: Design and develop data-centric applications using Java Spring Boot and AWS services Create and maintain scalable ETL pipelines using AWS EMR and Apache NiFi Implement data workflows and orchestration using AWS MWAA (Managed Workflows for Apache Airflow) Build real-time data processing solutions using AWS SNS/SQS and AWS Pipes Develop and optimize data storage solutions using AWS Aurora and S3 Manage data discovery and metadata using AWS Glue Data Catalog Create search and analytics solutions using AWS OpenSearch Service Design and implement event-driven architectures for data processing Technical Requirements: Primary Skills: Strong proficiency in Java and Spring Boot framework Extensive experience with AWS data services: AWS EMR for large-scale data processing AWS Glue Data Catalog for metadata management AWS OpenSearch Service for search and analytics AWS Aurora for relational databases AWS S3 for data lake implementation Expertise in data pipeline development using: Apache NiFi AWS MWAA AWS Pipes AWS SNS/SQS",,,,
4229448752,Data Engineer,bp,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Senior Data Engineer will be Responsible for delivering business analysis and consulting activities for the defined specialism using sophisticated technical capabilities, building and maintaining effective working relationships with a range of customers, ensuring relevant standards are defined and maintained, and implementing process and system improvements to deliver business value. Specialisms: Business Analysis; Data Management and Data Science; Digital Innovation!!! Senior Data Engineer will work as part of an Agile software delivery team; typically delivering within an Agile Scrum framework. Duties will include attending daily scrums, sprint reviews, retrospectives, backlog prioritisation and improvements! Will coach, mentor and support the data engineering squad on the full range of data engineering and solutions development activities covering requirements gathering and analysis, solutions design, coding and development, testing, implementation and operational support. Will work closely with the Product Owner to understand requirements / user stories and have the ability to plan and estimate the time taken to deliver the user stories. Proactively collaborate with the Product Owner, Data Architects, Data Scientists, Business Analysts, and Visualisation developers to meet the acceptance criteria Will be very highly skilled and experienced in use of tools and techniques such as AWS Data Lake technologies, Redshift, Glue, Spark SQL, Athena Years of Experience: 8- 12 Essential domain expertise: Experience in Big Data Technologies ‚Äì AWS, Redshift, Glue, Py-spark Experience of MPP (Massive Parallel Processing) databases helpful ‚Äì e.g. Teradata, Netezza Challenges involved in Big Data ‚Äì large table sizes (e.g. depth/width), even distribution of data Experience of programming- SQL, Python Data Modelling experience/awareness ‚Äì Third Normal Form, Dimensional Modelling Data Pipelining skills ‚Äì Data blending, etc Visualisation experience ‚Äì Tableau, PBI, etc Data Management experience ‚Äì e.g. Data Quality, Security, etc Experience of working in a cloud environment - AWS Development/Delivery methodologies ‚Äì Agile, SDLC. Experience working in a geographically disparate team",,,"Python, SQL, Tableau",
4240638182,Data Engineer,Uplers,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job Data Engineer II Experience: 2 - 6 Years Exp Salary: Competitive Preferred Notice Period : Within 30 Days Shift : 9:30 AM to 6:30 PM IST Opportunity Type: Onsite (Mumbai) Placement Type: Permanent (*Note: This is a requirement for one of Uplers' Clients) Must have skills required : Kafka, Amazon AWS, GCP, Redshift, AWS Redshift Shaadi.com (One of Uplers' Clients) is Looking for: Data Engineer II who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you. Role Overview Description We are looking for an experienced Data Engineer having experience in building large-scale data pipelines and data lake ecosystems. Our daily work is around solving interesting and exciting problems against high engineering standards. Even though you will be a part of the backend team, you will be working with cross-functional teams across the org. This role demands good hands-on different programming languages, especially Python, and the knowledge of technologies like Kafka, AWS Glue, Cloudformation, ECS, etc. You will be spending most of your time on facilitating seamless streaming, tracking, and collaborating huge data sets. This is a back-end role, but not limited to it. You will be working closely with producers and consumers of the data and build optimal solutions for the organization. Will appreciate a person with lots of patience and data understanding. Also, we believe in extreme ownership! Design and build systems to efficiently move data across multiple systems and make it available for various teams like Data Science, Data Analytics, and Product. Design, construct, test, and maintain data management systems. Understand data and business metrics required by the product and architect the systems to make that data available in a usable/queryable manner. Ensure that all systems meet the business/company requirements as well as best industry practices. Keep ourselves abreast of new technologies in our domain. Recommend different ways to constantly improve data reliability and quality. Bachelors/Masters, Preferably in Computer Science or a related technical field. 2-5 years of relevant experience. Deep knowledge and working experience of Kafka ecosystem. Good programming experience, preferably in Python, Java, Go, and a willingness to learn more. Experience in working with large sets of data platforms. Strong knowledge of microservices, data warehouse, and data lake systems in the cloud, especially AWS Redshift, S3, and Glue. Strong hands-on experience in writing complex and efficient ETL jobs. Experience in version management systems (preferably with Git). Strong analytical thinking and communication. Passion for finding and sharing best practices and driving discipline for superior data quality and integrity. Intellectual curiosity to find new and unusual ways of how to solve data management issues. How to apply for this opportunity: Easy 3-Step Process: 1. Click On Apply! And Register or log in on our portal 2. Upload updated Resume & Complete the Screening Form 3. Increase your chances to get shortlisted & meet the client for the Interview! About Our Client: Shaadi.com, the world's No.1 Matchmaking platform, pioneered online personals at the turn of the century and has continued to lead the exciting space for 20 years. About Uplers: Our goal is to make hiring and getting hired reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant product and engineering job opportunities and progress in their career. (Note: There are many more opportunities apart from this on the portal.) So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!",,,Python,
4204032592,Data Engineer,Accenture in India,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Project Role : Data Engineer Project Role Description : Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems. Must have skills : Informatica Data Quality Good to have skills : NA Minimum 5 Year(s) Of Experience Is Required Educational Qualification : 15 years full time education Summary: As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Be involved in the end-to-end data management process. Roles & Responsibilities: Gather, process, and analyze data to generate meaningful insights that drive strategic decision-making. Design and implement efficient data collection systems to enhance accuracy and optimize workflow. Utilize statistical techniques to interpret data and create detailed reports that support business goals. Work closely with cross-functional teams to solve business challenges through data-driven solutions. Strong proficiency in SQL/SOQL for data querying and analysis. Professional & Technical Skills: - Must To Have Skills: Proficiency in Informatica Data Quality. - Strong understanding of data modeling and data architecture. - Experience with SQL and database management systems. - Hands-on experience with data integration tools. - Knowledge of data warehousing concepts. Additional Information: - The candidate should have a minimum of 5 years of experience in Informatica Data Quality. - This position is based at our Hyderabad office. - A 15 years full-time education is required. 15 years full time education",,,SQL,
4248487522,Data Engineer,Takeda,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job By clicking the ‚ÄúApply‚Äù button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda‚Äôs Privacy Notice and Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge. Job Description: The Future Begins Here: At Takeda, we are leading digital evolution and global transformation. By building innovative solutions and future-ready capabilities, we are meeting the need of patients, our people, and the planet. Bangalore, the city, which is India‚Äôs epicenter of Innovation, has been selected to be home to Takeda‚Äôs recently launched Innovation Capability Center. We invite you to join our digital transformation journey. In this role, you will have the opportunity to boost your skills and become the heart of an innovative engine that is contributing to global impact and improvement. At Takeda‚Äôs ICC we Unite in Diversity : Takeda is committed to creating an inclusive and collaborative workplace, where individuals are recognized for their backgrounds and abilities they bring to our company. We are continuously improving our collaborators journey in Takeda, and we welcome applications from all qualified candidates. Here, you will feel welcomed, respected, and valued as an important contributor to our diverse team. Job Summary : As a Data Engineer, you will be building and maintaining data systems and construct datasets that are easy to analyze and support Business Intelligence requirements as well as downstream systems. Develops and maintains scalable data pipelines and builds out new integrations using AWS native technologies to support continuing increases in data source, volume, and complexity. Collaborates with analytics and business teams to improve data models that feed business intelligence tools and dashboards, increasing data accessibility and fostering data-driven decision making across the organization. Implements processes and systems to drive data reconciliation, monitor data quality, ensuring production data is always accurate and available for key stakeholders, downstream systems, and business processes that depend on it. Writes unit/integration/performance test scripts, contributes to engineering wiki, and documents work. Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Works closely with a team of frontend and backend engineers, product managers, and analysts. Works with DevOps and Cloud Center of Excellence to deploy data pipeline solutions in Takeda AWS environments meeting security and performance requirements. Skills and Qualifications : Bachelors‚Äô Degree, from an accredited institution in Engineering, Computer Science, or related field. 3+ years of experience in software, data, data warehouse, data lake, and analytics reporting development. Strong experience in data/Big Data, data integration, data model, modern database (Graph, SQL, No-SQL, etc.) query languages and AWS cloud technologies including DMS, Lambda, Databricks, SQS, Step Functions, Data Streaming, Visualization, etc. Solid experience in DBA, dimensional modeling, SQL optimization - Aurora is preferred. Experience designing, building, maintaining data integrations using SOAP/REST web services/API, as well as schema design and dimensional data modeling. Excellent written and verbal communication skills including the What Takeda Can Offer You: Takeda is certified as a Top Employer, not only in India, but also globally. No investment we make pays greater dividends than taking good care of our people. At Takeda, you take the lead on building and shaping your own career. Joining the ICC in Bangalore will give you access to high-end technology, continuous training and a diverse and inclusive network of colleagues who will support your career growth. Benefits: It is our priority to provide competitive compensation and a benefit package that bridges your personal life with your professional career. Amongst our benefits are Competitive Salary + Performance Annual Bonus Flexible work environment, including hybrid working Comprehensive Healthcare Insurance Plans for self, spouse, and children Group Term Life Insurance and Group Accident Insurance programs Health & Wellness programs including annual health screening, weekly health sessions for employees. Employee Assistance Program Broad Variety of learning platforms Diversity, Equity, and Inclusion Programs No Meeting Days Reimbursements ‚Äì Home Internet & Mobile Phone Employee Referral Program Leaves ‚Äì Paternity Leave (4 Weeks) , Maternity Leave (up to 26 weeks), Bereavement Leave (5 days) About ICC in Takeda: Takeda is leading a digital revolution. We‚Äôre not just transforming our company; we‚Äôre improving the lives of millions of patients who rely on our medicines every day. As an organization, we are committed to our cloud-driven business transformation and believe the ICCs are the catalysts of change for our global organization. Locations: IND - Bengaluru Worker Type: Employee Worker Sub-Type: Regular Time Type: Full time",manager,,"SQL, Data Analysis",
4238096896,Data Engineer,Recro,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Role - Data Engineer Location - Bangalore Experience - 3-6 yrs Primary_skills(Must_have) SQL, Python, Documentation, PySpark in distributed environment, exposure to Data Management Services (DMS) on cloud (AWS/GCP/Azure), DWH/Data Modelling. Preferable but not mandatory: Lending/BFS domain knowledge",,,"Python, SQL",
4249414745,AWS Data Engineer,SourcingXPress,"Mumbai, Maharashtra, India (Remote)",Remote,‚Çπ800K/yr - ‚Çπ1.2M/yr,"Cloud Consulting & Data Analytics Salary Range: ‚Çπ 8-12 Lacs PA Job Description Mactores is a trusted leader among businesses in providing modern data platform solutions. Since 2008 , Mactores has been enabling businesses to accelerate their value through automation by providing End-to-End Data Solutions that are automated, agile, and secure. We collaborate with customers to strategize, navigate, and accelerate an ideal path forward with a digital transformation via assessments, migration, or modernization. As an AWS Data Engineer, you are a full-stack data engineer who loves solving business problems. You work with business leads, analysts, and data scientists to understand the business domain and engage with fellow engineers to build data products that empower better decision-making. You are passionate about the data quality of our business metrics and the flexibility of your solution that scales to respond to broader business questions. If you love to solve problems using your skills, then come join Team Mactores. We have a casual and fun office environment that actively steers clear of rigid ""corporate"" culture, focuses on productivity and creativity, and allows you to be part of a world-class team while still being yourself. What will you do? Write efficient code in - PySpark, Amazon Glue Write SQL Queries in - Amazon Athena, Amazon Redshift Explore new technologies and learn new techniques to solve business problems creatively Collaborate with many teams - engineering and business, to build better data products and services Deliver the projects along with the team collaboratively and manage updates to customers on time What are we looking for? 1 to 3 years of experience in Apache Spark, PySpark, Amazon Glue 2+ years of experience in writing ETL jobs using PySpark and Spark SQL 2+ years of experience in SQL queries and stored procedures Have a deep understanding of all the Dataframe API with all the transformation functions supported by Spark 2.7+ You Will Be Preferred If You Have Prior experience in working on AWS EMR, Apache Airflow Certifications AWS Certified Big Data ‚Äì Specialty, OR Cloudera Certified Big Data Enginee,r OR Hortonworks Certified Big Data Engineer Understanding of DataOps Engineering Desired Skills and Experience BigQuery, Amazon Redshift, AWS, AWS Glue, Data Engineering, Big Data, SQL Tuning, PySpark, ETL, Apache Spark Job search faster with Premium Access company insights like strategic priorities, headcount trends, and more Hitesh and millions of other members use Premium Retry Premium for ‚Çπ0 1-month free trial. Cancel whenever. We‚Äôll remind you 7 days before your trial ends. About the company SourcingXPress 23,769 followers Follow Technology, Information and Internet 11-50 employees 15 on LinkedIn Post a free Job > Sort with % match > View full profile > Liked the candidate > Download for free Welcome to SourcingXPress, where we‚Äôre building a future-ready hiring ecosystem designed for transparency, efficiency, and human connection. Our mission is simple: to create a seamless platform where employers, job seekers, and recruiters thrive together. From cutting-edge matchmaking algorithms that evaluate candidates to real-time updates for job seekers, we ensure clarity and empowerment at every step of the hiring process. What We Offer: For Employers: Save time with our advanced candidate-matching algorithms and make informed hiring decisions faster. For Job Seekers: Track every action recruiters take on your profile, from resume views to shortlisting or rejections‚Äîno more waiting in the dark. For Recruiters: Access a curated marketplace and tools to source the best talent efficiently. At SourcingXPress, we‚Äôre not just a hiring platform‚Äîwe‚Äôre a community-driven ecosystem focused on empowering people through meaningful connections and cutting-edge technology. Join us as we reshape the hiring journey‚Äîone connection at a time. Post a free Job ‚û°Ô∏è www.sourcingxpress.com Follow us for updates, job opportunities, and hiring insights! ‚Ä¶ show more Show more","About the job Company: Mactores Website: Visit Website Business Type: Small/Medium Business Company Type: Service Business Model: B2B Funding Stage: Bootstrapped Industry: Cloud Consulting & Data Analytics Salary Range: ‚Çπ 8-12 Lacs PA Job Description Mactores is a trusted leader among businesses in providing modern data platform solutions. Since 2008 , Mactores has been enabling businesses to accelerate their value through automation by providing End-to-End Data Solutions that are automated, agile, and secure. We collaborate with customers to strategize, navigate, and accelerate an ideal path forward with a digital transformation via assessments, migration, or modernization. As an AWS Data Engineer, you are a full-stack data engineer who loves solving business problems. You work with business leads, analysts, and data scientists to understand the business domain and engage with fellow engineers to build data products that empower better decision-making. You are passionate about the data quality of our business metrics and the flexibility of your solution that scales to respond to broader business questions. If you love to solve problems using your skills, then come join Team Mactores. We have a casual and fun office environment that actively steers clear of rigid ""corporate"" culture, focuses on productivity and creativity, and allows you to be part of a world-class team while still being yourself. What will you do? Write efficient code in - PySpark, Amazon Glue Write SQL Queries in - Amazon Athena, Amazon Redshift Explore new technologies and learn new techniques to solve business problems creatively Collaborate with many teams - engineering and business, to build better data products and services Deliver the projects along with the team collaboratively and manage updates to customers on time What are we looking for? 1 to 3 years of experience in Apache Spark, PySpark, Amazon Glue 2+ years of experience in writing ETL jobs using PySpark and Spark SQL 2+ years of experience in SQL queries and stored procedures Have a deep understanding of all the Dataframe API with all the transformation functions supported by Spark 2.7+ You Will Be Preferred If You Have Prior experience in working on AWS EMR, Apache Airflow Certifications AWS Certified Big Data ‚Äì Specialty, OR Cloudera Certified Big Data Enginee,r OR Hortonworks Certified Big Data Engineer Understanding of DataOps Engineering Desired Skills and Experience BigQuery, Amazon Redshift, AWS, AWS Glue, Data Engineering, Big Data, SQL Tuning, PySpark, ETL, Apache Spark",,,"SQL, R",
4235911374,Data Engineer III [T500-18134],McDonald's,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About McDonald‚Äôs: One of the world‚Äôs largest employers with locations in more than 100 countries, McDonald‚Äôs Corporation has corporate opportunities in Hyderabad. Our global offices serve as dynamic innovation and operations hubs, designed to expand McDonald's global talent base and in-house expertise. Our new office in Hyderabad will bring together knowledge across business, technology, analytics, and AI, accelerating our ability to deliver impactful solutions for the business and our customers across the globe. Who we‚Äôre looking for: Primary Responsibilities: Builds and maintains relevant and reliable Menu data products that support menu and marketing Analytics. Develops and implements new technology solutions as needed to ensure ongoing improvement with data reliability and observability in-view. Participates in new software development engineering and Lead data engineering initiatives supporting Product Mix Analytics, ensuring timely and accurate delivery of marketing and menu-related products. Work closely with the Product owner and help to define business rules that determines the quality of Menu datasets. Drive and implement best practices for pipeline development, data governance, data security and quality across marketing and menu-related datasets. Ensure scalability, maintainability, and quality of data systems powering menu item tracking, promotion data, and marketing analytics. Staying up to date with emerging data engineering technologies, trends, and best practices, and evaluating their applicability to meet evolving Product Mix analytics needs. Documenting data engineering processes, workflows, and solutions for knowledge sharing and future reference. Mentor and coach junior data engineers, particularly in areas related to menu item tracking, promotion data, and marketing analytics. Ability and flexibility to coordinate and work with teams distributed across time zones, as needed Skill: Leads teams to drive scalable data engineering practices and technical excellence within the Menu Data ecosystem. Bachelor's or master's degree in computer science or related engineering field and deep experience with Cloud computing 5+ years of professional experience in data engineering or related fields Proficiency in Python, Java, or Scala for data processing and automation Hands-on experience with data orchestration tools (e.g., Apache Airflow, Luigi) and big data ecosystems (e.g., Hadoop, Spark, NoSQL) Expert knowledge of Data quality functions like cleansing, standardization, parsing, de-duplication, mapping, hierarchy management, etc. Ability to perform extensive data analysis (comparing multiple datasets) using a variety of tools Proven ability to mentor team members and lead technical initiatives across multiple workstreams Effective communication and stakeholder management skills to drive alignment and adoption of data engineering standards Demonstrated experience in data management & data governance capabilities Familiarity with data warehousing principles and best practices. Excellent problem solver - use of data and technology to solve problems or answer complex data related questions Excellent collaboration skills to work effectively in cross-functional teams. Work location: Hyderabad, India Work hours: Work pattern: Full time role. Work mode: Hybrid.",,,"Python, Data Analysis",
4251613633,SQL or Python Developer,HRM Counsel Private Limited,India (Remote),Remote,Full-time,,"About the job We are looking for an experienced Python developer to join our engineering team and help us create dynamic software applications for our clients. In this role, you will be responsible for writing and testing scalable code, developing back-end components, and integrating user-facing elements in collaboration with front-end developers. To be successful as a Python developer, you should possess in-depth knowledge of object-relational mapping, experience with server-side logic, and above-average knowledge of Python programming. Ultimately, a top-class Python developer is able to design highly responsive web-applications that perfectly meet the needs of the client. Responsibilities: Coordinating with development teams to determine application requirements. Writing scalable code using Python programming language. Testing and debugging applications. Developing back-end components. Integrating user-facing elements using server-side logic. Assessing and prioritizing client feature requests. Integrating data storage solutions. Coordinating with front-end developers. Reprogramming existing databases to improve functionality. Developing digital tools to monitor online traffic. Requirements: Bachelor's degree in computer science, computer engineering, or related field. Expert knowledge of Python and related frameworks including Django and Flask. A deep understanding and multi-process architecture and the threading limitations of Python. Familiarity with server-side templating languages including Jinja 2 and Mako. Ability to integrate multiple data sources into a single system. Familiarity with testing tools. Ability to collaborate on projects and work independently when required.",Manager,,Python,
4235859084,Data Scientist/Data Engineer,Team Geek Solutions,"Pune, Maharashtra, India (On-site)",On-site,‚Çπ2M/yr - ‚Çπ2.4M/yr,,"About the job Title: Data Scientist/Data Engineer Experience: 8+ Years Location: Pune(Onsite) Notice Period: Immediate Joiner Data Engineering Job Responsibilities and Requirement: Design, build, and maintain scalable and robust data pipelines to support data integration and data warehousing. Develop and optimize ETL processes to ingest, clean, and transform data from various sources like Qualys, CMDB etc Ensure the reliability, availability, and performance of data systems. Data Management: Manage and maintain data architecture, data models, and data schemas. Implement and maintain data governance and data quality standards. Work with relational and NoSQL databases, ensuring data integrity and security. Performance Monitoring and Optimization: Monitor database performance and optimize query execution for maximum efficiency. Troubleshoot and resolve database-related issues. Data Integrity and Security: Ensure data integrity and security, including managing user access and permissions. Develop and implement backup and recovery procedures to minimize data loss in the event of hardware or software failure. Cloud-Based Database Management: Manage cloud-based databases on platforms like AWS, Azure, and Google Cloud Platform. Keep up-to-date with the latest PostgreSQL/MongoDB releases, features, and patches. Collaboration and Documentation: Collaborate with developers and other IT staff to ensure database systems meet business requirements. Document database processes and procedures. Additional Responsibilities: Evaluate business needs and objectives. Interpret trends and patterns. Must Have Qualification, Experience, Technical and Functional Skills Proficiency in PostgreSQL and MongoDB. Strong skills in Python and Ansible for development and automation. Familiarity with cloud-based database management (AWS, Azure, Google Cloud Platform). Experience in creating and managing databases, tables, and indexes. Strong understanding of database performance monitoring and optimization Good To Have Excellent problem-solving and troubleshooting skills. Strong collaboration and communication skills. Ability to document database processes and procedures clearly. Skills: postgresql,ansible,data engineer,etl processes,data governance,database performance monitoring,google cloud platform,troubleshooting,nosql,cloud-based databases,data scientist,python,mongodb,aws,cmdb,azure,etl,gcp,data quality",,,Python,
4250255589,Data Engineer,Teamware Solutions,"Chennai, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job This is a Data Engineer/Data Stewardship position for the C360 Ingestion and Stewardship team in GDIA to manage customer data ingestion for C360 Core Platform to build customer knowledge, purpose-built data products and make data available for enterprise users to generate insights, product development and AI capabilities leveraging Enterprise Data Platform (Google Cloud Platform) tools and services. This position will play a key role to land the data from diverse internal and external source system applications to Enterprise data lake by ensuring data quality, privacy, security, compliance and collaborating with business, data governance and EDP teams. Skills Required: Python, SQL, Google Cloud Platform, ETL, Tekton Skills Preferred: LLM, AI Experience Required: 7+ years of relevant experience",,,"Python, SQL",
4241504234,Data Science and Machine Learning engineer,Winbold Capital Platforms,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Company Description Winbold is a startup working on a AI driven investment platform. Job Description This requires immediate joining. Do not apply if you have notice period. We are looking for a very motivated Data Science and Machine Learning Engineer with hands-on experience in feature engineering, model training and building models that run and provide real-time predictions and forecasts. The candidate must be well versed with the following: ML Packages - sklean, xgboost, timeseries models Feature engineering and feature stores MLFlow for Model experimentation Model visualization Model deployment in Cloud and in hosted environments at scale For immediate consideration you can send email to me directly with a note on why you think you are the better suited for this role at madhu at winbolddatasystems.com Qualifications We dont care what degree you have or if you have one. We want a hands-on developer who can get shit done. Additional Information All your information will be kept confidential according to EEO guidelines.",,,Machine Learning,
4216787178,Ascendeum - Data Engineer - Python,Ascendeum,Greater Kolkata Area (On-site),On-site,Full-time,,"About the job This job is sourced from a job board. Learn More Job Responsibilities Identify valuable data sources and automate collection processes Undertake preprocessing of structured and unstructured data. Analyze large amounts of information to discover trends and patterns Helping develop reports and analysis. Present information using data visualisation techniques. Assessing tests, implementing new or upgraded software, and assisting with strategic decisions on new systems. Evaluating changes and updates to source production systems. Develop, implement, and maintain leading-edge analytic systems, taking complicated problems and building simple frameworks. Providing technical expertise in data storage structures, data mining, and data cleansing. Propose solutions and strategies to business Skills and Experience : Relevant 2+ years of experience in Data Analysis Complete understanding of Operations Research, Data Modelling, ML, and AI concepts. Knowledge of Python is mandatory, familiarity with MySQL, SQL, Scala, Java or C++ is an asset Experience using visualization tools (e.g. Jupyter Notebook) and data frameworks (e.g. Hadoop) Analytical mind and business acumen Strong math skills (e.g. statistics, algebra) Problem-solving aptitude Excellent communication and presentation skills. Bachelors / Master's Degree in Computer Science, Engineering, Data Science or other quantitative or relevant field is preferred (ref:hirist.tech)",,,"Python, SQL, Data Analysis",
3627616261,Data Engineer,SenecaGlobal,"Hyderabad, Telangana, India (On-site)",On-site,‚Çπ1.5M/yr - ‚Çπ2M/yr,,"About the job This job is sourced from a job board. Learn More Should have good experience with Python or Scala/PySpark/Spark/ Experience with Advanced SQL Experience with Azure data factory, data bricks, Experience with Azure IOT, Cosmos DB, BLOB Storage API management, FHIR API development, Proficient with Git and CI/CD best practices Experience working with Snowflake is a plus Skills:- Python, PySpark, Spark, Scala and Microsoft Azure Data factory",associate,,"Python, SQL",
4225887279,Data Engineer,Ascendion,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Looking for someone: Role: Data Engineer Senior : 7-10 Year Location: Bangalore/ Hyderabad/ Pune Skills: Python, SQL, PySpark, Azure Databricks, Data Pipelines",,,"Python, SQL",
4240034738,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You‚Äôll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you‚Äôll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you‚Äôll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour‚Äôs. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization. Strong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources. Proficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities. Hands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements Preferred Technical And Professional Experience Understanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling. Ability to implement robust data validation, cleansing, and governance frameworks within ETL processes. Proficiency in SQL and/or Shell scripting for custom transformations and automation tasks",,,"SQL, Machine Learning",
4224302943,AWS Data Engineer,Talent Worx,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Senior AWS Data Engineer Overview: We are seeking experienced AWS Data Engineers to design, implement, and maintain robust data pipelines and analytics solutions using AWS services. The ideal candidate will have a strong background in AWS data services, big data technologies, and programming languages. Exp- 3 to 7 years Location- Bangalore, Pune, Hyderabad, Coimbatore, Delhi NCR, Mumbai Requirements Key Responsibilities: Design and implement scalable, high-performance data pipelines using AWS services Develop and optimize ETL processes using AWS Glue, EMR, and Lambda Build and maintain data lakes using S3 and Delta Lake Create and manage analytics solutions using Amazon Athena and Redshift Design and implement database solutions using Aurora, RDS, and DynamoDB Develop serverless workflows using AWS Step Functions Write efficient and maintainable code using Python/PySpark, and SQL/PostgrSQL Ensure data quality, security, and compliance with industry standards Collaborate with data scientists and analysts to support their data needs Optimize data architecture for performance and cost-efficiency Troubleshoot and resolve data pipeline and infrastructure issues Required Qualifications: bachelor's degree in computer science, Information Technology, or related field Relevant years of experience as a Data Engineer, with at least 60% of experience focusing on AWS Strong proficiency in AWS data services: Glue, EMR, Lambda, Athena, Redshift, S3 Experience with data lake technologies, particularly Delta Lake Expertise in database systems: Aurora, RDS, DynamoDB, PostgreSQL Proficiency in Python and PySpark programming Strong SQL skills and experience with PostgreSQL Experience with AWS Step Functions for workflow orchestration Technical Skills: AWS Services: Glue, EMR, Lambda, Athena, Redshift, S3, Aurora, RDS, DynamoDB, Step Functions Big Data: Hadoop, Spark, Delta Lake Programming: Python, PySpark Databases: SQL, PostgreSQL, NoSQL Data Warehousing and Analytics ETL/ELT processes Data Lake architectures Version control: Git Agile methodologies",,,"Python, SQL",
4245678737,Data Engineer,Grid Dynamics,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job JOB DESCRIPTION: We are seeking a skilled Data Engineer & Data Analyst with over 4+ years of experience to design, build, and maintain scalable data pipelines and perform advanced data analysis to support business intelligence and data-driven decision-making. The ideal candidate will have a strong foundation in computer science principles, extensive experience with SQL and big data tools, and proficiency in cloud platforms and data visualization tools. Key Responsibilities: Design, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools. Build and optimize data architectures including data lakes and data warehouses. Integrate data from multiple sources ensuring data quality and consistency. Collaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions. Analyze complex datasets to identify trends, generate actionable insights, and support decision-making. Develop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation. Manage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB. Work with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow. Utilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery. Support CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools. Ensure data governance, security, and compliance standards are met. Participate in Agile and DevOps processes to enhance data engineering workflows. Required Qualifications: 4+ years of professional experience in data engineering and data analysis roles. Strong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB. Hands-on experience with big data tools like Hadoop and Apache Spark. Proficient in Python programming. Experience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks. Proven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi. Familiarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow. Experience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory). Understanding of data modeling concepts and data lake/data warehouse architectures. Experience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows. Knowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB. Exposure to Agile and DevOps methodologies. Experience with at least one cloud platform: Google Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub) Amazon Web Services (S3, Glue, Redshift, Lambda, Athena) Microsoft Azure (Data Factory, Synapse Analytics, Blob Storage) Preferred Skills: Strong problem-solving and communication skills. Ability to work independently and collaboratively in a team environment. Experience with service development, REST APIs, and automation testing is a plus. Familiarity with version control systems and workflow automation.",,,"Python, SQL, Tableau, Power BI, Data Analysis",
4098200188,Data Engineer,PayU,"Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job About PayU PayU, a leading payment and Fintech company in 50+ high-growth markets throughout Asia, Central and Eastern Europe, Latin America, the Middle East and Africa, part of Prosus group, one of the largest technology investors in the world is redefining the way people buy and sell online for our 300.000 + merchants and millions of consumers. As a leading online payment service provider, we deploy more than 400 payment methods and PCI-certified platforms to process approximately 6 million payments every single day. Thinking of becoming a PayUneer and you are curious to know more about us? Read more about the life in PayU here Role: Data Engineer Company: PayU Location: Gurgaon /Bangalore/ Mumbai About Company: PayU is the payments and fintech business of Prosus, a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. The leading online payment service provider in 36 countries, PayU is dedicated to creating a fast, simple and efficient payment process for merchants and buyers. Focused on empowering people through financial services and creating a world without financial borders where everyone can prosper, PayU is one of the biggest investors in the fintech space globally, with investments totalling $700 million- to date. PayU also specializes in credit products and services for emerging markets across the globe. We are dedicated to removing risks to merchants, allowing consumers to use credit in ways that suit them and enabling a greater number of global citizens to access credit services. Our local operations in Asia, Central and Eastern Europe, Latin America, the Middle East, Africa and South East Asia enable us to combine the expertise of high growth companies with our own unique local knowledge and technology to ensure that our customers have access to the best financial services. India is the biggest market for PayU globally and the company has already invested $400 million in this region in last 4 years. PayU in its next phase of growth is developing a full regional fintech ecosystem providing multiple digital financial services in one integrated experience. We are going to do this through 3 mechanisms: build, co-build/partner; select strategic investments. PayU supports over 350,000+ merchants and millions of consumers making payments online with over 250 payment methods and 1,800+ payment specialists. The markets in which PayU operates represent a potential consumer base of nearly 2.3 billion people and a huge growth potential for merchants. Job responsibilities: Design infrastructure for data, especially for but not limited to consumption in machine learning applications Define database architecture needed to combine and link data, and ensure integrity across different sources Ensure performance of data systems for machine learning to customer-facing web and mobile applications using cutting-edge open source frameworks, to highly available RESTful services, to back-end Java based systems Work with large, fast, complex data sets to solve difficult, non-routine analysis problems, applying advanced data handling techniques if needed Build data pipelines, includes implementing, testing, and maintaining infrastructural components related to the data engineering stack. Work closely with Data Engineers, ML Engineers and SREs to gather data engineering requirements to prototype, develop, validate and deploy data science and machine learning solutions Requirements to be successful in this role: Strong knowledge and experience in Python, Pandas, Data wrangling, ETL processes, Spark statistics, data visualization, Data Modelling and Informatica. Strong experience with scalable compute solutions such as in Kafka, Snowflake Strong experience with workflow management libraries and tools such as Airflow, AWS Step Functions etc. Strong experience with data engineering practices (i.e. data ingestion pipelines and ETL) A good understanding of machine learning methods, algorithms, pipelines, testing practices and frameworks B. Tech / BE /MEng/MSc/PhD degree in computer science, engineering, mathematics, physics, or equivalent (preference: DS/ AI) Experience with designing and implementing tools that support sharing of data, code, practices across organizations at scale About Us At PayU, we are a global fintech investor and our vision is to build a world without financial borders where everyone can prosper. We give people in high-growth markets the financial services and products they need to thrive. Our expertise in 18 high-growth markets enables us to extend the reach of financial services. This drives everything we do, from investing in technology entrepreneurs, to offering credit to underserved individuals, to helping merchants buy, sell and operate online. Being part of Prosus, one of the largest technology investors in the world, gives us the presence and expertise to make a real impact. Find out more www.payu.com Our Commitment To Building A Diverse And Inclusive Workforce As a global and multi-cultural organization with varied ethnicities thriving across locations, we realize that our responsibility towards fulfilling the D&I commitment is huge. Therefore, we continuously strive to create a diverse, inclusive and safe environment, for all of our people, communities and customers. Our leaders are committed to create an inclusive work culture which enables transparency, flexibility and unbiased attention to each and every PayUneer so they can succeed, irrespective of gender, color or personal faith. An environment where every person feels they belong, that they are listened to, and where they are empowered to speak up. At PayU we have zero tolerance towards any form of prejudice whether a specific race, ethnicity, or of persons with disabilities or the LGBTQ communities.",,,"Python, Machine Learning",
4243903147,Data Engineer,Astreya,India (Remote),Remote,Full-time,,"About the job Data Engineer Astreya offers comprehensive IT support and managed services. These services include Data Center and Network Management, Digital Workplace Services (like Service Desk, Audio Visual, and IT Asset Management), as well as Next-Gen Digital Engineering services encompassing Software Engineering, Data Engineering, and cybersecurity solutions. Astreya's expertise lies in creating seamless interactions between people and technology to help organizations achieve operational excellence and growth. Job Description We are seeking experienced Data Engineer to join our analytics division. You will be aligned with our Data Analytics and BI vertical. You will have to conceptualize and own the build out of problem-solving data marts for consumption by data science and BI teams, evaluating design and operational tradeoffs within systems. Design, develop, and maintain robust data pipelines and ETL processes using data platforms for the organization's centralized data warehouse. Create or contribute to frameworks that improve the efficacy of logging data, while working with the Engineering team to triage issues and resolve them. Validate data integrity throughout the collection process, performing data profiling to identify and comprehend data anomalies. Influence product and cross-functional (engineering, data science, operations, strategy) teams to identify data opportunities to drive impact. Requirements Experience & Education Bachelor's degree in Computer Science, Mathematics, a related field, or equivalent practical experience. 5 years of experience coding with SQL or one or more programming languages (e.g., Python, Java, R, etc.) for data manipulation, analysis, and automation 5 years of experience designing data pipelines (ETL) and dimensional data modeling for synchronous and asynchronous system integration and implementation. Experience in managing troubleshooting technical issues, and working with Engineering and Sales Services teams. Preferred qualifications: Master‚Äôs degree in Engineering, Computer Science, Business, or a related field. Experience with cloud-based services relevant to data engineering, data storage, data processing, data warehousing, real-time streaming, and serverless computing. Experience with experimentation infrastructure, and measurement approaches in a technology platform. Experience with data processing software (e.g., Hadoop, Spark) and algorithms (e.g., MapReduce, Flume).",manager,,"Python, SQL, R",
4230049576,Data Engineer,Uplers,"Noida, Uttar Pradesh, India (Remote)",Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4088114029,Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About Amgen Amgen harnesses the best of biology and technology to fight the world‚Äôs toughest diseases, and make people‚Äôs lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what‚Äôs known today. What You Will Do Let‚Äôs do this. Let‚Äôs change the world. In this vital role you will be responsible for designing, building, maintaining, analyzing, and interpreting data to provide actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and executing data initiatives and, visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture and ETL processes Design, develop, and maintain data solutions for data generation, collection, and processing Be a key team member that assists in design and development of the data pipeline Create data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems Contribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions Develop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency Implement data security and privacy measures to protect sensitive data Leverage cloud platforms (AWS preferred) to build scalable and efficient data solutions Collaborate and communicate effectively with product teams Collaborate with Data Architects, Business SMEs, and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions Identify and resolve complex data-related challenges Adhere to best practices for coding, testing, and designing reusable code/component Explore new tools and technologies that will help to improve ETL platform performance Participate in sprint planning meetings and provide estimations on technical implementation What We Expect Of You Master‚Äôs degree and 1 to 3 years of Computer Science, IT or related field experience OR Bachelor‚Äôs degree and 3 to 5 years of Computer Science, IT or related field experience OR Diploma and 7 to 9 years of Computer Science, IT or related field experience Basic Qualifications: Hands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL),Snowflake, workflow orchestration, performance tuning on big data processing Proficiency in data analysis tools (eg. SQL) Proficient in SQL for extracting, transforming, and analyzing complex datasets from relational data stores Experience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development Strong understanding of data modeling, data warehousing, and data integration concepts Proven ability to optimize query performance on big data platforms Preferred Qualifications: Experience with Software engineering best-practices, including but not limited to version control, infrastructure-as-code, CI/CD, and automated testing Knowledge of Python/R, Databricks, SageMaker, cloud data platforms Strong understanding of data governance frameworks, tools, and best practices. Knowledge of data protection regulations and compliance requirements (e.g., GDPR, CCPA) Professional Certifications: AWS Certified Data Engineer preferred Databricks Certificate preferred Soft Skills: Excellent critical-thinking and problem-solving skills Strong communication and collaboration skills Demonstrated awareness of how to function in a team setting Demonstrated presentation skills EQUAL OPPORTUNITY STATEMENT Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.",Associate,,"Python, SQL, R, Machine Learning, Data Analysis",
4230739372,Data Engineer,Postman,"Hyderabad, Telangana, India",,Full-time,,"About the job Who Are We? Postman is the world‚Äôs leading API platform, used by more than 40 million developers and 500,000 organizations, including 98% of the Fortune 500. Postman is helping developers and professionals across the globe build the API-first world by simplifying each step of the API lifecycle and streamlining collaboration‚Äîenabling users to create better APIs, faster. The company is headquartered in San Francisco and has an office in Bangalore, where it was founded. Postman is privately held, with funding from Battery Ventures, BOND, Coatue, CRV, Insight Partners, and Nexus Venture Partners. Learn more at postman.com or connect with Postman on X via @getpostman. P.S: We highly recommend reading The ""API-First World"" graphic novel to understand the bigger picture and our vision at Postman. The Opportunity The Data team enables all functions with the power of data and insights, allowing Postman to take high impact data-driven decisions with confidence. The Data Engineering team is responsible for building the performant, reliable, and secure data platform. We are looking for a passionate Data Engineer to join the Data Engineering Team. In this role, you will be responsible for designing and building Ingestion pipelines, you will closely work with Data Analysts and Engineering teams to help them reach insights faster. This position is based in Hyderabad (Hybrid). What You'll Do Design and build scalable data pipelines from internal and external sources to serve business needs. Continually make enhancements that improve data architecture, making it performant, secure, and easier to maintain (e.g., ETL pipelines in singer-taps, dbt transformations, reverse ETL flows, and BI tools performance). Collaborate with analysts and product teams to drive solutions and POC discussions. Refine and implement data governance policies in data stack. Actively participates in the code review process, design discussions, and constructively identifies problems and proposes solutions. About You 3-5 years of experience in Data Engineering at Postman‚Äôs scale. Advanced SQL and proficient with one or more programming languages (Python (preferred), JavaScript, Golang). Familiarity with cloud platforms (such as AWS (preferred), Azure, or Google Cloud) and their data-related services. Strong knowledge of data warehouse/ data lake fundamentals, especially on the performance aspects and data governance. Excellent problem-solving and analytical skills with the ability to translate business requirements into technical designs and detailed specifications. Nice to Have Prior experience in building data applications on top of Kubernetes (Most of our data applications run in EKS). Exposure to orchestration (preferably Airflow) and data quality tools. Worked on initiatives that use Generative AI to optimize Data Engineering processes. Experience in maintaining a large dbt installation (>4000 models) and building CI rules. Experience in building reverse ETL pipelines. What Else? In addition to Postman's pay-on-performance philosophy, and a flexible schedule working with a fun, collaborative team, Postman offers a comprehensive set of benefits, including full medical coverage, flexible PTO, wellness reimbursement, and a monthly lunch stipend. Along with that, our wellness programs will help you stay in the best of your physical and mental health. If you have little ones in your family, the creche allowance can help in supporting your work-life balance. Our frequent and fascinating team-building events will keep you connected, while our donation-matching program can support the causes you care about. We‚Äôre building a long-term company with an inclusive culture where everyone can be the best version of themselves. At Postman, we embrace a hybrid work model. For all roles based out of San Francisco Bay Area, Boston, Bangalore, Noida, Hyderabad, and New York, employees are expected to come into the office 3-days a week. We were thoughtful in our approach which is based on balancing flexibility and collaboration and grounded in feedback from our workforce, leadership team, and peers. The benefits of our hybrid office model will be shared knowledge, brainstorming sessions, communication, and building trust in-person that cannot be replicated via zoom. Our Values At Postman, we create with the same curiosity that we see in our users. We value transparency and honest communication about not only successes, but also failures. In our work, we focus on specific goals that add up to a larger vision. Our inclusive work culture ensures that everyone is valued equally as important pieces of our final product. We are dedicated to delivering the best products we can. Equal opportunity Postman is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Headhunters and recruitment agencies may not submit resumes/CVs through this website or directly to managers. Postman does not accept unsolicited headhunter and agency resumes. Postman will not pay fees to any third-party agency or company that does not have a signed agreement with Postman.",manager,,"Python, SQL",
4203767624,Data Engineer,Impetus,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job About Impetus Impetus Technologies is a digital engineering company focused on delivering expert services and products to help enterprises achieve their transformation goals. We solve the analytics, AI, and cloud puzzle, enabling businesses to drive unmatched innovation and growth. Founded in 1991, we are cloud and data engineering leaders providing solutions to fortune 100 enterprises, headquartered in Los Gatos, California, with development centers in NOIDA, Indore, Gurugram, Bengaluru, Pune, and Hyderabad with over 3000 global team members. We also have offices in Canada and collaborate with a number of established companies, including American Express, Bank of America, Capital One, Toyota, United Airlines, and Verizon. Experience- 3-8 years Location- Gurgaon & Bangalore Job Description You should have extensive production experience in GCP, Other cloud experience would be a strong bonus. - Strong background in Data engineering 2-3 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc. - Exposure to enterprise application development is a must Roles & Responsibilities Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Big Query, GCS - At least 4 of these Services. Good to have knowledge on Cloud Composer, Cloud SQL, Big Table, Cloud Function. Strong experience in Big Data technologies ‚Äì Hadoop, Sqoop, Hive and Spark including DevOPs. Good hands on expertise on either Python or Java programming. Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM. Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos. Ability to drive the deployment of the customers‚Äô workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations. Experience with technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities. Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies. Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.",,,"Python, SQL",
4249905315,Freelancer-AI/ML Engineer,APPIT Software Inc,India (Remote),Remote,Contract,,"About the job Design, develop, and implement AI and machine learning models and algorithms to solve complex business problems Analyze and preprocess large datasets, ensuring data quality for model training and deployment Collaborate with cross-functional teams to integrate AI solutions into existing systems and workflow Monitor, test, and optimize model performance, retraining as necessary to maintain accuracy and efficiency Stay updated with the latest advancements in AI/ML technologies and best practices to drive continuous innovation",,,Machine Learning,
4227983437,Data Integration Engineer,Lingaro,India (Remote),Remote,Full-time,"Pride, and Global Mentoring Program for female talents planning their careers in IT.","About the job Role: Data Integration Engineer Location: India US Shift: 7:30 PM to 3:30 AM IST About Lingaro : Lingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data. Since 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team . About Data Engineering : Data engineering involves the development of solutions for the collection, transformation, storage and management of data to support data-driven decision making and enable efficient data analysis by end users. It focuses on the technical aspects of data processing, integration, and delivery to ensure that data is accurate, reliable, and accessible in a timely manner. It also focuses on the scalability, cost-effectiveness, security, and supportability of the solution. Data engineering encompasses multiple toolsets and architectural concepts across on-premises and cloud stacks, including but not limited to data warehousing, data lakes, lake house, data mesh, and includes extraction, ingestion, and synchronization of structured and unstructured data across the data ecosystem. It also includes processing organization and orchestration, as well as performance optimization of data processin g. Requirement: A bachelor's or master‚Äôs degree in computer science, Information Systems, or a related field is typically required. Additional certifications in cloud are advantageous. Minimum of 4 years of experience in data engineering or a related field. Must have: ETL Tools, including Azure Data Factory, Azure Databricks, Data Lake - implementing data ingestion pipelines from multiple data sourcesDatabricks/Spark development. Very good knowledge of cloud data services, data warehousing, big data technologies, and data lakes. Especially Azure, DataBricksSQL - designing, building, and managing SQL Server databases in the Azure cloud Programming skills for data analysis (especially PySpark, SparkSQL, Python, SQL).Understanding of data visualization tools (e.g., Power BI). Task: Provide technical expertise and direction in data engineering, guiding the team in selecting appropriate tools, technologies, and methodologies. Stay updated with the latest advancements in data engineering and ensure the team follows best practices and industry standards. Collaborate with stakeholders to understand project requirements, define scope, and create project plans. Support project managers to ensure that projects are executed effectively, meeting timelines, budgets, and quality standards. Monitor progress, identify risks, and implement mitigation strategies. Align coding standards, conduct code reviews to ensure proper code quality level. Identify and introduce quality assurance processes for data pipelines and workflows. Optimize data processing and storage for performance, efficiency and cost savings. Evaluate and implement new technologies to improve data engineering processes on various aspects (CICD, Quality Assurance, Coding standards). Maintain technical documentation of the project, control validity and perform regular reviews of it. Ensure compliance with security standards and regulations. Why join us: Stable employment. On the market since 2008, 1300+ talents currently on board in 7 global sites. 100% remote. Flexibility regarding working hours. Full-time position Comprehensive online onboarding program with a ‚ÄúBuddy‚Äù from day 1. Cooperation with top-tier engineers and experts. Unlimited access to the Udemy learning platform from day 1. Certificate training programs. Lingarians earn 500+ technology certificates yearly. Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly. Grow as we grow as a company. 76% of our managers are internal promotions. A diverse, inclusive, and values-driven community. Autonomy to choose the way you work. We trust your ideas. Create our community together. Refer your friends to receive bonuses. Activities to support your well-being and health. Plenty of opportunities to donate to charities and support the environment.",manager,,"Python, SQL, Power BI, Data Analysis",
4248302893,"Senior Data Engineer, ITC",Nike,"Karnataka, India (On-site)",On-site,Full-time,,"About the job Who You‚Äôll Work With This role is part of the Nike‚Äôs Content Technology team within Consumer Product and Innovation (CP&I) organization, working very closely with the globally distributed Engineering and Product teams. This role will roll up to the Director Software Engineering based out of Nike India Tech Centre. Who We Are Looking For We are looking for experienced Technology focused and hands on Lead Engineer to join our team in Bengaluru, India. As a Senior Data Engineer, you will play a key role in ensuring that our data products are robust and capable of supporting our Data Engineering and Business Intelligence initiatives. A data engineer with 5+ years of experience working with cloud-native platforms. Advanced skills in SQL, PySpark, Apache Airflow (or similar workflow management tools), Databricks, and Snowflake. Deep understanding of Spark optimization, Delta Lake, and Medallion architecture. Strong experience in data modeling and data quality practices. Experience with Tableau for data validation and monitoring. Exposure to DevOps practices, CI/CD, Git, and security aspects. Effective mentorship and team collaboration skills. Strong communication skills, able to explain technical concepts clearly. Experience with Kafka or other real-time systems Preferred: Familiarity with ML/GenAI integration into pipelines. Databricks Data Engineer certification. What You‚Äôll Work On Own and optimize large-scale ETL/ELT pipelines and reusable frameworks. Collaborate with cross-functional teams to translate business requirements into technical solutions. Guide junior engineers through code reviews and design discussions. Monitor data quality, availability, and system performance. Lead CI/CD implementation and improve workflow automation.",Director,,"SQL, Tableau",
4239393931,Azure Data Engineer,WTW,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job Description Azure Data Engineer involved in all stages of the development life cycle, through requirements analysis, coding, unit testing, system testing, UAT and warranty-support Analyzing data sets and designing and coding stable and scalable data pipelined also integrating into existing data pipelines Ensure timely delivery of systems that meet functional requirements by performing comprehensive unit testing. Implement processes and tools to ensure data quality and reliability. Review, refactor, and debug code to identify and resolve defects, improve performance, and address technical debt. Provide technical support and troubleshooting assistance for developed systems, ensuring smooth operation and issue resolution. Collaborate with clients and stakeholders to understand business requirements and align technical solutions to meet their needs. Qualifications Graduate with Engineering and IT specialization",,,,
4248685393,Data Engineer,APEXX Global,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Overview: APEXX is pioneering payment orchestration, transforming the global payments landscape with cutting-edge technology and innovation. As a leader in Fintech, we're committed to excellence, driving forward-thinking solutions that empower businesses worldwide. As a Data Engineer, you will be responsible for architecting, developing, and maintaining our data pipelines and databases as an individual contributor. You will collaborate closely with Business Analysts, and other product stakeholders to ensure data availability, accuracy, and security. This role requires a deep understanding of data engineering principles, exceptional problem-solving skills, and a passion for working in a fast-paced & growing FinTech environment. About You: Innovative Thinker: You thrive on finding creative solutions to complex integration challenges, leveraging your ingenuity to overcome obstacles and drive innovation. Collaborative Team Player: You excel in collaborative environments, actively contributing your expertise and insights to cross-functional teams to achieve shared goals and objectives. Continuous Learner: You have a growth mindset and a passion for learning, staying abreast of emerging technologies, trends, and best practices in integration engineering and related domains Adaptable and Resilient: You thrive in dynamic, fast-paced environments, adapting quickly to changing priorities and requirements while maintaining a high level of resilience and composure under pressure. Problem Solver: You possess strong analytical and problem-solving skills, with the ability to dissect complex issues, identify root causes, and develop practical, effective solutions. Effective Communicator: You communicate with clarity, precision, and empathy, effectively conveying technical concepts and ideas to diverse audiences, both verbally and in writing. Key Responsibilities: Data Pipeline Development: Architect, build, and maintain scalable and efficient data pipelines, design and implement ETL (Extract, Transform, Load) processes to gather data from various sources and Ensure data quality, integrity, and reliability across the data infrastructure. Database Management: Develop, manage, and optimize databases and data warehouses, Implement strategies for database performance tuning and storage optimization and apply data security best practices to protect sensitive information. Collaboration and Support: Collaborate with different stakeholders in the Product and Development team to understand and meet data requirements, provide advanced support for data-related queries and issues and work closely with the engineering team to integrate data solutions into the overall architecture. Data Governance and Compliance: Implement and enforce data governance policies and procedures, ensure compliance with regulatory requirements and industry standard and lead initiatives to improve data quality and governance practices. Dashboard Management: Building and maintaining various interactive ad hoc dashboards across different environments according to the clients requirements and scheduling the automated generation of reports based on user groups. Qualifications and Technical Skills: Bachelor's or Master's degree in Computer Science, Engineering, Information Technology, or a related field. 5+ years of experience in data engineering or a related role. Proven experience in the fintech or payments domain is highly desirable. Expert proficiency in SQL and experience with relational databases (e.g.,MYSQL, PostgreSQL). Proficiency in Pyspark and Data Warehousing in Snowflake. Apache Airflow / AWS Glue Expertise in data visualization tools (e.g., Quicksight , Power BI , Tableau, etc). Extensive experience with data pipelines and ETL tools (e.g., Apache Nifi, Talend, Apache Airflow). Strong familiarity and knowledge of Cloud platform AWS and their data services. Good Programming skills in Python, Specially in designing serverless data pipelines in AWS Lambda. Benefits: Health: Insurance (Employee, spouse, and two children) optional cover for parents or in laws 15 personal days per year + Festival holidays. You can also take your birthday off as paid leave, and we offer a paid day off for any charity work you want to participate in. Enrichment: Learning and development training, with a yearly budget, to strengthen your career and progression, leadership in house training and monthly lunch and learn, where you eat lunch and learn with our guest presenter Social: Monthly team outings & incentives. Some of our recent ones include- Hackathon, Lunch and Learns and team building games. About Apexx APEXX is a dynamic Fintech scale-up founded in 2016 with the goal of creating cutting-edge payment technology. Our vision is to be the payment industry's most merchant-centric provider. Through our platform a merchant can connect via a simple API connection to the world's payment ecosystem, increasing conversion at lower cost and satisfying their entire payments needs. We excel at bringing transparency, efficiency, and competition to the payments market. The team is incredibly committed and enthusiastic about what we are building - top of the class payments solutions and a valuable business. This is infectious and creates a wonderful office atmosphere. We have a supportive, relaxed yet high-performing, and high trust culture. To learn more about APEXX and experience the dynamics of being part of our team, feel free to check out the provided media channels below: Linkedin: https://www.linkedin.com/company/apexxglobal/ Instagram: https://www.instagram.com/apexx.global/?hl=en Careers Site: https://careers.apexx.global/ Main Website: https://apexx.global/ APEXX is an equal opportunities employer committed to encouraging equality, diversity, and inclusion among our employees and eliminating discrimination. We do not accept speculative CVs or candidate profiles from Recruitment Agencies.",,,"Python, SQL, Excel, Tableau, Power BI",
4250529802,Lead Consultant Data Engineer,Thoughtworks,"Bengaluru, Karnataka, India (Remote)",Remote,Full-time,,"About the job Lead data engineers at Thoughtworks develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. They might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On projects, they will be leading the design of technical solutions, or perhaps overseeing a program inception to build a new product. Alongside hands-on coding, they are leading the team to implement the solution. Job responsibilities You will lead and manage data engineering projects from inception to completion, including goal-setting, scope definition and ensuring on-time delivery with cross team collaboration You will collaborate with stakeholders to understand their strategic objectives and identify opportunities to leverage data and data quality You will design, develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions You will be responsible to create, design and develop intricate data processing pipelines, addressing clients' most challenging problems You will collaborate with data scientists to design scalable implementations of their models You write clean and iterative code based on TDD and leverage various continuous delivery practices to deploy, support and operate data pipelines You will lead and advise clients on how to use different distributed storage and computing technologies from the plethora of options available You will develop data models by selecting from a variety of modeling techniques and implementing the chosen data model using the appropriate technology stack You will be responsible for data governance, data security and data privacy to support business and compliance requirements You will define the strategy for and incorporate data quality into your day-to-day work Job Qualifications Technical Skills You have experience in leading the system design and implementation of technical solutions Working with data excites you; You have created Big Data architecture, can build and operate data pipelines, and maintain data storage, all within distributed systems You have a deep understanding of data modeling and experience with modern data engineering tools and platforms You have experience in writing clean, high-quality code using the preferred programming language You have built and deployed large-scale data pipelines and data-centric applications using any of the distributed storage platforms and distributed processing platforms in a production setting You have experience with data visualization techniques and can communicate the insights as per the audience You have experience with data-driven approaches and can apply data security and privacy strategy to solve business problems You have experience with different types of databases (i.e.: SQL, NoSQL, data lake, data schemas, etc.) Professional Skills You understand the importance of stakeholder management and can easily liaise between clients and other key stakeholders throughout projects, ensuring buy-in and gaining trust along the way You are resilient in ambiguous situations and can adapt your role to approach challenges from multiple perspectives You don‚Äôt shy away from risks or conflicts, instead you take them on and skillfully manage them You coach, mentor and motivate others and you aspire to influence teammates to take positive action and accountability for their work You enjoy influencing others and always advocate for technical excellence while being open to change when needed You are a proven leader with a track record of encouraging teammates in their professional development and relationships Cultivating strong partnerships comes naturally to you; You understand the importance of relationship building and how it can bring new opportunities to our business Other things to know Learning & Development There is no one-size-fits-all career path at Thoughtworks: however you want to develop your career is entirely up to you. But we also balance autonomy with the strength of our cultivation culture. This means your career is supported by interactive tools, numerous development programs and teammates who want to help you grow. We see value in helping each other be our best and that extends to empowering our employees in their career journeys. About Thoughtworks Thoughtworks is a global technology consultancy that integrates strategy, design and engineering to drive digital innovation. For 30+ years, our clients have trusted our autonomous teams to build solutions that look past the obvious. Here, computer science grads come together with seasoned technologists, self-taught developers, midlife career changers and more to learn from and challenge each other. Career journeys flourish with the strength of our cultivation culture, which has won numerous awards around the world. Join Thoughtworks and thrive. Together, our extra curiosity, innovation, passion and dedication overcomes ordinary.",,,SQL,
4237412741,AWS Data Engineer,Straive,"Noida, Uttar Pradesh, India (Remote)",Remote,Full-time,,"About the job About Straive: Straive is a market leading Content and Data Technology company providing data services, subject matter expertise, & technology solutions to multiple domains. Data Analytics & Al Solutions, Data Al Powered Operations and Education & Learning form the core pillars of the company‚Äôs long-term vision. The company is a specialized solutions provider to business information providers in finance, insurance, legal, real estate, life sciences and logistics. Straive continues to be the leading content services provider to research and education publishers. Data Analytics & Al Services: Our Data Solutions business has become critical to our client's success. We use technology and Al with human experts-in loop to create data assets that our clients use to power their data products and their end customers' workflows. As our clients expect us to become their future-fit Analytics and Al partner, they look to us for help in building data analytics and Al enterprise capabilities for them. With a client-base scoping 30 countries worldwide, Straive‚Äôs multi-geographical resource pool is strategically located in eight countries - India, Philippines, USA, Nicaragua, Vietnam, United Kingdom, and the company headquarters in Singapore. Website: https://www.straive.com/ Overview /Objective: We is looking for a Data Engineer with experience in building modern data platforms from the ground up. The successful candidate will build and maintain cloud-centric data processing capabilities that unleash the value of the League‚Äôs data assets to gain competitive advantage in the marketplace. They will be a hands-on contributor in the design and implementation of our cloud data platform that powers advanced analytics workloads. The Data Engineer will work in an agile environment and will be responsible for building and maintaining data integration, ingestion, curation and pipeline orchestration capabilities. They are comfortable challenging assumptions to improve existing solutions and ensure the team is building the best scalable and cost-efficient product. Responsibilities: Develop, test, and deploy software to generate data assets (relational, non-relational) for use by downstream BI engineers and data scientists Work with big data and cloud technologies such as EC2, Lambda, AWS Glue, Airflow, dbt, Redshift etc. Work closely with stakeholders to ensure successful data asset design and development Create software artifacts and patterns for reuse within the Data Engineering team. Ensure data pipelines are scalable, resilient and produced with the highest quality standards, metadata and validated for completeness and accuracy Work on a cross-functional Agile team responsible for end-to-end delivery of business needs Help improve data management processes - acquiring, transforming and storing massive volumes of structured and unstructured data Work closely with development teams to learn about needs, current processes and to promote best practices. Required Qualifications: University degree in Computer Science, Mathematics, Engineering, or related field. 5+ years of experience in software engineering with strong focus on data. Experience working with Cloud data platforms, preferably AWS (Lambda, Step Functions, S3, AWS Glue, Athena, Redshift). An expert in Python and SQL, including query optimization for relational, NoSQL and columnar databases. Sound knowledge of CI workflows and build/test/deploy automation. Strong understanding of data modelling concepts and best practices. Relevant experience with IaC (Terraform, CloudFormation) Relevant experience with modern big data processing and orchestration tools such as dbt and Airflow. A great teammate and self-starter, strong detail orientation is critical in this role .",,,"Python, SQL",
4250224960,Cognitive Data and Analytics Data Engineer,Dell Technologies,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Data Engineering Advisor Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand new methodologies, tools, statistical methods and models. What‚Äôs more, we are in collaboration with leading academics, industry experts and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data. Join us to do the best work of your career and make a profound social impact as a Data Engineering Advisor on our Data Engineering Team in Bangalore /Hyderabad/Gurgaon. What You‚Äôll Achieve As a Data Engineer, you will build leading edge AI-Fueled, Data-Driven business solutions within Services Parts, Services Repair and Services Engineering Space. You will: Interact with business leaders and internal customers to create and establish design standards and assurance processes for software, systems and applications development to ensure compatibility and operability. Analyze business goals, defines project scope and identifies functional and technical requirements to produce accurate business insights. Develop fundamentally new approaches to generate meaning from data, creating specifications for reports and analysis based on business needs. Support existing AI and Analytics products by ensuring optimal operations and enhance for new functionalities. Take the first step towards your dream career Every Dell Technologies team member brings something unique to the table. Here‚Äôs what we are looking for with this role: Essential Requirements Uses proactive outcomes-driven mindset Uses detailed analytical data skills Uses hands on experience developing coding data and analytics solutions Uses hands on expert knowledge in SQL Uses hands on expert knowledge in Python, Java, or other modern programming language Desirable Requirements Teradata macro programming Data management concepts Who We Are We believe that each of us has the power to make an impact. That‚Äôs why we put our team members at the center of everything we do. If you‚Äôre looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we‚Äôre looking for you. Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us. Application closing date: 30th June '25 Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here . Job ID: R269703",,,"Python, SQL",
4230137412,Data Research - Database Engineer,Forbes Advisor,"Mumbai, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Responsibilities Job Description Design, develop, and maintain the database infrastructure to store and manage company data efficiently and securely. Work with databases of varying scales, including small-scale databases, and databases involving big data processing. Work on data security and compliance, by implementing access controls, encryption, and compliance standards. Collaborate with cross-functional teams to understand data requirements and support the design of the database architecture. Migrate data from spreadsheets or other sources to a relational database system (e.g., PostgreSQL, MySQL) or cloud-based solutions like Google BigQuery. Develop import workflows and scripts to automate the data import process and ensure data accuracy and consistency. Optimize database performance by analyzing query execution plans, implementing indexing strategies, and improving data retrieval and storage mechanisms. Work with the team to ensure data integrity and enforce data quality standards, including data validation rules, constraints, and referential integrity. Monitor database health and identify and resolve issues. Collaborate with the full-stack web developer in the team to support the implementation of efficient data access and retrieval mechanisms. Implement data security measures to protect sensitive information and comply with relevant regulations. Demonstrate creativity in problem-solving and contribute ideas for improving data engineering processes and workflows. Embrace a learning mindset, staying updated with emerging database technologies, tools, and best practices. Explore third-party technologies as alternatives to legacy approaches for efficient data pipelines. Familiarize yourself with tools and technologies used in the team's workflow, such as Knime for data integration and analysis. Use Python for tasks such as data manipulation, automation, and scripting. Collaborate with the Data Research Engineer to estimate development efforts and meet project deadlines. Assume accountability for achieving development milestones. Prioritize tasks to ensure timely delivery, in a fast-paced environment with rapidly changing priorities. Collaborate with and assist fellow members of the Data Research Engineering Team as required. Perform tasks with precision and build reliable systems. Leverage online resources effectively like StackOverflow, ChatGPT, Bard, etc., while considering their capabilities and limitations. Skills And Experience Bachelor's degree in Computer Science, Information Systems, or a related field is desirable but not essential. Experience with data warehousing concepts and tools (e.g., Snowflake, Redshift) to support advanced analytics and reporting, aligning with the team‚Äôs data presentation goals. Skills in working with APIs for data ingestion or connecting third-party systems, which could streamline data acquisition processes. Proficiency with tools like Prometheus, Grafana, or ELK Stack for real-time database monitoring and health checks beyond basic troubleshooting. Familiarity with continuous integration/continuous deployment (CI/CD) tools (e.g., Jenkins, GitHub Actions). Deeper expertise in cloud platforms (e.g., AWS Lambda, GCP Dataflow) for serverless data processing or orchestration. Knowledge of database development and administration concepts, especially with relational databases like PostgreSQL and MySQL. Knowledge of Python programming, including data manipulation, automation, and object-oriented programming (OOP), with experience in modules such as Pandas, SQLAlchemy, gspread, PyDrive, and PySpark. Knowledge of SQL and understanding of database design principles, normalization, and indexing. Knowledge of data migration, ETL (Extract, Transform, Load) processes, or integrating data from various sources. Knowledge of cloud-based databases, such as AWS RDS and Google BigQuery. Eagerness to develop import workflows and scripts to automate data import processes. Knowledge of data security best practices, including access controls, encryption, and compliance standards. Strong problem-solving and analytical skills with attention to detail. Creative and critical thinking. Strong willingness to learn and expand knowledge in data engineering. Familiarity with Agile development methodologies is a plus. Experience with version control systems, such as Git, for collaborative development. Ability to thrive in a fast-paced environment with rapidly changing priorities. Ability to work collaboratively in a team environment. Good and effective communication skills. Comfortable with autonomy and ability to work independently. Qualifications 5+ Years exp in Database Engineering. Additional Information Perks Day off on the 3rd Friday of every month (one long weekend each month) Monthly Wellness Reimbursement Program to promote health well-being Monthly Office Commutation Reimbursement Program Paid paternity and maternity leaves",,,"Python, SQL",
4055648130,Data Engineer,RxLogix Corporation,"Gautam Buddha Nagar, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job This job is sourced from a job board. Learn More Date: 11th October 2024 Job Status: Full Time Job Location: Noida, India Job Summary We are seeking a skilled Data Engineer with 2-5 years of industry experience to join our dynamic team. The ideal candidate will have a strong background in data engineering and a keen interest in the drug safety or pharmacovigilance domain. You will be responsible for designing, developing, and maintaining data pipelines and infrastructure to support our data-driven initiatives. Key Responsibilities Design, develop, and maintain scalable data pipelines and ETL processes. Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions. Ensure data quality and integrity through rigorous testing and validation. Optimize and tune data processing systems for performance and scalability. Implement data governance and security best practices. Work with large datasets from various sources, including clinical trials, adverse event reports, and other pharmacovigilance data. Develop and maintain documentation for data engineering processes and systems. Qualifications Bachelors degree in computer science, Engineering, or a related field. 2-5 years of experience in data engineering or a related role. Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL, Oracle RDBMS). Experience with big data technologies (e.g., Hadoop, Spark). Familiarity with data warehousing solutions (e.g., Redshift, Big Query). Strong programming skills in Python, Java, or Scala. Experience with cloud platforms (e.g., AWS, Azure, GCP). Knowledge of data modeling, ETL processes, and data integration. Experience in the drug safety or pharmacovigilance domain is a plus. Excellent problem-solving skills and attention to detail. Strong communication and collaboration skills.",,,"Python, SQL",
4250198909,Data Engineer ( Immediate Joiner),Recro,"Mumbai, Maharashtra, India (On-site)",On-site,Full-time,,"About the job We are looking for an experienced Data Engineer having experience in building large-scale data pipelines and data lake ecosystems. Our daily work is around solving interesting and exciting problems against high engineering standards. Even though you will be a part of the backend team, you will be working with cross-functional teams across the org. This role demands good hands-on different programming languages, especially Python, and the knowledge of technologies like Kafka, AWS Glue, Cloudformation, ECS, etc. You will be spending most of your time on facilitating seamless streaming, tracking, and collaborating huge data sets. This is a back-end role, but not limited to it. You will be working closely with producers and consumers of the data and build optimal solutions for the organization. Will appreciate a person with lots of patience and data understanding. Also, we believe in extreme ownership! Design and build systems to efficiently move data across multiple systems and make it available for various teams like Data Science, Data Analytics, and Product. Design, construct, test, and maintain data management systems. Understand data and business metrics required by the product and architect the systems to make that data available in a usable/queryable manner. Ensure that all systems meet the business/company requirements as well as best industry practices. Keep ourselves abreast of new technologies in our domain. Recommend different ways to constantly improve data reliability and quality. Bachelors/Masters, Preferably in Computer Science or a related technical field. 2-5 years of relevant experience. Deep knowledge and working experience of Kafka ecosystem. Good programming experience, preferably in Python, Java, Go, and a willingness to learn more. Experience in working with large sets of data platforms. Strong knowledge of microservices, data warehouse, and data lake systems in the cloud, especially AWS Redshift, S3, and Glue. Strong hands-on experience in writing complex and efficient ETL jobs. Experience in version management systems (preferably with Git). Strong analytical thinking and communication. Passion for finding and sharing best practices and driving discipline for superior data quality and integrity. Intellectual curiosity to find new and unusual ways of how to solve data management issues.",,,Python,
4244771304,PySpark Data Engineer,Viraaj HR Solutions Private Limited,"Bhubaneswar, Odisha, India (On-site)",On-site,‚Çπ1.2M/yr - ‚Çπ2M/yr,,"About the job Company Overview Viraaj HR Solutions is a leading talent management firm dedicated to providing exceptional human resource services for businesses in various sectors. Our mission is to connect companies with the best talent while fostering a supportive and inclusive workplace culture. We value innovation, integrity, and excellence, ensuring that our clients thrive in a competitive market. Role Overview We are seeking a skilled PySpark Data Engineer to join our dynamic team in India. The ideal candidate will have a strong background in data engineering, with expertise in processing large volumes of data using PySpark. You will play a crucial role in designing and implementing data pipelines and ensuring efficient data storage and retrieval. This position requires strong analytical skills and the ability to work collaboratively in a fast-paced environment. Role Responsibilities Design, develop, and maintain robust data pipelines using PySpark. Implement ETL processes for data extraction, transformation, and loading. Work with structured and unstructured data sources and ensure data quality. Optimize data processing workflows for better performance and efficiency. Collaborate with data scientists and analysts to understand data requirements. Create and manage data warehousing solutions for end-user access. Integrate data from various sources into a unified platform. Monitor and troubleshoot data pipeline performance issues. Conduct data validation and cleansing to maintain data accuracy. Document processes and workflows for team transparency. Stay updated on new technologies and trends in data engineering. Participate in code reviews and ensure best practices in data engineering. Assist in technical designs and architecture for data solutions. Provide technical support and training to junior staff. Contribute to strategic discussions around data architecture and management. Qualifications Bachelor's degree in Computer Science, Engineering, or related field. Proven experience as a Data Engineer or similar role. Strong proficiency in PySpark and the Spark ecosystem. Hands-on experience with SQL and database management systems. Familiarity with Hadoop and big data technologies. Experience with cloud platforms like AWS, Azure, or Google Cloud. Understanding of data modeling and database design concepts. Proficient in Python for data manipulation and analysis. Strong analytical and troubleshooting skills. Excellent communication and teamwork abilities. Knowledge of data governance and compliance practices. Experience in implementing ETL and data warehousing solutions. Able to work in a fast-paced environment and manage multiple priorities. Familiarity with version control systems (e.g., Git). Willingness to learn and adapt to new technologies. This is an exciting opportunity to work with a forward-thinking company that values expertise and innovation in the field of data engineering. If you are passionate about data and eager to contribute to transformative projects, we encourage you to apply today. Skills: big data technologies,etl,database management systems,aws glue,problem-solving skills,google cloud,version control systems,sql,data modeling,aws,azure,pyspark,data engineer,data warehousing,data governance,python,git,hadoop",,,"Python, SQL",
4250153336,GCP Data Engineer,Tata Consultancy Services,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Required Technical Skill Set** Bigquery, Bigtable, Dataproc, Dataprep Proven experience as a Subject Matter Expert in Google Cloud Storage or a similar role. In-depth knowledge of Google Cloud Platform (GCP) and its storage tools. Strong Data management and Data Storage capacity planning skills. Excellent communication and presentation skills. Familiarity with cloud-native architectures and storage best practices. Good to have: Google Cloud Professional Cloud Architect or Google Cloud Professional Data Engineer certification. Experience with data migration and optimization. Knowledge of data security and compliance best practices.",,,,
4232793765,Data Engineer,Experian,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Company Description Experian is a global data and technology company, powering opportunities for people and businesses around the world. We help to redefine lending practices, uncover and prevent fraud, simplify healthcare, create marketing solutions, and gain deeper insights into the automotive market, all using our unique combination of data, analytics and software. We also assist millions of people to realize their financial goals and help them save time and money. We operate across a range of markets, from financial services to healthcare, automotive, agribusiness, insurance, and many more industry segments. We invest in people and new advanced technologies to unlock the power of data. As a FTSE 100 Index company listed on the London Stock Exchange (EXPN), we have a team of 22,500 people across 32 countries. Our corporate headquarters are in Dublin, Ireland. Learn more at experianplc.com. Job Description The Data Engineer will be responsible for development of data software applications, data platforms and data products. This Engineer will closely work with application engineers, data engineers, data scientists and CRM to design, document, develop, test and implement solutions. This engineer will work with other engineers, and individually contribute to the development including defining engineering standards, data analysis, data modeling, coding, testing/QA, code review, deployment, operational readiness, and agile management. Be a key contributor to deliver, on budget, high value complex projects Take technical responsibility for all stages and/or iterations in a software component Specify and ensure the design and development of technology solutions properly fulfills all business requirements Ensure project stakeholders receive regular communications about status of the work Anticipate change management requirements and ensure effective solution adoption by ensuring appropriate knowledge transfer, training and deployment readiness Document design approaches, code artifacts, communicate to and participate in knowledge sharing Provide technical guidance to associates, colleagues or customers Communicate difficult concepts and negotiate with others to adopt a different point of view Interpret internal/external business challenges and recommend best-practices to improve products, processes, or services Ability to work in multi-project environment and support multiple internal departments You will report to Manager Qualifications 5 to 7 years of experience as an engineer in data platform centric environments Extensive knowledge of data platform paradigms and software architecture Experience with cloud development on the Amazon Web Services (AWS) platform with services including API Gateway, Lambda, EC2, ECS, SQS, SNS, Dynamo DB, Redshift and Aurora Expert-level Pyspark, Python & SQL skills Ability to comprehend and implement detailed project specifications as well as the ability to adapt to various technologies and simultaneously work on multiple projects Proficiency in CI/CD tools (Jenkins, GitLab, etc.) Additional Information Our uniqueness is that we celebrate yours. Experian's culture and people are important differentiators. We take our people agenda very seriously and focus on what matters; DEI, work/life balance, development, authenticity, collaboration, wellness, reward & recognition, volunteering... the list goes on. Experian's people first approach is award-winning; World's Best Workplaces‚Ñ¢ 2024 (Fortune Top 25), Great Place To Work‚Ñ¢ in 24 countries, and Glassdoor Best Places to Work 2024 to name a few. Check out Experian Life on social or our Careers Site to understand why. Experian is proud to be an Equal Opportunity and Affirmative Action employer. Innovation is an important part of Experian's DNA and practices, and our diverse workforce drives our success. Everyone can succeed at Experian and bring their whole self to work, irrespective of their gender, ethnicity, religion, colour, sexuality, physical ability or age. If you have a disability or special need that requires accommodation, please let us know at the earliest opportunity. Benefits Experian care for employee's work life balance, health, safety and wellbeing. In support of this endeavor, we offer the best family well-being benefits, Enhanced medical benefits and paid time off. Experian Careers - Creating a better tomorrow together Find out what its like to work for Experian by clicking here",associate,,"Python, SQL, Data Analysis",
4246782032,Senior Data Engineer,Epsilon,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job About the job This position in the Engineering team under the Digital Experience organization. We drive the first mile of the customer experience through personalization of offers and content. We are currently on the lookout for a smart, highly driven engineer. You will be part of a team that is focused on building & managing solutions, pipelines using marketing technology stacks. You will also be expected to Identify and implement improvements including for optimizing data delivery and automate processes/pipelines. The incumbent is also expected to partner with various stakeholders, bring scientific rigor to design and develop high quality solutions. Candidate must have excellent verbal and written communication skills and be comfortable working in an entrepreneurial, ‚Äòstartup‚Äô environment within a larger company. Click here to view how Epsilon transforms marketing with 1 View, 1 Vision and 1 Voice. Brief Description of Role: Experience with both structured and unstructured data Experience working on AdTech or MarTech technologies. Experience in relational and non-relational databases and SQL (NoSQL is a plus). Understanding of Data Modeling, Data Catalog concepts and tools Ability to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc. Collaborate with other members of the team to ensure high quality deliverables Learning and implementing the latest design patterns in data engineering Data Management Experience with both structured and unstructured data Experience building Data and CI/CD pipelines Experience working on AdTech or MarTech technologies is added advantage Experience in relational and non-relational databases and SQL (NoSQL is a plus). Hands on experience building ETL workflows/pipelines on large volumes of data Good understanding of Data Modeling, Data Warehouse, Data Catalog concepts and tools Able to identify, join, explore, and examine data from multiple disparate sources and formats Ability to reduce large quantities of unstructured or formless data and get it into a form in which it can be analyzed Ability to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc. Development Ability to write code in programming languages such as Python and shell script on Linux Familiarity with development methodology such as Agile/Scrum Love to learn new technologies, keep abreast of the latest technologies within the cloud architecture, and drive your organization to adapt to emerging best practices Good knowledge of working in UNIX/LINUX systems Qualifications Bachelor‚Äôs degree in computer science with 2+ years of similar experience Tech Stack: Python, SQL, Scripting language (preferably JavaScript) Experience or knowledge on Adobe Experience Platform (RT-CDP/AEP) Experience working in Cloud Platforms (GCP or AWS) Familiarity with automated unit/integration test frameworks Good written and spoken communication skills, team player. Strong analytic thought process and ability to interpret findings In addition, the candidate should have strong business acumen, and interpersonal and communication skills, yet also be able to work independently. He/she should be able to communicate findings and the way techniques work in a manner that all stakeholders, both technical and non-technical, will understand. Epsilon is a global data, technology and services company that powers the marketing and advertising ecosystem. For decades, we‚Äôve provided marketers from the world‚Äôs leading brands the data, technology and services they need to engage consumers with 1 View, 1 Vision and 1 Voice. 1 View of their universe of potential buyers. 1 Vision for engaging each individual. And 1 Voice to harmonize engagement across paid, owned and earned channels. Epsilon‚Äôs comprehensive portfolio of capabilities across our suite of digital media, messaging and loyalty solutions bridge the divide between marketing and advertising technology. We process 400+ billion consumer actions each day using advanced AI and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Epsilon is a global company with more than 9,000 employees around the world. Epsilon has a core set of 5 values that define our culture and guide us to create value for our clients, our people and consumers. We are seeking candidates that align with our company values, demonstrate them and make them meaningful in their day-to-day work: Act with integrity. We are transparent and have the courage to do the right thing. Work together to win together. We believe collaboration is the catalyst that unlocks our full potential. Innovate with purpose. We shape the market with big ideas that drive big outcomes. Respect all voices. We embrace differences and foster a culture of connection and belonging. Empower with accountability. We trust each other to own and deliver on common goals. Additional Information- Because You Matter YOUniverse. A work-world with you at the heart of it! At Epsilon, we believe people make the place. And everything we do is designed with you in mind. That‚Äôs why our work-world, aptly named ‚ÄòYOUniverse‚Äô is focused on creating a nurturing environment that elevates your growth, wellbeing and work-life harmony. So, come be part of a people-centric workspace where care for you is at the core of all we do. Take a trip to YOUniverse and explore our unique benefits, here Epsilon is an Equal Opportunity Employer. Epsilon is committed to promoting diversity, inclusion, and equal employment opportunities by using reasonable efforts to attract, recruit, engage and retain qualified individuals of all ethnicities and backgrounds, including, but not limited to, women, people of color, LGBTQ individuals, people with disabilities and any other underrepresented groups, traits or characteristics.",,,"Python, SQL",
4236366596,Data Engineer,Impact Analytics,"Bangalore Urban, Karnataka, India (On-site)",On-site,Full-time,,"About the job About the Company Impact Analytics‚Ñ¢ ( Series D Funded ) delivers AI-native SaaS solutions and consulting services that help companies maximize profitability and customer satisfaction through deeper data insights and predictive analytics. With a fully integrated, end-to-end platform for planning, forecasting, merchandising, pricing, and promotions, Impact Analytics empowers companies to make smarter decisions based on real-time insights rather than relying on last year‚Äôs inputs to forecast and plan this year‚Äôs business. Powered by over one million machine learning models, Impact Analytics has been leading AI innovation for a decade, setting new benchmarks in forecasting, planning, and operational excellence across the retail, grocery, manufacturing, and CPG sectors. In 2025, Impact Analytics is at the forefront of the Agentic AI revolution, delivering autonomous solutions that enable businesses to adapt in real time, optimize operations, and drive profitability without manual intervention. Here‚Äôs a link to our website: www.impactanalytics.co . Some of our accolades include: Ranked as one of America's Fastest-Growing Companies by Financial Times for five consecutive years: 2020-2024. Ranked as one of America's Fastest-Growing Private Companies by Inc. 5000 for seven consecutive years: 2018-2024. Voted #1 by more than 300 retailers worldwide in the RIS Software LeaderBoard 2024 report. Ranked #72 in America‚Äôs Most Innovative Companies list in 2023‚Äîby Fortune‚Äîalongside companies like Microsoft, Tesla, Apple, IBM, etc. Forged a strategic partnership with Google to equip retailers with cutting-edge generative AI tools. Recognized in multiple Gartner reports, including Market Guides and Hype Cycle, spanning assortments, merchandising, forecasting, algorithmic retailing, and Unified Price, Promotion, and Markdown Optimization Applications. About the Role We are looking for a Data Engineer to join our data team to solve data-driven critical business problems. The person in this role will be responsible for expanding and optimizing the existing end-to-end architecture including the data pipeline architecture. The Data Engineer will collaborate with software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture (consistency throughout ongoing projects). The right candidate should have hands-on experience in developing a hybrid set of data-pipelines depending on the business requirements. Responsibilities Develop, construct, test and maintain existing and new data-driven architectures. Align architecture with business requirements and provide solutions which fits best to solve the business problems. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS - big data technologies. Data acquisition from multiple sources across the organization. Use programming language and tools efficiently to collate the data. Identify ways to improve data reliability, efficiency, and quality. Use data to discover tasks that can be automated. Deliver updates to stakeholders based on analytics. Set up practices on data reporting and continuous monitoring. Qualifications Graduate or Postgraduate in Computer Science or in similar quantitative area (B. Tech/B.E. or M. Tech/M.E. in Computers, IT). 1+ years of relevant work experience as a Data Engineer or in a similar role. Required Skills Hands-on experience on different databases, python tools and ability to solve complex business problems using data. Advanced SQL knowledge, Data-Modelling and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience in developing and optimizing ETL pipelines, big data data pipelines, and data-driven architectures. Strong knowledge in programming using Python Shell. Strong analytical skills related to working with different types of datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. Experience supporting and working with cross-functional teams in a dynamic environment. Strong knowledge of working with different OS Linux, Windows, etc. Preferred Skills Experience with big data tools: Hadoop, Spark, Hive, etc. Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. Experience with GCP cloud services: GCS, Big query, VMs. Experience with object-oriented/object function scripting languages: Python.",,,"Python, SQL, Machine Learning",
4247963409,Data Engineer,Accenture in India,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job Project Role : Data Engineer Project Role Description : Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems. Must have skills : PySpark Good to have skills : NA Minimum 5 Year(s) Of Experience Is Required Educational Qualification : 15 years full time education Summary: As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization. Roles & Responsibilities: - Expected to be an SME. - Collaborate and manage the team to perform. - Responsible for team decisions. - Engage with multiple teams and contribute on key decisions. - Provide solutions to problems for their immediate team and across multiple teams. - Mentor junior team members to enhance their skills and knowledge in data engineering. - Continuously evaluate and improve data processes to enhance efficiency and effectiveness. Professional & Technical Skills: - Must To Have Skills: Proficiency in PySpark. - Strong understanding of data modeling and database design principles. - Experience with data warehousing solutions and ETL tools. - Familiarity with cloud platforms such as AWS, Azure, or Google Cloud. - Knowledge of data governance and data quality frameworks. Additional Information: - The candidate should have minimum 5 years of experience in PySpark. - This position is based at our Bengaluru office. - A 15 years full time education is required. 15 years full time education",,,,
4237649555,Data Engineer,LSEG,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Location : Bengaluru We are looking for an outstanding talent in an Individual contributor role to help build a new data lakehouse. Shown experience as a data engineer(IC, hands on role) or similar role, with a focus on cloud distributed data processing platform for spark, and modern open table concept like delta/iceberg. Solid experience with Azure: Synapse Analytics, Data Factory, Data Lake, Databricks, Microsoft Purview, Monitor, SQL Database, SQL Managed Instance, Stream Analytics, Cosmos DB, Storage Services, ADLS, Azure Functions, Log Analytics, Serverless Architecture, ARM Templates. Strong proficiency in Spark, SQL, and Python/scala/Java. Must have Skills : Python, Spark, Azure Data Factory, Azure Fabric, Azure Functions, ADLS, SQL, Azure SQL, Log Analytics Experience in building Lakehouse architecture using open-source table formats like delta, parquet and tools like Jupyter notebook. Strong notions of security standard methodologies (e.g., using Azure Key Vault, IAM, RBAC, Monitor etc.). Proficient in integrating, redefining, and consolidating data from various structured and unstructured data systems into a structure that is suitable for building analytics solutions. Understand the data through exploration, experience with processes related to data retention, validation, visualization, preparation, matching, fragmentation, segmentation, and improvement. Demonstrates ability to understand business requirements Agile development processes (SCRUM and Kanban) Good communication, presentation, documentation, and social skills Able to self-manage and work independently in a fast-paced environment with multifaceted requirements and priorities. LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth. Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership , Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions. Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity. LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives. We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone‚Äôs race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants' and employees' religious practices and beliefs, as well as mental health or physical disability needs. Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it‚Äôs used for, and how it‚Äôs obtained, your rights and how to contact us as a data subject . If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.",,,"Python, SQL",
4172812190,Software Engineer I - Data Engineer (BI),Uplight,India (Remote),Remote,Full-time,,"About the job Description The Position We are seeking a seasoned engineer with a passion for changing the way millions of people save energy. You‚Äôll work within the Engineering team to build and improve our platforms to deliver flexible and creative solutions to our utility partners and end users and help us achieve our ambitious goals for our business and the planet. We are seeking a skilled and passionate Data Engineer - Business Intelligence with expertise in Data Engineering and BI Reporting to join our development team. As a Data Engineer, you will play a crucial role developing different components, harnessing the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, data processing and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. You will also work on creating BI reports as well as development of a Business Intelligence platform that will enable users to create reports and dashboards based on their requirements. You will coordinate with the rest of the team working on different layers of the infrastructure. Therefore, a commitment to collaborative problem solving, sophisticated design, and quality product is important. You will own the development and its quality independently and be responsible for high quality deliverables. And you will work with a great team with excellent benefits. Responsibilities & Skills You should: Be excited to work with talented, committed people in a fast-paced environment. Have a proven experience as a Data Engineer with a focus on BI reporting.. Be designing, building, and maintaining high performance solutions with reusable, and reliable code. Use a rigorous approach for product improvement and customer satisfaction. Love developing great software as a seasoned product engineer. Be ready, able, and willing to jump onto a call with stakeholders to help solve problems. Be able to deliver against several initiatives simultaneously. Have a strong eye for detail and quality of code. Have an agile mindset. Have strong problem-solving skills and attention to detail. Required Skills (Data Engineer) You ideally have more than 6 months of professional experience. Design, build, and maintain scalable data pipelines and ETL processes to support business analytics and reporting needs. Strong Experience with SQL for querying and transforming large datasets, and optimizing query performance in relational databases. Proficiency in Python for building and automating data pipelines, ETL processes, and data integration workflows. Familiarity with big data frameworks such as Apache Spark or PySpark for distributed data processing. Strong Understanding of data modeling principles for building scalable and efficient data architectures (e.g., star schema, snowflake schema). Good to have experience with Databricks for managing and processing large datasets, implementing Delta Lake, and leveraging its collaborative environment. Knowledge of Google Cloud Platform (GCP) services like BigQuery, Dataflow, Pub/Sub, and Cloud Storage for end-to-end data engineering solutions. Familiarity with version control systems such as Git and CI/CD pipelines for managing code and deploying workflows. Awareness of data governance and security best practices, including access control, data masking, and compliance with industry standards. Exposure to monitoring and logging tools like Datadog, Cloud Logging, or ELK stack for maintaining pipeline reliability. Ability to understand business requirements and translate them into technical requirements. Inclination to design solutions for complex data problems. Ability to deliver against several initiatives simultaneously as a multiplier. Demonstrable experience with writing unit and functional tests. Required Skills (BI Reporting) Strong experience in developing Business Intelligence reports and dashboards via tools such as Tableau, PowerBI, Sigma etc. Ability to analyse and deeply understand the data, relate it to the business application and derive meaningful insights from the data. Required The following experiences are not required, but you'll stand out from other applicants if you have any of the following, in our order of importance: You are an experienced developer - a minimum of 2+ years of professional experience. Work experience & strong proficiency in Python, SQL and BI Reporting and its associated frameworks (like Flask, FastAPI etc.). Experience with cloud infrastructure like AWS/GCP or other cloud service provider experience CI/CD experience You are a Git guru and revel in collaborative workflows You work on the command line confidently and are familiar with all the goodies that the linux toolkit can provide Familiarity with Apache Spark and PySpark. Qualifications Bachelor's or Master's degree in Computer Science, Engineering, or a related field. Uplight provides equal employment opportunities to all employees and applicants and prohibits discrimination and harassment of any type without regard to race (including hair texture and hairstyles), color, religion (including head coverings), age, sex, national origin, caste, disability status, genetics, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",associate,,"Python, SQL, Tableau",
4248825896,Senior Data Analytics Engineer,RefRelay,"New Delhi, Delhi, India (On-site)",On-site,Full-time,,"About the job Immediate/Early Joiners Preferred We‚Äôre seeking a seasoned Data Analytics Engineer (10+ years experience) to lead data architecture, analytics, and visualization initiatives for our client. This role involves building scalable data pipelines, transforming large datasets, and delivering actionable insights through BI tools. Key Responsibilities Design and maintain ETL pipelines , data models , and architectures . Analyze large-scale data using SQL and Python . Create dashboards and visualizations in Power BI or Tableau . Work with big data (Spark, Hadoop) and cloud platforms (AWS, Azure, or GCP). Manage structured and unstructured data (SQL, NoSQL: Cassandra/MongoDB). Collaborate with cross-functional teams to deliver data-driven solutions. Document systems and ensure performance monitoring and data integrity. Requirements 10+ years of experience, including 5+ years in data analytics and 5+ in data engineering . Proficient in SQL, Python, ETL , and data modeling . Hands-on with BI tools , big data tech, and cloud environments. Strong communication, problem-solving, and stakeholder engagement skills. Degree in Computer Science, Data Science, or related field (Master‚Äôs preferred). You may also share your resume with us at cv@refrelay.com.",,10+ years experience,"Python, SQL, Tableau, Power BI",
4215352019,Data Support Engineer,Workday,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Your work days are brighter here. At Workday, it all began with a conversation over breakfast. When our founders met at a sunny California diner, they came up with an idea to revolutionize the enterprise software market. And when we began to rise, one thing that really set us apart was our culture. A culture which was driven by our value of putting our people first. And ever since, the happiness, development, and contribution of every Workmate is central to who we are. Our Workmates believe a healthy employee-centric, collaborative culture is the essential mix of ingredients for success in business. That‚Äôs why we look after our people, communities and the planet while still being profitable. Feel encouraged to shine, however that manifests: you don‚Äôt need to hide who you are. You can feel the energy and the passion, it's what makes us unique. Inspired to make a brighter work day for all and transform with us to the next stage of our growth journey? Bring your brightest version of you and have a brighter work day here. At Workday, we value our candidates‚Äô privacy and data security. Workday will never ask candidates to apply to jobs through websites that are not Workday Careers. Please be aware of sites that may ask for you to input your data in connection with a job posting that appears to be from Workday but is not. In addition, Workday will never ask candidates to pay a recruiting fee, or pay for consulting or coaching services, in order to apply for a job at Workday. About The Team In 2024, Workday launched an Enterprise Data and Analytics team with a mission to transform and optimize the way Workday creates and shares trusted data to drive actionable insights and data led innovation across the enterprise. The team has introduced a new, cloud based technical stack, a Data Product methodology anchored in the principles of Data Ops, an Active Data Governance methodology to manage the quality and discovery of data, and a series of modern analytics tools that facilitate discovery, analysis, visualization, machine learning, and AI. The team‚Äôs goal is to lead high value cross-functional data and analytics work and to establish and support the broader Workday analytics community by establishing modern, common ways of working, facilitating training and communication, and creating secure data and analytics brokering capability across business units. About The Role Maintenance Engineering & L2 Production Support: Ensure timely resolution, delivery and maintenance of data pipelines and analytics solutions built on Snowflake and dbt. Responsible for triaging, investigating and providing resolution of production failures (L2) within agreed upon SLAs. Perform analysis of analytics logic and data pipelines to provide adequately descriptive solutions. Effectively document investigative steps and analysis to escalate L3 technical issues to the relevant engineering teams. Ensure data accuracy, integrity, and consistency across the platform. Proactively identify opportunities for process improvement and optimization and implement it by partnering with relevant engineering teams. Stakeholder Management: Build and maintain strong relationships with stakeholders across all departments. Participate in Incident Management Calls to provide root cause analysis and suggest corrective actions as well as contributing to preventive measures. Strong stakeholder management skills in case of production incidents. About You The person in this role should have a good understanding of the data engineering domain with a proven track record of building and supporting data and analytics engineering solutions using modern data engineering tools and technologies. Basic Qualifications: 3+ years of experience working on building and supporting data and analytics pipelines. Experience with modern cloud based data engineering tools and technologies especially Snowflake and dbt. Well versed with advanced SQL and query/cost optimization techniques to improve overall system performance and reduce query cost. Should have relevant experience in providing L2 support for data pipeline failures in mission critical environments. Should have experience in managing stakeholder communication and handling failures in production environments. Other Qualifications: Proven track record of independently delivering successful analytic engineering solutions. Strong business sense and understanding of business processes. Effective communication skills with technical and non-technical audiences. Our Approach to Flexible Work With Flex Work, we‚Äôre combining the best of both worlds: in-person time and remote. Our approach enables our teams to deepen connections, maintain a strong community, and do their best work. We know that flexibility can take shape in many ways, so rather than a number of required days in-office each week, we simply spend at least half (50%) of our time each quarter in the office or in the field with our customers, prospects, and partners (depending on role). This means you'll have the freedom to create a flexible schedule that caters to your business, team, and personal needs, while being intentional to make the most of time spent together. Those in our remote ""home office"" roles also have the opportunity to come together in our offices for important moments that matter. Are you being referred to one of our roles? If so, ask your connection at Workday about our Employee Referral process!",,,"SQL, Machine Learning",
4233216212,Data Engineer,Chevron,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job About The Position Utilizes software engineering principles to deploy and maintain fully automated data transformation pipelines that combine a large variety of storage and computation technologies to handle a distribution of data types and volumes in support of data architecture design. A Data Engineer designs data products and data pipelines that are resilient to change, modular, flexible, scalable, reusable, and cost effective. Key Responsibilities Design, develop, and maintain data pipelines and ETL processes using Microsoft Azure services (e.g., Azure Data Factory, Azure Synapse, Azure Databricks, Azure Fabric). Utilize Azure data storage accounts for organizing and maintaining data pipeline outputs. (e.g., Azure Data Lake Storage Gen 2 & Azure Blob storage). Collaborate with data scientists, data analysts, data architects and other stakeholders to understand data requirements and deliver high-quality data solutions. Optimize data pipelines in the Azure environment for performance, scalability, and reliability. Ensure data quality and integrity through data validation techniques and frameworks. Develop and maintain documentation for data processes, configurations, and best practices. Monitor and troubleshoot data pipeline issues to ensure timely resolution. Stay current with industry trends and emerging technologies to ensure our data solutions remain cutting-edge. Manage the CI/CD process for deploying and maintaining data solutions. Required Qualifications Bachelor‚Äôs degree in Computer Science, Engineering, or a related field (or equivalent experience) and able to demonstrate high proficiency in programming fundamentals. 3-5 years experience At least 2 years of proven experience as a Data Engineer or similar role dealing with data and ETL processes. Strong knowledge of Microsoft Azure services, including Azure Data Factory, Azure Synapse, Azure Databricks, Azure Blob Storage and Azure Data Lake Gen 2. Experience utilizing SQL DML to query modern RDBMS in an efficient manner (e.g., SQL Server, PostgreSQL). Strong understanding of Software Engineering principles and how they apply to Data Engineering (e.g., CI/CD, version control, testing). Experience with big data technologies (e.g., Spark). Strong problem-solving skills and attention to detail. Excellent communication and collaboration skills. Preferred Qualifications Learning agility Technical Leadership Consulting and managing business needs Strong experience in Python is preferred but experience in other languages such as Scala, Java, C#, etc is accepted. Experience building spark applications utilizing PySpark. Experience with file formats such as Parquet, Delta, Avro. Experience efficiently querying API endpoints as a data source. Understanding of the Azure environment and related services such as subscriptions, resource groups, etc. Understanding of Git workflows in software development. Using Azure DevOps pipeline and repositories to deploy and maintain solutions. Understanding of Ansible and how to use it in Azure DevOps pipelines. Chevron ENGINE supports global operations, supporting business requirements across the world. Accordingly, the work hours for employees will be aligned to support business requirements. The standard work week will be Monday to Friday. Working hours are 8:00am to 5:00pm or 1.30pm to 10.30pm. Chevron participates in E-Verify in certain locations as required by law.",,5 years experience,"Python, SQL",
4244682314,Data Engineer,FORVIA HELLA,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Location Hinjewadi, Pune - Maharashtra, India Pacesetting. Passionate. Together. HELLA, one of the leading automotive suppliers worldwide, has shaped the industry with innovative lighting systems and vehicle electronics. In addition, the company is one of the most important partners of the aftermarket and independent workshops. What motivates us: Shaping the mobility of tomorrow and fostering the central market trends such as autonomous driving, efficiency and electrification, connectivity and digitization as well as individualization. Every day, 36,000 employees worldwide are committed to this with passion, know-how and innovative strength. YOUR TASKS We are looking for a talented Data Engineer and AI Deployment Engineer to join our innovative team. The ideal candidate will have a strong foundation in data engineering and AI deployment, with hands-on experience in creating and deploying data pipelines, working with data lakes, and performing ETL processes. You will be responsible for designing, implementing, and maintaining scalable data and AI solutions to support our business needs. Join our team in designing and deploying AI solutions to enhance the cost, quality, and efficiency of manufacturing and development processes. Key Responsibilities Design, develop, and maintain data pipelines using Python, PySpark, and SQL. Implement ETL processes to extract, transform, and load data from various sources. Deploy and manage containerized applications using Docker. Develop and maintain CI/CD pipelines to automate deployment processes. Collaborate with cross-functional teams to understand data and AI requirements and deliver solutions. Deploy and manage AI/ML models in production environments. Create and manage data lakes to store and process large volumes of data. Required Skills Proficiency in Python, PySpark, RDBMS, and SQL. Hands-on experience writing optimized PySpark and SQL scripts using best practices. Experience in creating and deploying data pipelines. Knowledge of data lake architecture and management. Strong understanding of ETL processes. Hands-on experience with Docker and Git. Experience in CI/CD pipeline development and maintenance. Strict adherence to software development best practices. Excellent communication skills in English with an independent and team-focused working style. Knowledge of Palantir. Familiarity with data streaming services like Apache Kafka, RabbitMQ, etc. Experience with Azure DevOps pipelines. Experience with Apache Airflow. Exposure to AI/ML and MLOps. Your Qualifications Qualifications & Experience: BE in Computer Science, Information Technology. (Engineering Qualification is a must) 2 to 5 years of industry experience in software development. Minimum 2 years of relevant experience as a Data Engineer. Work Location - Hinjewadi Phase -1 with Hybrid Working. Immediate Joiner Prefererd. Take the opportunity to reveal your potential within a global, family-run company that offers you the best possible conditions for progressing in your career. Please send us your application through our careers portal, citing reference number req16080. HELLA India Automotive Pvt Ltd. Rimsha Shaikh",,,"Python, SQL",
4218614446,Data Engineer,Invictus,"Chennai, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job This job is sourced from a job board. Learn More Responsibilities Design, develop, and implement ETL data pipelines that load data into an information product that helps the organization in reaching strategic goals. Work on ingesting, storing, cleansing, processing, and analyzing large data sets from heterogeneous data sources. Finalizing the scope of the system and delivering Big Data solutions. Translate complex technical and functional requirements into detailed designs. Schedule, orchestrate, and implement data pipelines and processes that scale with an increase in data volume. Investigate and analyze alternative solutions to data storing, processing, etc., to ensure the most streamlined approaches are implemented. Collaborate with business consultants, data scientists, and application developers to develop analytics solutions. Help define data governance policies and support data versioning processes. Maintain security and data privacy by working closely with the Data Protection Officer internally. Analyze a vast number of data stores and uncover insights. Create data tools for business team members that assist them in analysis that gives them the competitive edge. Monitor data application performance for potential bottlenecks and resolve performance issues. Identify and implement cost-saving strategies to reduce ongoing Big Data and Cloud costs/expenses. Requirements Strong understanding of Data Warehousing concepts and data modeling techniques, and columnar databases. Strong hands-on experience in Data Analysis, preprocessing, and validation of the engineered dataset in SQL/NoSQL databases. Hands-on Experience in data cleansing, processing, and loading using Big Data tools mainly (Py)Spark and Hive/Presto. Exposure to any workflow scheduling and Orchestration tools Airflow/Prefect/SQS, would be helpful. Comfortable working with any programming/scripting language like Python(Preferred)/Scala/Shell scripting. Strong data engineering skills on anyCloud Platform are essential(Preferred: AWS). Exposure to a data lake and a data lake house would be an added advantage. Solid Understanding of data structures and algorithms. Knowledge of distributed/MPP systems of data storage and computing. Ability to effectively communicate with both business and technical teams. Good interpersonal skills and a positive attitude. Experience in working with agile methodology in a fast-paced environment. This job was posted by Subashini T from Invictus. Desired Skills and Experience Data Analysis,Data Warehousing,ETL,Hive,NoSQL,SQL,Spark",,,"Python, SQL, Data Analysis",
4199155742,Data Engineer,Accenture in India,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job Project Role : Data Engineer Project Role Description : Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems. Must have skills : Google Cloud Machine Learning Services Good to have skills : NA Minimum 5 Year(s) Of Experience Is Required Educational Qualification : BTech Summary: As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture, ETL processes, and ensuring data integrity. Roles & Responsibilities: - Expected to be an SME - Collaborate and manage the team to perform - Responsible for team decisions - Engage with multiple teams and contribute on key decisions - Provide solutions to problems for their immediate team and across multiple teams - Lead data architecture design - Implement and optimize ETL processes - Conduct data quality assessments Professional & Technical Skills: - Must To Have Skills: Proficiency in Google Cloud Machine Learning Services - Strong understanding of data architecture principles - Experience with ETL tools and processes - Knowledge of data modeling and database design - Hands-on experience with data migration and deployment processes Additional Information: - The candidate should have a minimum of 5 years of experience in Google Cloud Machine Learning Services - This position is based at our Hyderabad office - A BTech degree is required BTech",,,Machine Learning,
4250539575,Data Engineer,Nielsen,"Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Job Title: Principal Data Engineer (MTS4 / Principal Engineer) About the Role As a Principal Data Engineer, you will drive the strategy, architecture, and execution of large-scale data solutions across our function. This role involves tackling highly ambiguous, complex challenges where the business problem may not be fully defined at the outset. You will partner closely with cross-functional teams (Engineering, Product, Operations) to shape and deliver our data roadmap. Your work will have a profound impact on our functions' data capabilities, influencing multiple teams‚Äô technical and product direction. You should bring deep expertise in designing and developing robust data pipelines and platforms, leveraging technologies such as Spark, Airflow, Kafka, and other emerging tools. You will set standards and best practices that raise the bar for engineering excellence across the organization. Key Responsibilities Architect & Define Scope Own end-to-end design of critical data pipelines and platforms in an environment characterized by high ambiguity. Translate loosely defined business objectives into a clear technical plan, breaking down complex problems into achievable milestones. Technology Leadership & Influence Provide thought leadership in data engineering, driving the adoption of Spark, Airflow, Kafka, and other relevant technologies (e.g., Hadoop, Flink, Kubernetes, Snowflake, etc.). Lead design reviews and champion best practices for coding, system architecture, data quality, and reliability. Influence senior stakeholders (Engineers, EMs, Product Managers) on technology decisions and roadmap priorities. Execution & Delivery Spearhead strategic, multi-team projects that advance the organization‚Äôs data infrastructure and capabilities. Deconstruct complex architectures into simpler components that can be executed by various teams in parallel. Drive operational excellence, owning escalations and ensuring high availability, scalability, and cost-effectiveness of our data solutions. Mentor and develop engineering talent, fostering a culture of collaboration and continuous learning. Impact & Technical Complexity Shape how the organization operates by introducing innovative data solutions and strategic technical direction. Solve endemic, highly complex data engineering problems with robust, scalable, and cost-optimized solutions. Continuously balance short-term business needs with long-term architectural vision. Process Improvement & Best Practices Set and enforce engineering standards that elevate quality and productivity across multiple teams. Lead by example in code reviews, automation, CI/CD practices, and documentation. Champion a culture of continuous improvement, driving adoption of new tools and methodologies to keep our data ecosystem cutting-edge. Qualifications Education & Experience : Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Engineering, or related field (or equivalent experience). 5+ years of software/data engineering experience, with significant exposure to large-scale distributed systems. Technical Expertise : Demonstrated proficiency with Spark, Airflow, Kafka, and at least one major programming language (e.g., Python, Scala, Java). Experience with data ecosystem technologies such as Hadoop, Flink, Snowflake, Kubernetes, etc. Proven track record of architecting and delivering highly scalable data infrastructure solutions. Leadership & Communication : Ability to navigate and bring clarity in ambiguous situations. Strong cross-functional collaboration skills, influencing both technical and non-technical stakeholders. Experience coaching and mentoring senior engineers. Problem-Solving : History of tackling complex, ambiguous data challenges and delivering tangible results. Comfort making informed trade-offs between opportunity vs. architectural complexity.",Manager,,Python,
4170270597,Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About Amgen Amgen harnesses the best of biology and technology to fight the world‚Äôs toughest diseases, and make people‚Äôs lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what‚Äôs known today. About The Role Role Description: As part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks. Roles & Responsibilities: Design, develop, and maintain data solutions for data generation, collection, and processing. Be a key team member that assists in design and development of the data pipeline. Create data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems. Schedule and manage workflows the ensure pipelines run on schedule and are monitored for failures. Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs. Develop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency. Implement data security and privacy measures to protect sensitive data. Leverage cloud platforms (AWS preferred) to build scalable and efficient data solutions. Collaborate and communicate effectively with product teams. Collaborate with data scientists to develop pipelines that meet dynamic business needs. Share and discuss findings with team members practicing SAFe Agile delivery model. Functional Skills: Basic Qualifications: Master‚Äôs degree and 1 to 3 years of Computer Science, IT or related field experience OR Bachelor‚Äôs degree and 3 to 5 years of Computer Science, IT or related field experience OR Diploma and 7 to 9 years of Computer Science, IT or related field experience Preferred Qualifications: Hands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart, etc. Proficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools Excellent problem-solving skills and the ability to work with large, complex datasets Understanding of data governance frameworks, tools, and best practices Knowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA) Good-to-Have Skills: Experience with ETL tools and various Python packages related to data processing, machine learning model development Strong understanding of data modeling, data warehousing, and data integration concepts Knowledge of Python/R, Databricks, cloud data platforms Experience working in Product team's environment Experience working in an Agile environment Professional Certifications: AWS Certified Data Engineer preferred Databricks Certificate preferred Soft Skills: Initiative to explore alternate technology and approaches to solving problems Skilled in breaking down problems, documenting problem statements, and estimating efforts Excellent analytical and troubleshooting skills Strong verbal and written communication skills Ability to work effectively with global, virtual teams High degree of initiative and self-motivation Ability to manage multiple priorities successfully Team-oriented, with a focus on achieving team goals EQUAL OPPORTUNITY STATEMENT Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. Apply now for a career that defies imagination Objects in your future are closer than they appear. Join us. careers.amgen.com As an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease. Amgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Associate,,"Python, SQL, R, Machine Learning, Data Analysis",
4075793761,Data ETL Engineer,IDT Corporation,"Pune, Maharashtra, India (Remote)",Remote,Full-time,,"About the job This is a full-time work from home opportunity for a star BI data engineer from India. IDT (www.idt.net) is a communications and financial services company founded in 1990 and headquartered in New Jersey, US. Today it is an industry leader in prepaid communication and payment services and one of the world‚Äôs largest international voice carriers. We are listed on the NYSE, employ over 1500 people across 20+ countries, and have revenues in excess of $1.5 billion. We are looking for a Mid-level Business Intelligence Engineer to join our global team. If you are highly intelligent, motivated, ambitious, ready to learn and make a direct impact, this is your opportunity! The individual in this role will perform data analysis, ELT/ETL design and support functions to deliver on strategic initiatives to meet organizational goals across many lines of business. The interview process will be conducted in English Responsibilities: Develop, document, and test ELT/ETL solutions using industry standard tools (Snowflake, Denodo Data Virtualization, Looker) Recommend process improvements to increase efficiency and reliability in ELT/ETL development Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient ELT/ ETL processes Collaborate with Quality Assurance resources to debug ELT/ETL development and ensure the timely delivery of products Should be willing to explore and learn new technologies and concepts to provide the right kind of solution Target and result oriented with strong end user focus Effective oral and written communication skills with BI team and user community Requirements: 5+ years of experience in ETL/ELT design and development, integrating data from heterogeneous OLTP systems and API solutions, and building scalable data warehouse solutions to support business intelligence and analytics Demonstrated experience in utilizing python for data engineering tasks, including transformation, advanced data manipulation, and large-scale data processing Experience in data analysis, root cause analysis and proven problem solving and analytical thinking capabilities Experience designing complex data pipelines extracting data from RDBMS, JSON, API and Flat file sources Demonstrated expertise in SQL and PLSQL programming, with advanced mastery in Business Intelligence and data warehouse methodologies, along with hands-on experience in one or more relational database systems and cloud-based database services such as Oracle, MySQL, Amazon RDS, Snowflake, Amazon Redshift, etc Proven ability to analyze and optimize poorly performing queries and ETL/ELT mappings, providing actionable recommendations for performance tuning Understanding of software engineering principles and skills working on Unix/Linux/Windows Operating systems, and experience with Agile methodologies Proficiency in version control systems, with experience in managing code repositories, branching, merging, and collaborating within a distributed development environment Excellent English communication skills Interest in business operations and comprehensive understanding of how robust BI systems drive corporate profitability by enabling data-driven decision-making and strategic insights. Pluses Experience in developing ETL/ELT processes within Snowflake and implementing complex data transformations using built-in functions and SQL capabilities Experience using Pentaho Data Integration (Kettle) / Ab Initio ETL tools for designing, developing, and optimizing data integration workflows. Experience designing and implementing cloud-based ETL solutions using Azure Data Factory, DBT, AWS Glue, Lambda and open-source tools Experience with reporting/visualization tools (e.g., Looker) and job scheduler software Experience in Telecom, eCommerce, International Mobile Top-up Education: BS/MS in computer science, Information Systems or a related technical field or equivalent industry expertise Preferred Certification: AWS Solution Architect, AWS Cloud Data Engineer, Snowflake SnowPro Core Please attach CV in English. The interview process will be conducted in English. Only accepting applicants from INDIA",,,"Python, SQL, Data Analysis",
4212699492,Data Engineer,LSEG,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job The Data Platform team is responsible for the overall data management effort at Tradeweb, a part of LSEG. As a data engineering team our focus is on data pipeline development, data infrastructure management, and data quality maintenance. We handle a wide portfolio of applications in the analytics space, serving the reporting needs for both internal and external consumers. Working with both business and technology partners, we are ultimately responsible for maintaining the consistency, availability and accuracy of the data within these systems, ensuring that we deliver high quality views into our enterprise data for all customers. Job Responsibilities Develop Tradeweb‚Äôs data platform based on open source software and cloud services. Build DAG-based batch and stream pipelines applying a variety of technologies as AWS, Kafka, Spark, Databases, and Containers. Ensure at all times the integrity, precision, and quality of Tradeweb‚Äôs historic and realtime data that flows through the platform. Define schemas, test and deploy software in various environments, and setup related monitoring. Help develop machine learning development frameworks and pipelines. Support significant production services and user requests. Qualifications 5+ years of experience in a Data Engineering role. Strong software engineering expertise in Python and containerized ecosystems. Familiarity with standard methodologies in python code and the ability to craft and debug scalable, parallelized systems. Good experience working with SQL and knowledge of query optimization techniques. Familiarity with a variety databases such as MySQL, SQL Server, Snowflake, Redshift, Presto, etc. Experience building batch and stream processing pipelines using Kafka, Spark, Flink, Airflow/Prefect, etc. Familiarity with data science stack: e.g. Juypter, Pandas, Scikit-learn, Dask, Pytorch, MLFlow, Kubeflow, etc. Experience with using AWS/GCP (S3/GCS, EC2/GCE, IAM, etc.) and Linux in production. Ability to look after ambiguity and pinpoint production issues quickly. Strong proclivity for automation and DevOps practices. Experience with leading growing data volume, velocity and variety. Agile, self-starter and is passionate about getting things done. Good communicator. Participate in on-call outside of regular business hours. People are at the heart of what we do and drive the success of our business. Our colleagues thrive personally and expertly through our shared values of Integrity, Partnership, Change and Excellence, which are at the core of our culture. We embrace diversity and actively seek to attract people with unique backgrounds and perspectives. We are always looking at ways to become more agile, so we meet the needs of our teams and customers. We believe that an inclusive collaborative workplace is pivotal to our success and supports the potential and growth of all colleagues at LSEG. LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth. Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership , Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions. Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity. LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives. We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone‚Äôs race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants' and employees' religious practices and beliefs, as well as mental health or physical disability needs. Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it‚Äôs used for, and how it‚Äôs obtained, your rights and how to contact us as a data subject . If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.",,,"Python, SQL, Machine Learning",
4245370000,Data Engineer,Insight Global,India (Remote),Remote,Contract,,"About the job üìç Location: Remote But would be nice for this person to be near pune - Pune, India (5th & Ground Floor, Amar Tech Park, 501 & 502, Balewadi - Hinjawadi Rd, Baner, Pune, Maharashtra Title: Data Engineer üíº Type: Contract üí∞ Compensation: 27LPA üïí Working Hours: ( 3pm-11pm ist) üöÄ Start Date: Immediate (No notice period) Immediate interviews available! Priority will be given to candidates who: Submit their resume promptly Are available for immediate interviews Connect via LinkedIn with their resume Provide availability for a video screen with recruiter Key Requirements Comfortable working EST hours ( 3pm-11pm ist) Available to work full-time Strong communication and collaboration skills Ability to work independently and solve problems proactively Must be ready to join immediately (no notice period) Must-Haves: 6+ years of experience in data engineering or a related field Proven experience in a senior data engineering role Expertise in SQL, including complex query development Hands-on experience with SQL Server databases Develop and manage ETL processes and SSIS packages for data integration and transformation Knowledge of APIs and data server management Excellent problem-solving skills and attention to detail Strong communication and collaboration skills Preferred Qualifications: Experience with cloud platforms (e.g., AWS, Azure) Knowledge of data warehousing concepts and tools Experience with data visualization tools (e.g., Tableau, Power BI) Proficiency in Python coding Key Responsibilities: Design, develop, and maintain scalable data pipelines and systems Write efficient and maintainable Python code for data processing and automation Develop complex SQL queries to extract, transform, and load data Manage and optimize databases, SQL Server Integrate and manage data from various sources using REST APIs and data servers Collaborate with data scientists, analysts, and other stakeholders to understand data needs and deliver solutions Ensure data quality, integrity, and security across all data platforms Troubleshoot and resolve data-related issues in a timely manner Description: As a Senior Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure on the largest project in our organization. You will work closely with cross-functional teams to ensure the efficient and effective use of data across the organization. You are partly responsible for managing the systems used by After Sales, Marketing & Sales, Parts, and North American and European dealers. These applications are used inside and outside our company, mainly for commercial and operational processes and purposes. Failure of these systems often means that the business comes to a partial standstill.",manager,,"Python, SQL, Tableau, Power BI",
4250877348,Data Engineer,RapidBrains,India (Remote),Remote,Contract,,"About the job Job title : Data Engineer Experience: 5‚Äì8 Years Location: Remote Shift: IST (Indian Standard Time) Contract Type: Short-Term Contract Job Overview We are seeking an experienced Data Engineer with deep expertise in Microsoft Fabric to join our team on a short-term contract basis. You will play a pivotal role in designing and building scalable data solutions and enabling business insights in a modern cloud-first environment. The ideal candidate will have a passion for data architecture, strong hands-on technical skills, and the ability to translate business needs into robust technical solutions. Key Responsibilities Design and implement end-to-end data pipelines using Microsoft Fabric components (Data Factory, Dataflows Gen2). Build and maintain data models , semantic layers , and data marts for reporting and analytics. Develop and optimize SQL-based ETL processes integrating structured and unstructured data sources. Collaborate with BI teams to create effective Power BI datasets , dashboards, and reports. Ensure robust data integration across various platforms (on-premises and cloud). Implement mechanisms for data quality , validation, and error handling. Translate business requirements into scalable and maintainable technical solutions. Optimize data pipelines for performance and cost-efficiency . Provide technical mentorship to junior data engineers as needed. Required Skills Hands-on experience with Microsoft Fabric : Dataflows Gen2, Pipelines, OneLake. Strong proficiency in Power BI , including semantic modeling and dashboard/report creation. Deep understanding of data modeling techniques: star schema, snowflake schema, normalization, denormalization. Expertise in SQL , stored procedures, and query performance tuning. Experience integrating data from diverse sources: APIs, flat files, databases, and streaming. Knowledge of data governance , lineage, and data catalog tools within the Microsoft ecosystem. Strong problem-solving skills and ability to manage large-scale data workflows.",,,"SQL, Power BI",
4249143931,Python Developer,MyCareernet,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Company:IT Services Organization Key Skills: Python Development, AI/Gen-AI, PL/SQL, Microservices (FastAPI/Django), REST/SOAP APIs, Web Services, Open-Source Tools & Frameworks, Insurance Domain Knowledge, Cloud Platforms (Azure Preferred), Maven/Gradle, Git, Frontend (ReactJS/AngularJS). Roles and Responsibilities: Develop and maintain high-quality Python applications, with a strong focus on AI/Generative AI solutions. Design and build efficient, reusable, and reliable Python code aligned with project and business requirements. Collaborate with cross-functional teams to understand functional requirements and translate them into technical solutions. Build and consume web services using RESTful and SOAP protocols. Implement microservices using frameworks such as FastAPI or Django, ensuring scalability and performance. Contribute to the full software development life cycle (SDLC), including design, development, deployment, and support. Leverage open-source tools, libraries, and frameworks for efficient development and rapid prototyping. Maintain high standards of code quality through code reviews, unit testing, and automation practices. Work with PL/SQL for backend database interactions and complex data processing. Participate in agile/scrum development cycles, contributing to planning, estimations, and sprint reviews. Collaborate with stakeholders to understand business requirements in the insurance domain and align technical solutions accordingly. Ensure timely delivery and take ownership of assigned tasks. Experience Requirements: 4-10 years of hands-on experience in Python development. Proficient in AI and Generative AI technologies with real-world application experience. Strong experience in working with PL/SQL and microservices architecture. Practical knowledge of RESTful APIs, web services, and API integration. Familiar with Maven/Gradle, Git, and modern version control systems. Exposure to modern frontend frameworks like ReactJS, AngularJS, or BackboneJS is a plus. Prior experience in insurance or related industries is preferred. Experience working in cloud environments (preferably Azure) is a strong advantage. Hands-on experience in deploying and maintaining services in production environments. Education: Any Graduation.",,,"Python, SQL",
4240911643,Data Engineer,Brenolabs,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job This job is sourced from a job board. Learn More We seek a skilled Data Engineer to join our data and analytics team. You will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure that enable efficient data collection, transformation, and storage to support data-driven decision-making across the organization. Responsibilities Design, develop, and maintain robust, scalable, and high-performance data pipelines and ETL/ELT processes. Integrate data from a wide variety of sources using APIs, batch processes, and streaming platforms. Build and optimize data warehouses, data lakes, and data marts for analytics and BI needs. Ensure data quality, integrity, and consistency across all systems and platforms. Collaborate with data scientists, analysts, and other stakeholders to deliver reliable data products. Monitor and improve data pipeline performance and troubleshoot issues as they arise. Implement best practices for data modeling, governance, and security. Requirements Bachelor's degree in Computer Science, Engineering, or related field (Master's preferred). 2+ years of experience in data engineering or related roles. Strong programming skills in Python, SQL, or Scala. Experience with ETL tools (e. g., Apache Airflow, DBT, Talend). Hands-on experience with big data technologies like Spark, Hadoop, and Kafka. Proficiency in working with cloud platforms (e. g., AWS, GCP, or Azure) and cloud-native data services like Redshift, BigQuery, or Snowflake. Experience with data modeling, warehousing, and performance tuning. Preferred Skills Familiarity with DevOps/DataOps tools (Docker, Kubernetes, Git, CI/CD pipelines). Experience with real-time data processing and stream analytics. Knowledge of data governance and privacy regulations (GDPR, HIPAA, etc. ). This job was posted by Gaurav G from Brenolabs.",,,"Python, SQL",
4212652765,Data Engineer,BayOne Solutions,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job Job Title: Data Engineer Work Mode: Hybrid (3 Days from Office ‚Äì Only 4 Hours Onsite per Day) Location: Gurgaon About the Role BayOne is looking for a skilled Data Engineer to join our dynamic team in Gurgaon. This hybrid role offers flexibility, with just 4 hours per day required in-office , 3 days a week. If you're passionate about building scalable data solutions using Azure and Databricks and thrive in a fast-paced environment, we'd love to hear from you. Key Responsibilities Design and build scalable data pipelines and data lake/warehouse solutions on Azure and Databricks . Work extensively with SQL , schema design, and dimensional data modeling . Develop and maintain ETL/ELT processes using tools like ADF, Talend, Informatica , etc. Leverage Azure Synapse, Azure SQL, Snowflake, Redshift, or BigQuery to manage and optimize data storage and retrieval. Utilize Spark, PySpark, and Spark SQL for big data processing. Collaborate cross-functionally to gather requirements, design solutions, and implement best practices in data engineering. Required Qualifications Minimum 5 years of experience in data engineering, data warehousing, or data lake technologies. Strong experience on Azure cloud platform (preferred over others). Proven expertise in SQL , data modeling, and data warehouse architecture. Hands-on with Databricks, Spark , and proficient programming in PySpark/Spark SQL . Experience with ETL/ELT tools such as Azure Data Factory (ADF) , Talend , or Informatica . Strong communication skills and the ability to thrive in a fast-paced, dynamic environment . Self-motivated, independent learner with a proactive mindset. Nice-to-Have Skills Knowledge of Azure Event Hub , IoT Hub , Stream Analytics , Cosmos DB , and Azure Analysis Services . Familiarity with SAP ECC, S/4HANA, or HANA data sources. Intermediate skills in Power BI , Azure DevOps , CI/CD pipelines , and cloud migration strategies . About BayOne BayOne is a 12-year-old software consulting company headquartered in Pleasanton, California . We specialize in Talent Solutions , helping clients build diverse and high-performing teams. Our mission is to #MakeTechPurple by driving diversity in tech while delivering cutting-edge solutions across: Project & Program Management Cloud & IT Infrastructure Big Data & Analytics Software & Quality Engineering User Experience Design Explore More: üîó Company Website üîó LinkedIn üîó Glassdoor Reviews Join us to shape the future of data-driven decision-making while working in a flexible and collaborative environment.",,,"SQL, Power BI",
4208910074,Senior Big Data Engineer,Qualcomm,"Hyderabad, Telangana, India",,Full-time,"experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets Strong data visualization skills Basic understanding of Machine Learning; Prior experience in ML Engineering a plus Ability to manage on-premises data and make it inter-operate with AWS based pipelines Ability to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus Education Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline Preferred Qualifications: Masters in CS/ECE with a Data Science / ML Specialization Minimum Qualifications: Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience. OR Master's degree in Engineering, Information Systems, Computer Science, or related field OR PhD in Engineering, Information Systems, Computer Science, or related field. 3+ years of experience with Programming Language such as C, C++, Java, Python, etc. Develops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design. Principal Duties And Responsibilities: Completes assigned coding tasks to specifications on time without significant errors or bugs. Adapts to changes and setbacks in order to manage pressure and meet deadlines. Collaborates with others inside project team to accomplish project objectives. Communicates with project lead to provide status and information about impending obstacles. Quickly resolves complex software issues and bugs. Gathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions. Seeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently. Participates in technical conversations with tech leads/managers. Anticipates and communicates issues with project team to maintain open communication. Makes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks. Prioritizes project deadlines and deliverables with minimal supervision. Resolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues). Writes readable code for large features or significant bug fixes to support collaboration with other engineers. Determines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner. Unit tests own code to verify the stability and functionality of a feature. Applicants : Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here . Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries). Qualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law. To all Staffing and Recruiting Agencies : Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications. If you would like more information about this role, please contact Qualcomm Careers . 3074475 Loading job details","About the job Company: Qualcomm India Private Limited Job Area: Engineering Group, Engineering Group > Software Engineering General Summary: As a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces. Minimum Qualifications: Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience. OR Master's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience. OR PhD in Engineering, Information Systems, Computer Science, or related field. 2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc. General Summary: Preferred Qualifications 3+ years of experience as a Data Engineer or in a similar role Experience with data modeling, data warehousing, and building ETL pipelines Solid working experience with Python, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,) Experience with Big Data tools, platforms and architecture with solid working experience with SQL Experience working in a very large data warehousing environment, Distributed System. Solid understanding on various data exchange formats and complexities Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets Strong data visualization skills Basic understanding of Machine Learning; Prior experience in ML Engineering a plus Ability to manage on-premises data and make it inter-operate with AWS based pipelines Ability to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus Education Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline Preferred Qualifications: Masters in CS/ECE with a Data Science / ML Specialization Minimum Qualifications: Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience. OR Master's degree in Engineering, Information Systems, Computer Science, or related field OR PhD in Engineering, Information Systems, Computer Science, or related field. 3+ years of experience with Programming Language such as C, C++, Java, Python, etc. Develops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design. Principal Duties And Responsibilities: Completes assigned coding tasks to specifications on time without significant errors or bugs. Adapts to changes and setbacks in order to manage pressure and meet deadlines. Collaborates with others inside project team to accomplish project objectives. Communicates with project lead to provide status and information about impending obstacles. Quickly resolves complex software issues and bugs. Gathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions. Seeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently. Participates in technical conversations with tech leads/managers. Anticipates and communicates issues with project team to maintain open communication. Makes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks. Prioritizes project deadlines and deliverables with minimal supervision. Resolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues). Writes readable code for large features or significant bug fixes to support collaboration with other engineers. Determines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner. Unit tests own code to verify the stability and functionality of a feature. Applicants : Qualcomm is an equal opportunity employer. If you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that Qualcomm is committed to providing an accessible process. You may e-mail disability-accomodations@qualcomm.com or call Qualcomm's toll-free number found here . Upon request, Qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. Qualcomm is also committed to making our workplace accessible for individuals with disabilities. (Keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. We will not respond here to requests for updates on applications or resume inquiries). Qualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of Company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law. To all Staffing and Recruiting Agencies : Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications. If you would like more information about this role, please contact Qualcomm Careers . 3074475",manager,,"Python, SQL, Machine Learning",
4230055073,Data Engineer,Uplers,"Faridabad, Haryana, India (Remote)",Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!",executive,,"Python, SQL",
4212652765,Data Engineer,BayOne Solutions,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job Job Title: Data Engineer Work Mode: Hybrid (3 Days from Office ‚Äì Only 4 Hours Onsite per Day) Location: Gurgaon About the Role BayOne is looking for a skilled Data Engineer to join our dynamic team in Gurgaon. This hybrid role offers flexibility, with just 4 hours per day required in-office , 3 days a week. If you're passionate about building scalable data solutions using Azure and Databricks and thrive in a fast-paced environment, we'd love to hear from you. Key Responsibilities Design and build scalable data pipelines and data lake/warehouse solutions on Azure and Databricks . Work extensively with SQL , schema design, and dimensional data modeling . Develop and maintain ETL/ELT processes using tools like ADF, Talend, Informatica , etc. Leverage Azure Synapse, Azure SQL, Snowflake, Redshift, or BigQuery to manage and optimize data storage and retrieval. Utilize Spark, PySpark, and Spark SQL for big data processing. Collaborate cross-functionally to gather requirements, design solutions, and implement best practices in data engineering. Required Qualifications Minimum 5 years of experience in data engineering, data warehousing, or data lake technologies. Strong experience on Azure cloud platform (preferred over others). Proven expertise in SQL , data modeling, and data warehouse architecture. Hands-on with Databricks, Spark , and proficient programming in PySpark/Spark SQL . Experience with ETL/ELT tools such as Azure Data Factory (ADF) , Talend , or Informatica . Strong communication skills and the ability to thrive in a fast-paced, dynamic environment . Self-motivated, independent learner with a proactive mindset. Nice-to-Have Skills Knowledge of Azure Event Hub , IoT Hub , Stream Analytics , Cosmos DB , and Azure Analysis Services . Familiarity with SAP ECC, S/4HANA, or HANA data sources. Intermediate skills in Power BI , Azure DevOps , CI/CD pipelines , and cloud migration strategies . About BayOne BayOne is a 12-year-old software consulting company headquartered in Pleasanton, California . We specialize in Talent Solutions , helping clients build diverse and high-performing teams. Our mission is to #MakeTechPurple by driving diversity in tech while delivering cutting-edge solutions across: Project & Program Management Cloud & IT Infrastructure Big Data & Analytics Software & Quality Engineering User Experience Design Explore More: üîó Company Website üîó LinkedIn üîó Glassdoor Reviews Join us to shape the future of data-driven decision-making while working in a flexible and collaborative environment.",,,"SQL, Power BI",
4230047930,Data Engineer,Uplers,"Chandigarh, India (Remote)",Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4233764942,Data Engineer-Data Modeling,IBM,"Mumbai, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Your Role And Responsibilities As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. In this role, your responsibilities may include: Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise 4+ years of experience in data modelling, data architecture. Proficiency in data modelling tools ERwin, IBM Infosphere Data Architect and database management systems Familiarity with different data models like relational, dimensional and NoSQl databases. Understanding of business processes and how data supports business decision making. Strong understanding of database design principles, data warehousing concepts, and data governance practices Preferred Technical And Professional Experience Excellent analytical and problem-solving skills with a keen attention to detail. Ability to work collaboratively in a team environment and manage multiple projects simultaneously. Knowledge of programming languages such as SQL",,,"SQL, Machine Learning",
4185232052,Data Security Engineer,EY,"Kochi, Kerala, India (On-site)",On-site,Full-time,,"About the job At EY, you‚Äôll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we‚Äôre counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all. STS Service Provisioning - Senior Security Analyst Today‚Äôs world is fueled by vast amounts of information. Data is more valuable than ever before. Protecting data and information systems is central to doing business, and everyone in EY Information Security has a critical role to play. Join a global team of almost 950 people who collaborate to support the business of EY by protecting EY and client information assets! Our Information Security professionals enable EY to work securely and deliver secure products and services, as well as detect and quickly respond to security events as they happen. Together, the efforts of our dedicated team help protect the EY brand and build client trust. In Information Security, we combine risk strategy, digital identity, cyber defense, application security, and technology solutions throughout the security lifecycle. You will join a team of hardworking, security-focused individuals dedicated to supporting, protecting, and enabling the business through innovative, secure solutions that provide speed to market and business value. The opportunity The Security Technology Services (STS) group is a division of Information Security that ensures secure access to systems and information for more than 390,000 people in over 150 countries. You will be part of STS DLP Engineering Team specifically to support the Enterprise Data Loss Prevention (DLP) solution. The STS team is responsible for the delivery of DLP Services and the maintenance of the global DLP applications. The team is geographically dispersed and comprises of all disciplines required to deliver Data Security Services for our customers. Your Key Responsibilities The Senior Analyst will be primarily responsible for configuring Data Security Solutions like the enterprise Data Loss Prevention System. The main duties of this role will include helping to translate business requirements to secure data with our technical capabilities, especially overseeing the configuration of the DLP environments in a heterogenous global environment. This role requires strong hands-on experience in leading data security solutions with a strong focus on DLP. It will also participate in Data Access Governance Program development and assist in administering the program. The Senior Analyst will help to supervise and mentor junior analysts on the team. The role will also interface with internal customers, stakeholders and support teams at various levels within the organization including Legal, Data Protection, IT Operations and Engineering. Skills And Attributes For Success Technical knowledge in Data Protection technology (DLP, SIEM, SOAR, Data Access Governance, Networking) Administration of the DLP tools which includes configuring policies, upgrading, and patching, etc Proven effective verbal and written communication skills Ability to independently research and solve technical issues Demonstrated integrity in a professional environment Knowledge of core Information Security concepts related to Governance, Risk & Compliance Excellent teaming skills Ability to work in and adapt to a changing environment Flexibility to adjust to multiple demands, shifting priorities, ambiguity and rapid change Ability to efficiently handle customer concerns and difficult situations with ease and professionalism Essential Functions Of The Job Work with vendors to support the different security technologies Configuration of the Security tools which includes configuring policies, response rules & notifications Work with Monitor & Response team to analyse alerts generating from various systems to tune their configuration Understand and follow the incident response process through event escalations Work with Senior level stakeholders (Risk Management, Compliance & Data Protection) Understand Business requirements and translate into technical controls Ability to work within and alongside diverse, global and virtual teams To qualify for the role you must have Degree in Computer Science, Information Systems, Engineering or a related field. Knowledge of security controls: data classification; data labeling and data loss 3-5 years of experience in one or more of the following: Data Loss Prevention (DLP) Technology support and Event Handling Information Security concepts related to Governance, Risk & Compliance Supporting Information Security Technology English language skills - excellent written and verbal communication Exceptional judgement, tact and decision-making ability Ideally, you‚Äôll also have Demonstrated integrity in a professional environment Ability to work within diverse, global, virtual teams Ability to appropriately balance firm security needs with business impact and benefit What We Look For Good interpersonal, communication and presentation skills Ability to deal with ambiguity and change, and exercise appropriate time management to meet deliverables Prioritization of work items to ensure timelines are achieved Good judgment, tact, and decision-making ability Deep critical thinking skills demonstrating analytical and systematic approach to problem solving Experience working in a global virtual environment Ability to work independently but also within a team environment What We Offer As part of this role, you will work in a highly coordinated, globally diverse team with the opportunity and tools to grow, develop and drive your career forward. Here, you can combine global opportunity with flexible working. The EY benefits package goes above and beyond too, focusing on your physical, emotional, financial and social well-being. Your recruiter can talk to you about the benefits available in your country. Here‚Äôs a snapshot of what we offer: Continuous learning: You will develop the mindset and skills to navigate whatever comes next. Success as defined by you: We will provide the tools and flexibility, so you can make a significant impact, your way. Transformative leadership: We will give you the insights, coaching and confidence to be the leader the world needs. Diverse and inclusive culture: You will be accepted for who you are and empowered to use your voice to help others find theirs. EY | Building a better working world EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.",,,,
4227617554,Data Engineer,Acuity Knowledge Partners,"Gurugram, Haryana, India (On-site)",On-site,Full-time,,"About the job We're seeking a skilled Software Engineer with expertise in Python and it would be nice to have experience of working on Large Language Models (LLM) to join our team. Essential skills Minimum of a bachelor‚Äôs degree in a technical or quantitative field with strong academic background Demonstrated ability to implement data engineering pipelines and real-time applications in python (C++ is a plus) Proficiency with object oriented programming in python is a must Experience with Linux/Unix shell/ scripting languages and Git is a must Experience with python based tools like Jupyter notebook, coding standards like pep8 is a plus Strong problem-solving skills and understanding of data structures and algorithms Experience with large-scale data processing and pipeline development Understanding of various LLM frameworks and experience with prompt engineering using Python or other scripting languages Nice to have Knowledge of natural language processing (NLP) concepts, familiarity with integrating and leveraging LLM APIs for various applications Key Responsibilities Design, develop, and maintain projects using Python along with operational support Transform a wide range of structured and unstructured data into standardized outputs for quantitative analysis and financial engineering Participate in code reviews, ensure coding standards, and contribute to the improvement of the codebase Develop the utility tools that can further automate the software development, testing and deployment workflow Collaborate with internal and external cross-functional teams",Executive,,Python,
4212699492,Data Engineer,LSEG,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job The Data Platform team is responsible for the overall data management effort at Tradeweb, a part of LSEG. As a data engineering team our focus is on data pipeline development, data infrastructure management, and data quality maintenance. We handle a wide portfolio of applications in the analytics space, serving the reporting needs for both internal and external consumers. Working with both business and technology partners, we are ultimately responsible for maintaining the consistency, availability and accuracy of the data within these systems, ensuring that we deliver high quality views into our enterprise data for all customers. Job Responsibilities Develop Tradeweb‚Äôs data platform based on open source software and cloud services. Build DAG-based batch and stream pipelines applying a variety of technologies as AWS, Kafka, Spark, Databases, and Containers. Ensure at all times the integrity, precision, and quality of Tradeweb‚Äôs historic and realtime data that flows through the platform. Define schemas, test and deploy software in various environments, and setup related monitoring. Help develop machine learning development frameworks and pipelines. Support significant production services and user requests. Qualifications 5+ years of experience in a Data Engineering role. Strong software engineering expertise in Python and containerized ecosystems. Familiarity with standard methodologies in python code and the ability to craft and debug scalable, parallelized systems. Good experience working with SQL and knowledge of query optimization techniques. Familiarity with a variety databases such as MySQL, SQL Server, Snowflake, Redshift, Presto, etc. Experience building batch and stream processing pipelines using Kafka, Spark, Flink, Airflow/Prefect, etc. Familiarity with data science stack: e.g. Juypter, Pandas, Scikit-learn, Dask, Pytorch, MLFlow, Kubeflow, etc. Experience with using AWS/GCP (S3/GCS, EC2/GCE, IAM, etc.) and Linux in production. Ability to look after ambiguity and pinpoint production issues quickly. Strong proclivity for automation and DevOps practices. Experience with leading growing data volume, velocity and variety. Agile, self-starter and is passionate about getting things done. Good communicator. Participate in on-call outside of regular business hours. People are at the heart of what we do and drive the success of our business. Our colleagues thrive personally and expertly through our shared values of Integrity, Partnership, Change and Excellence, which are at the core of our culture. We embrace diversity and actively seek to attract people with unique backgrounds and perspectives. We are always looking at ways to become more agile, so we meet the needs of our teams and customers. We believe that an inclusive collaborative workplace is pivotal to our success and supports the potential and growth of all colleagues at LSEG. LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth. Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership , Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions. Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity. LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives. We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone‚Äôs race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants' and employees' religious practices and beliefs, as well as mental health or physical disability needs. Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it‚Äôs used for, and how it‚Äôs obtained, your rights and how to contact us as a data subject . If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.",,,"Python, SQL, Machine Learning",
4232513557,Data Engineer Associate - Operate,PwC Acceleration Centers in India,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job At PwC, our people in managed services focus on a variety of outsourced solutions and support clients across numerous functions. These individuals help organisations streamline their operations, reduce costs, and improve efficiency by managing key processes and functions on their behalf. They are skilled in project management, technology, and process optimization to deliver high-quality services to clients. Those in managed service management and strategy at PwC will focus on transitioning and running services, along with managing delivery teams, programmes, commercials, performance and delivery risk. Your work will involve the process of continuous improvement and optimising of the managed services process, tools and services. Driven by curiosity, you are a reliable, contributing member of a team. In our fast-paced environment, you are expected to adapt to working with a variety of clients and team members, each presenting varying challenges and scope. Every experience is an opportunity to learn and grow. You are expected to take ownership and consistently deliver quality work that drives value for our clients and success as a team. As you navigate through the Firm, you build a brand for yourself, opening doors to more opportunities. Skills Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to: Apply a learning mindset and take ownership for your own development. Appreciate diverse perspectives, needs, and feelings of others. Adopt habits to sustain high performance and develop your potential. Actively listen, ask questions to check understanding, and clearly express ideas. Seek, reflect, act on, and give feedback. Gather information from a range of sources to analyse facts and discern patterns. Commit to understanding how the business works and building commercial awareness. Learn and apply professional and technical standards (e.g. refer to specific PwC tax and audit guidance), uphold the Firm's code of conduct and independence requirements. Role: Associate Tower: Data, Analytics & Specialist Managed Service Experience: 2.0 - 5.5 years Key Skills: AWS Educational Qualification: BE / B Tech / ME / M Tech / MBA Work Location: India.;l Job Description As a Associate, you will work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to: Use feedback and reflection to develop self-awareness, personal strengths, and address development areas. Flexible to work in stretch opportunities/assignments. Demonstrate critical thinking and the ability to bring order to unstructured problems. Ticket Quality and deliverables review, Status Reporting for the project. Adherence to SLAs, experience in incident management, change management and problem management. Seek and embrace opportunities which give exposure to different situations, environments, and perspectives. Use straightforward communication, in a structured way, when influencing and connecting with others. Able to read situations and modify behavior to build quality relationships. Uphold the firm's code of ethics and business conduct. Demonstrate leadership capabilities by working, with clients directly and leading the engagement. Work in a team environment that includes client interactions, workstream management, and cross-team collaboration. Good team player, take up cross competency work and contribute to COE activities. Escalation/Risk management. Position Requirements Required Skills: AWS Cloud Engineer Job description: Candidate is expected to demonstrate extensive knowledge and/or a proven record of success in the following areas: Should have minimum 2 years hand on experience building advanced Data warehousing solutions on leading cloud platforms. Should have minimum 1-3 years of Operate/Managed Services/Production Support Experience Should have extensive experience in developing scalable, repeatable, and secure data structures and pipelines to ingest, store, collect, standardize, and integrate data that for downstream consumption like Business Intelligence systems, Analytics modelling, Data scientists etc. Designing and implementing data pipelines to extract, transform, and load (ETL) data from various sources into data storage systems, such as data warehouses or data lakes. Should have experience in building efficient, ETL/ELT processes using industry leading tools like AWS, AWS GLUE, AWS Lambda, AWS DMS, PySpark, SQL, Python, DBT, Prefect, Snoflake, etc. Design, implement, and maintain data pipelines for data ingestion, processing, and transformation in AWS. Work together with data scientists and analysts to understand the needs for data and create effective data workflows. Implementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data. Improve the scalability, efficiency, and cost-effectiveness of data pipelines. Monitoring and troubleshooting data pipelines and resolving issues related to data processing, transformation, or storage. Implementing and maintaining data security and privacy measures, including access controls and encryption, to protect sensitive data Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases Should have experience in Building and maintaining Data Governance solutions (Data Quality, Metadata management, Lineage, Master Data Management and Data security) using industry leading tools Scaling and optimizing schema and performance tuning SQL and ETL pipelines in data lake and data warehouse environments. Should have Hands-on experience with Data analytics tools like Informatica, Collibra, Hadoop, Spark, Snowflake etc. Should have Experience of ITIL processes like Incident management, Problem Management, Knowledge management, Release management, Data DevOps etc. Should have Strong communication, problem solving, quantitative and analytical abilities. Nice To Have AWS certification Managed Services- Data, Analytics & Insights Managed Service At PwC we relentlessly focus on working with our clients to bring the power of technology and humans together and create simple, yet powerful solutions. We imagine a day when our clients can simply focus on their business knowing that they have a trusted partner for their IT needs. Every day we are motivated and passionate about making our clients‚Äô better. Within our Managed Services platform, PwC delivers integrated services and solutions that are grounded in deep industry experience and powered by the talent that you would expect from the PwC brand. The PwC Managed Services platform delivers scalable solutions that add greater value to our client‚Äôs enterprise through technology and human-enabled experiences. Our team of highly skilled and trained global professionals, combined with the use of the latest advancements in technology and process, allows us to provide effective and efficient outcomes. With PwC‚Äôs Managed Services our clients are able to focus on accelerating their priorities, including optimizing operations and accelerating outcomes. PwC brings a consultative first approach to operations, leveraging our deep industry insights combined with world class talent and assets to enable transformational journeys that drive sustained client outcomes. Our clients need flexible access to world class business and technology capabilities that keep pace with today‚Äôs dynamic business environment. Within our global, Managed Services platform, we provide Data, Analytics & Insights where we focus more so on the evolution of our clients‚Äô Data and Analytics ecosystem. Our focus is to empower our clients to navigate and capture the value of their Data & Analytics portfolio while cost-effectively operating and protecting their solutions. We do this so that our clients can focus on what matters most to your business: accelerating growth that is dynamic, efficient and cost-effective. As a member of our Data, Analytics & Insights Managed Service team, we are looking for candidates who thrive working in a high-paced work environment capable of working on a mix of critical Data, Analytics & Insights offerings and engagement including help desk support, enhancement, and optimization work, as well as strategic roadmap and advisory level work. It will also be key to lend experience and effort in helping win and support customer engagements from not only a technical perspective, but also a relationship perspective.",Associate,,"Python, SQL",
4250112486,Big Data Engineer,Dexian India,"Chennai, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job We are intending to hire Data engineer to handle day-to-day activities involving data ingestion from multiple source locations, help identify data sources, to troubleshoot issues, and engage with a third-party vendor to meet stakeholders‚Äô needs. Work Location: Chennai or Hyderabad or Pune WFO. Shift hours: 2.00pm to 11.00pm IST. Required Immediate Joiners. Required Skills : Python Processing of large quantities of text documents Extraction of text from Office and PDF documents Input json to an API, output json to an API Nifi (or similar technology compatible with current EMIT practices) Basic understanding of AI/ML concepts Database/Search engine/SOLR skills SQL ‚Äì build queries to analyze, create and update databases Understands the basics of hybrid search Experience working with terabytes (TB) of data Basic OpenML/Python/Azure knowledge Scripting knowledge/experience in an Azure environment to automate Cloud systems experience related to search and databases Platforms: DataBricks Snowflake ESRI ArcGIS / SDE New GenAI app being developed Scope of work : 1. Ingest TB of data from multiple sources identified by the Ingestion Lead 2. Optimize data pipelines to improve on data processing, speed, and data availability 4. Make data available for end users from several hundred LAN and SharePoint areas 5. Monitor data pipelines daily and fix issues related to scripts, platforms, and ingestion 6. Work closely with the Ingestion Lead & Vendor on issues related to data ingestion Technical Skills demonstrated: 1. SOLR - Backend database 2. Nifi - Data movement 3. Pyspark - Data Processing 4. Hive & Oozie - For jobs monitoring 5. Querying - SQL, HQl and SOLR querying 6. SQL 7. Python Behavioral Skills demonstrated: 1. Excellent communication skills 2. Ability to receive direction from a Lead and implement 3. Prior experience working in an Agile setup, preferred 4. Experience troubleshooting technical issues and quality control checking of work 5. Experience working with a globally distributed team in different",Manager,,"Python, SQL",
4241886590,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You‚Äôll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you‚Äôll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you‚Äôll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour‚Äôs. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization. Strong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources. Proficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities. Hands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements Preferred Technical And Professional Experience Understanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling. Ability to implement robust data validation, cleansing, and governance frameworks within ETL processes. Proficiency in SQL and/or Shell scripting for custom transformations and automation tasks",,,"SQL, Machine Learning",
4247344393,Data Engineer IRC260453,GlobalLogic,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job Description Join GlobalLogic, to be a valid part of the team working on a huge software project for the world-class company providing M2M / IoT 4G/5G modules e.g. to the automotive, healthcare and logistics industries. Through our engagement, we contribute to our customer in developing the end-user modules‚Äô firmware, implementing new features, maintaining compatibility with the newest telecommunication and industry standards, as well as performing analysis and estimations of the customer requirements. Requirements AWS Data Engineer with Snowflake experience (Good to have) Job responsibilities AWS Data Engineer with Snowflake experience (Good to have) What we offer Culture of caring. At GlobalLogic, we prioritize a culture of caring. Across every region and department, at every level, we consistently put people first. From day one, you‚Äôll experience an inclusive culture of acceptance and belonging, where you‚Äôll have the chance to build meaningful connections with collaborative teammates, supportive managers, and compassionate leaders. Learning and development. We are committed to your continuous learning and development. You‚Äôll learn and grow daily in an environment with many opportunities to try new things, sharpen your skills, and advance your career at GlobalLogic. With our Career Navigator tool as just one example, GlobalLogic offers a rich array of programs, training curricula, and hands-on opportunities to grow personally and professionally. Interesting & meaningful work. GlobalLogic is known for engineering impact for and with clients around the world. As part of our team, you‚Äôll have the chance to work on projects that matter. Each is a unique opportunity to engage your curiosity and creative problem-solving skills as you help clients reimagine what‚Äôs possible and bring new solutions to market. In the process, you‚Äôll have the privilege of working on some of the most cutting-edge and impactful solutions shaping the world today. Balance and flexibility. We believe in the importance of balance and flexibility. With many functional career areas, roles, and work arrangements, you can explore ways of achieving the perfect balance between your work and life. Your life extends beyond the office, and we always do our best to help you integrate and balance the best of work and life, having fun along the way! High-trust organization. We are a high-trust organization where integrity is key. By joining GlobalLogic, you‚Äôre placing your trust in a safe, reliable, and ethical global company. Integrity and trust are a cornerstone of our value proposition to our employees and clients. You will find truthfulness, candor, and integrity in everything we do. About GlobalLogic GlobalLogic, a Hitachi Group Company, is a trusted digital engineering partner to the world‚Äôs largest and most forward-thinking companies. Since 2000, we‚Äôve been at the forefront of the digital revolution ‚Äì helping create some of the most innovative and widely used digital products and experiences. Today we continue to collaborate with clients in transforming businesses and redefining industries through intelligent products, platforms, and services.",manager,,,
4230055068,Data Engineer,Uplers,"Noida, Uttar Pradesh, India (Remote)",Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4249156586,Python Developer Intern Remote,Softtrack By LI,India (Remote),Save Python Developer Intern Remote at Softtrack By LI,Full-time,,"About the job Lead India is is a IT Services. We‚Äôre passionate about building scalable, efficient, and elegant solutions. Our team is remote-first, collaborative, and fast-moving. Job Description: We are looking for a passionate Python Developer Intern to join our remote engineering team. You‚Äôll work closely with our developers on real-world projects that involve backend development, data processing, automation scripts, or integrations. Key Responsibilities: Write clean, maintainable, and efficient Python code Assist in building and maintaining backend services, APIs, or automation tools Collaborate with senior developers and product team to understand project requirements Write unit tests and participate in code reviews Debug and resolve technical issues Learn and adapt quickly in a dynamic environment Required Skills: Solid understanding of Python (basic to intermediate level) Familiarity with at least one Python framework (e.g., Flask, Django, FastAPI) Knowledge of REST APIs and basic understanding of web services Experience with Git and version control systems Good problem-solving and communication skills Ability to work independently in a remote setting Nice to Have (Not Required): Experience with databases (SQL or NoSQL) Exposure to cloud platforms (e.g., AWS, GCP, Azure) Understanding of Docker or basic DevOps concepts Prior project or internship experience What You‚Äôll Gain: Hands-on experience with real-world applications and scalable systems Opportunity to learn from experienced developers and mentors Flexible working hours and fully remote setup Certificate of Internship and Letter of Recommendation (based on performance) Potential for a full-time role upon successful completion You will get stipend upto 25,000/- per month.",,,"Python, SQL",
4198088717,"Data Engineer, WW Returns & ReComm Tech& Inn",Amazon,"Hyderabad, Telangana, India",,Full-time,,"About the job Description The Data Engineer will own the data infrastructure for the Reverse Logistics Team which includes collaboration with software development teams to build the data infrastructure and maintain a highly scalable, reliable and efficient data system to support the fast growing business. You will work with analytic tools, can write excellent SQL scripts, optimize performance of SQL queries and can partner with internal customers to answer key business questions. We look for candidates who are self-motivated, flexible, hardworking and who like to have fun. About The Team Reverse Logistics team at Amazon Hyderabad Development Center is an agile team whose charter is to deliver the next generation of Reverse Logistics platform. As a member of this team, your mission will be to design, develop, document and support massively scalable, distributed data warehousing, querying and reporting system. Basic Qualifications 2+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Knowledge of AWS Infrastructure Knowledge of writing and optimizing SQL queries in a business environment with large-scale, complex datasets Strong analytical and problem solving skills. Curious, self-motivated & a self-starter with a ‚Äòcan do attitude‚Äô. Comfortable working in fast paced dynamic environment Preferred Qualifications Bachelor's degree in a quantitative/technical field such as computer science, engineering, statistics Proven track record of strong interpersonal and communication (verbal and written) skills. Experience developing insights across various areas of customer-related data: financial, product, and marketing Proven problem solving skills, attention to detail, and exceptional organizational skills Ability to deal with ambiguity and competing objectives in a fast paced environment Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing and operations Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - Amazon Dev Center India - Hyderabad Job ID: A2942481",,,"Python, SQL",
4237275204,Big Data Engineer (Standard),Infogain,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job Roles & Responsibilities Job Description: Build pipelines to bring in wide variety of data from multiple sources within the organization as well as from social media and public data sources. Collaborate with cross functional teams to source data and make it available for downstream consumption. Work with the team to provide an effective solution design to meet business needs. Ensure regular communication with key stakeholders, understand any key concerns in how the initiative is being delivered or any risks/issues that have either not yet been identified or are not being progressed. Ensure dependencies and challenges (risks) are escalated and managed. Escalate critical issues to the Sponsor and/or Head of Data Engineering. Ensure timelines (milestones, decisions and delivery) are managed and value of initiative is achieved, without compromising quality and within budget. Ensure an appropriate and coordinated communications plan is in place for initiative execution and delivery, both internal and external. Ensure final handover of initiative to business as usual processes, carry out a post implementation review (as necessary) to ensure initiative objectives have been delivered, and any lessons learned are fed into future initiative management processes. Who We Are Looking For Competencies & Personal Traits Work as a team player Excellent problem analysis skills Experience with at least one Cloud Infra provider (Azure/AWS) Experience in building data pipelines using batch processing with Apache Spark (Spark SQL, Dataframe API) or Hive query language (HQL) Knowledge of Big data ETL processing tools Experience with Hive and Hadoop file formats (Avro / Parquet / ORC) Basic knowledge of scripting (shell / bash) Experience of working with multiple data sources including relational databases (SQL Server / Oracle / DB2 / Netezza). Basic understanding of CI CD tools such as Jenkins, JIRA, Bitbucket, Artifactory, Bamboo and Azure Dev-ops. Basic understanding of DevOps practices using Git version control Ability to debug, fine tune and optimize large scale data processing jobs Working Experience 1-3 years of broad experience of working with Enterprise IT applications in cloud platform and big data environments. Professional Qualifications Certifications related to Data and Analytics would be an added advantage Education Master/Bachelor‚Äôs degree in STEM (Science, Technology, Engineering, Mathematics) Language Fluency in written and spoken English Experience 3-4.5 Years Skills Primary Skill: Data Engineering Sub Skill(s): Data Engineering Additional Skill(s): Kafka, Big Data, Apache Hive, SQL Server DBA, CI/CD, Apache Spark About The Company Infogain is a human-centered digital platform and software engineering company based out of Silicon Valley. We engineer business outcomes for Fortune 500 companies and digital natives in the technology, healthcare, insurance, travel, telecom, and retail & CPG industries using technologies such as cloud, microservices, automation, IoT, and artificial intelligence. We accelerate experience-led transformation in the delivery of digital platforms. Infogain is also a Microsoft (NASDAQ: MSFT) Gold Partner and Azure Expert Managed Services Provider (MSP). Infogain, an Apax Funds portfolio company, has offices in California, Washington, Texas, the UK, the UAE, and Singapore, with delivery centers in Seattle, Houston, Austin, Krak√≥w, Noida, Gurgaon, Mumbai, Pune, and Bengaluru.",,,SQL,
4186996419,Python Data Engineer,WIN Home Inspection,Greater Delhi Area (Remote),Remote,Full-time,,"About the job ABOUT THE PYTHON DATA ENGINEER ROLE: We are looking for a skilled Python Data Engineer to join our team and work on building high-performance applications and scalable data solutions. In this role, you will be responsible for designing, developing, and maintaining robust Python-based applications, optimizing data pipelines, and integrating various APIs and databases. This is more than just a coding role‚Äîit requires strategic thinking, creativity, and a passion for data-driven decision-making to drive results and innovation. KEY RESPONSIBILITIES: Develop, test, and maintain efficient Python applications. Design, develop, and maintain ETL pipelines for efficient data extraction, transformation, and loading. Implement and integrate APIs, web scraping techniques, and database queries to extract data from various sources. Design and implement algorithms for data processing, transformation, and analysis. Write optimized SQL queries and work with relational databases to manage and analyse large datasets. Collaborate with cross-functional teams to understand technical requirements and deliver high-quality solutions. Ensure code quality, performance, and scalability through best practices and code reviews. Stay updated with the latest advancements in Python, data engineering, and backend development. REQUIRED QUALIFICATIONS: Bachelor‚Äôs/Master‚Äôs degree in Computer Science, Engineering, or a related field. 3‚Äì5+ years of hands-on experience as Data Engineer using Python Proficiency in Python frameworks and libraries such as Pandas, NumPy, and Scrapy. Experience with Data Visualization tools such as Power BI, Tableau Strong understanding of relational databases and SQL. Experience working with cloud platforms such as AWS Strong problem-solving skills with an analytical mindset. Excellent communication skills and the ability to work in a collaborative team environment. WHY JOIN US? Highly inclusive and collaborative culture built on mutual respect. Focus on core values, initiative, leadership, and adaptability. Strong emphasis on personal and professional development. Flexibility to work remotely and/or hybrid indefinitely. ABOUT WIN: Founded in 1993, WIN is a highly innovative proptech company revolutionizing the real estate industry with cutting-edge software platforms and products. With the stability and reputation of a 30-year legacy paired with the curiosity and agility of a start-up, we‚Äôve been recognized as an Entrepreneur 500 company, one of the Fastest Growing Companies, and the Most Innovative Home Services Company. OUR CULTURE: Our colleagues are driven by curiosity and tinkering and a desire to make an impact. They enjoy a culture of high energy and collaboration where we listen to each other with empathy, experience personal and professional growth, and celebrate small victories and big accomplishments. Click here to learn more about our company and culture: https://www.linkedin.com/company/winhomeinspection/life",,,"Python, SQL, Tableau, Power BI",
4238444544,Data Engineer,Docusign,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Company Overview Docusign brings agreements to life. Over 1.5 million customers and more than a billion people in over 180 countries use Docusign solutions to accelerate the process of doing business and simplify people‚Äôs lives. With intelligent agreement management, Docusign unleashes business-critical data that is trapped inside of documents. Until now, these were disconnected from business systems of record, costing businesses time, money, and opportunity. Using Docusign‚Äôs Intelligent Agreement Management platform, companies can create, commit, and manage agreements with solutions created by the #1 company in e-signature and contract lifecycle management (CLM). What you'll do Docusign is seeking a talented and results oriented Data Engineer to focus on delivering trusted data to the business. As a member of the Global Data Analytics (GDA) Team, the Data Engineer leverages a variety of technologies to design, develop and deliver new features in addition to loading, transforming and preparing data sets of all shapes and sizes for teams around the world. During a typical day, the Engineer will spend time developing new features to analyze data, develop solutions and load tested data sets into the Snowflake Enterprise Data Warehouse. The ideal candidate will demonstrate a positive ‚Äúcan do‚Äù attitude, a passion for learning and growing, and the drive to work hard and get the job done in a timely fashion. This individual contributor position provides plenty of room to grow -- a mix of challenging assignments, a chance to work with a world-class team, and the opportunity to use innovative technologies such as AWS, Snowflake, dbt, Airflow and Matillion. This position is an individual contributor role reporting to the Manager, Data Engineering. Responsibility Design, develop and maintain scalable and efficient data pipelines Analyze and Develop data quality and validation procedures. Work with stakeholders to understand the data requirements and provide solutions Troubleshoot and resolve data issues in a timely manner Learn and leverage available AI tools for increased developer productivity Collaborate with cross-functional teams to ingest data from various sources Evaluate and improve data architecture and processes continuously Own, monitor, and improve solutions to ensure SLAs are met Develop and maintain documentation for Data infrastructure and processes Executes projects using Agile Scrum methodologies and be a team player Job Designation Hybrid: Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation) Positions at Docusign are assigned a job designation of either In Office, Hybrid or Remote and are specific to the role/job. Preferred job designations are not guaranteed when changing positions within Docusign. Docusign reserves the right to change a position's job designation depending on business needs and as permitted by local law. What you bring Basic Bachelor‚Äôs Degree in Computer Science, Data Analytics, Information Systems, etc Experience developing data pipelines in one of the following languages: Python or Java 5+ years dimensional and relational data modeling experience Excellent SQL and database management skills Preferred 5+ years in data warehouse engineering (OLAP) Snowflake, BigQuery, Teradata, Redshift 5+ years with transactional databases (OLTP) Oracle, SQL Server, MySQL 5+ years with big data, Hadoop, Data Lake, Spark in a cloud environment(AWS) 5+ years with commercial ETL tools - DBT, Matillion etc 5+ years delivering ETL solutions from source systems, databases, APIs, flat-files, JSON Experience developing Entity Relationship Diagrams with Erwin, SQLDBM, or equivalent Experience working with job scheduling and monitoring systems (Airflow, Datadog, AWS SNS) Familiarity with Gen AI tools like Git Copilot and dbt copilot. Good understanding of Gen AI Application frameworks. Knowledge on any agentic platforms Experience building BI Dashboards with tools like Tableau Experience in the financial domain, sales and marketing, accounts payable, accounts receivable, invoicing Experience managing work assignments using tools like Jira and Confluence Experience with Scrum/Agile methodologies Ability to work independently and as part of a team Excellent analytical and problem solving and communication skills Life at Docusign Working here Docusign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what‚Äôs right, every day. At Docusign, everything is equal. We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you‚Äôll be loved by us, our customers, and the world in which we live. Accommodation Docusign is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application procedures. If you need such an accommodation, or a religious accommodation, during the application process, please contact us at accommodations@docusign.com. If you experience any issues, concerns, or technical difficulties during the application process please get in touch with our Talent organization at taops@docusign.com for assistance. Applicant and Candidate Privacy Notice",Manager,,"Python, SQL, Tableau",
4248662530,Data Engineer,Wissen Technology,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Azure Data Factory(ADF),Azure Databricks, Snowflake, DBT, Python, SSIS and TSQL",,,Python,
4243350514,Data Engineer,Deloitte,"Gurugram, Haryana, India (On-site)",On-site,Part-time,,"About the job What impact will you make? Every day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration and high performance. As the undisputed leader in professional services, Deloitte is where you will find unrivaled opportunities to succeed and realize your full potentiaL Deloitte is where you will find unrivaled opportunities to succeed and realize your full potential. The Team Deloitte‚Äôs practice can help you uncover and unlock the value buried deep inside vast amounts of data. Our global network provides strategic guidance and implementation services to help companies manage data from disparate sources and convert it into accurate, actionable information that can support fact-driven decision-making and generate an insight-driven advantage. Our practice addresses the continuum of opportunities in business intelligence & visualization, data management, performance management and next-generation analytics and technologies, including big data, cloud, cognitive and machine learnin g. Learn more about Analytics and Information Management Prac tice. As a Data Engineer , you will bring extensive expertise on data handling and curation capabilities to the team. You‚Äôll be responsible for building intelligent domains using market leading tools, ultimately improving the way we work in Marketing. Experience It is expected that the role holder will most likely have the following qualifications and experience 5-9 years In Data Engineering, software development such as ELT/ETL, data extraction and manipulation in Data Lake/Data Warehouse environment Expert level Hands to the following: Python, SQL PySpark DBT and Apache Airflow Postgres/others RDBMS DevOps, Jenkins, CI/CD Data Governance and Data Quality frameworks Data Lakes, Data Warehouse AWS services including S3, SNS, SQS, Lambda, EMR, Glue, Athena, EC2, VPC etc. Source code control - GitHub, VSTS etc. Key Tasks, Accountabilities and Challenges of this role: Design, develop, test, deploy, maintain and improve software Preparing and maintaining systems and program documentation. Assisting in the analysis and development of applications programs and databases. Modifying and troubleshooting applications programs. Coaching, mentoring, and guiding junior developer engineers Provide key support on fail and fix for assigned application/s. Undertake complex testing activities in relation to software solution Our purpose Deloitte is led by a purpose: To make an impact tha t matters. Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the Communities in which we live and work‚Äîalways striving to be an organization that is held up as a role model of quality, integrity, and positi ve change. Learn more about Deloitte's impact o n the world.",,,"Python, SQL",
4153227978,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you will work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. A career in IBM Consulting embraces long-term relationships and close collaboration with clients across the globe. You will collaborate with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio, including IBM Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you will be supported by mentors and coaches who will encourage you to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground-breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and learning opportunities in an environment that embraces your unique skills and experience. Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results Your Primary Responsibilities Include Develop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools. Liaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT. Work with data scientist and business analytics team to assist in data ingestion and data-related technical issues. Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter Knowledge of Cloud, Power BI, Data migration on cloud skills. Experience in Unix shell scripting and python Experience with relational SQL, Big Data etc Preferred Technical And Professional Experience Knowledge of MS-Azure Cloud Experience in Informatica PowerCenter Experience in Unix shell scripting and python",,,"Python, SQL, Power BI, Machine Learning",
4247673439,Data Engineer,Sundew,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Roles and Responsibilities: Data Pipeline Development: Design, develop, and maintain scalable data pipelines to support ETL (Extract, Transform, Load) processes using tools like Apache Airflow, AWS Glue, or similar. Database Management: Design, optimize, and manage relational and NoSQL databases (such as MySQL, PostgreSQL, MongoDB, or Cassandra) to ensure high performance and scalability. SQL Development: Write advanced SQL queries, stored procedures, and functions to extract, transform, and analyze large datasets efficiently. Cloud Integration: Implement and manage data solutions on cloud platforms such as AWS, Azure, or Google Cloud, utilizing services like Redshift, BigQuery, or Snowflake. Data Warehousing: Contribute to the design and maintenance of data warehouses and data lakes to support analytics and BI requirements. Programming and Automation: Develop scripts and applications in Python or other programming languages to automate data processing tasks. Data Governance: Implement data quality checks, monitoring, and governance policies to ensure data accuracy, consistency, and security. Collaboration: Work closely with data scientists, analysts, and business stakeholders to understand data needs and translate them into technical solutions. Performance Optimization: Identify and resolve performance bottlenecks in data systems and optimize data storage and retrieval. Documentation: Maintain comprehensive documentation for data processes, pipelines, and infrastructure. Stay Current: Keep up-to-date with the latest trends and advancements in data engineering, big data technologies, and cloud services. Required Skills and Qualifications: Education: Bachelor‚Äôs or Master‚Äôs degree in Computer Science, Information Technology, Data Engineering, or a related field. Technical Skills: Proficiency in SQL and relational databases (PostgreSQL, MySQL, etc.). Experience with NoSQL databases (MongoDB, Cassandra, etc.). Strong programming skills in Python; familiarity with Java or Scala is a plus. Experience with data pipeline tools (Apache Airflow, Luigi, or similar). Expertise in cloud platforms (AWS, Azure, or Google Cloud) and data services (Redshift, Big Query, Snowflake). Knowledge of big data tools like Apache Spark, Hadoop, or Kafka is a plus. Data Modeling: Experience in designing and maintaining data models for relational and non-relational databases. Analytical Skills: Strong analytical and problem-solving abilities with a focus on performance optimization and scalability. Soft Skills: Excellent verbal and written communication skills to convey technical concepts to non-technical stakeholders. Ability to work collaboratively in cross-functional teams. Certifications (Preferred): AWS Certified Data Analytics, Google Professional Data Engineer, or similar. Mindset: Eagerness to learn new technologies and adapt quickly in a fast-paced environment.",Executive,,"Python, SQL",
4252505393,Junior Developer - Interviewer,SME Work,India (Remote),Remote,Part-time,,"About the job Company Overview SME is a platform that bridges subject-matter experts with AI projects, enabling them to contribute their knowledge to improve AI models. It offers flexible opportunities to work on tasks like data labeling, quality assurance, and domain-specific problem-solving while earning competitive pay. About the Role We‚Äôre looking for junior developers proficient in Dart, Kotlin, Excel, R, or GO to join our growing team. In this role, you‚Äôll help shape the quality of our data annotation workforce by conducting technical interviews for candidates applying as Independent Contractors (ICs) who will perform annotation work. What you will do Conduct structured interviews to assess coding and logical reasoning skills of candidates. Evaluate communication clarity, technical fundamentals, and attention to detail in potential annotators. Use predefined evaluation rubrics to score candidate responses and provide concise feedback. Collaborate with operations and onboarding teams to ensure quality standards are upheld. Occasionally contribute to refining the interview process and evaluation tools. Who you are You have a working proficiency in Dart, Kotlin, Excel, R, or GO (at least one required). Use version control (Git) for code collaboration and history. Know the importance of setting up environmental variables. Understand the reason behind the need for types. Know how to write docstrings and the importance of them. Understand the reason behind style guidelines and linting rules. Know the technical difference between a relational database and a database that does not have ACID compliance. Understand and are able to control asynchronous code (e.g., promises, async/await). Know how to set up a REST API server. Can debug your way out of a problem by using a stack trace. Have written test cases before. Have participated in code reviews and incorporated feedback in your code or given feedback to other developers. Clear communicator with the ability to ask technical questions and interpret candidate responses. Strong attention to detail. Comfortable using remote communication tools (e.g. Google Meet). Prior interviewing or mentoring experience is a plus, but not required. What We Offer Flexible working hours ‚Äî conduct interviews at times that work for you. A supportive, remote-friendly team culture. Opportunities to grow into more technical or QA roles as we expand.",,,"Excel, R",
4248207569,Data Engineer (Freelancer),Soul AI,India (Remote),Remote,Part-time,,"About the job Step into the world of AI innovation with the Experts Community of Soul AI (By Deccan AI). We are looking for India‚Äôs top 1% Data Engineers for a unique job opportunity to work with the industry leaders. Who can be a part of the community? We are looking for top-tier Data Engineers who are proficient in designing, building and optimizing data pipelines. If you have experience in this field then this is your chance to collaborate with industry leaders. What‚Äôs in it for you? Pay above market standards The role is going to be contract based with project timelines from 2 - 12 months , or freelancing. Be a part of an Elite Community of professionals who can solve complex AI challenges. Work location could be: Remote (Highly likely) Onsite on client location Deccan AI‚Äôs Office: Hyderabad or Bangalore Responsibilities: Design and architect enterprise-scale data platforms, integrating diverse data sources and tools. Develop real-time and batch data pipelines to support analytics and machine learning. Define and enforce data governance strategies to ensure security, integrity, and compliance along with optimizing data pipelines for high performance, scalability, and cost efficiency in cloud environments. Implement solutions for real-time streaming data (Kafka, AWS Kinesis, Apache Flink) and adopt DevOps/DataOps best practices. Required Skills: Strong experience in designing scalable, distributed data systems and programming (Python, Scala, Java) with expertise in Apache Spark, Hadoop, Flink, Kafka, and cloud platforms (AWS, Azure, GCP). Proficient in data modeling, governance, warehousing (Snowflake, Redshift, BigQuery), and security/compliance standards (GDPR, HIPAA). Hands-on experience with CI/CD (Terraform, Cloud Formation, Airflow, Kubernetes) and data infrastructure optimization (Prometheus, Grafana). Nice to Have: Experience with graph databases, machine learning pipeline integration, real-time analytics, and IoT solutions. Contributions to open-source data engineering communities. What are the next steps? 1. Register on our Soul AI website. 2. Our team will review your profile. 3. Clear all the screening rounds: Clear the assessments once you are shortlisted. As soon as you qualify all the screening rounds (assessments, interviews) you will be added to our Expert Community! 4. Profile matching and Project Allocation: Be patient while we align your skills and preferences with the available project. Skip the Noise. Focus on Opportunities Built for You!",,,"Python, Machine Learning",
4249156602,Python Developer Trainee,Lead India,India (Remote),Remote,Full-time,,"About the job Job Title: Python Developer Trainee Location: Remote Job Type: Internship (Full-Time) Duration: 1‚Äì3 Months Stipend: ‚Çπ25,000/month Department: Software Development / Engineering Job Summa ry:We are looking for a passionate and enthusiast ic Python Developer Trai nee to join our remote development team. This is a great opportunity for recent graduates or final-year students to gain practical experience in real-world software development using Python. You will be trained and mentored by experienced developers and get the chance to work on live projec ts.Key Responsibiliti es:Assist in writing clean, efficient, and reusable Python c odeSupport development and maintenance of web applications, APIs, or automation scri ptsDebug, test, and optimize code under the guidance of senior develop ersWork with databases (e.g., MySQL, PostgreSQL, MongoDB) to store, retrieve, and manipulate d ataParticipate in daily stand-ups, code reviews, and training sessi onsDocument code, processes, and best practi cesQualificatio ns:Bachelor‚Äôs degree (or final year) in Computer Science, IT, or related fi eldStrong foundation in Python programm ingFamiliarity with web frameworks like Flask or Django is a p lusBasic understanding of REST APIs, Git, and version cont rolGood problem-solving and logical thinking ski llsWillingness to learn and adapt in a collaborative remote environm entPreferred Skills (Nice to Hav e):Exposure to front-end technologies (HTML, CSS, JavaScri pt)Experience with GitHub/GitLab, Postman, or any Python proje ctsUnderstanding of OOP concepts and design patte rnsFamiliarity with Agile or Scrum methodolog iesWhat We Off er:Monthly stipend of ‚Çπ25, 000100% remote work environm entMentorship and hands-on training from experienced develop ersOpportunity to work on live projects and real codeba sesCertificate of Complet ionPotential for full-time placement based on performa nc",,,Python,
4230053324,Data Engineer,Uplers,"Kanpur, Uttar Pradesh, India (Remote)",Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4237002838,Data Support Engineer,Forbes Advisor,"Chennai, Tamil Nadu, India (On-site)",On-site,Full-time,,"About the job Job Description Role Summary We are seeking a proactive and detail-oriented Data Support Engineer- to monitor production processes, manage incident tickets, and ensure seamless operations in our data platforms. The ideal candidate will have experience in Google Cloud Platform (GCP), Airflow, Python and SQL with a strong focus on enabling developer productivity and maintaining system reliability. Key Responsibilities Production Monitoring: Monitor and ensure the smooth execution of production data pipelines and workflows. Identify and promptly address anomalies or failures in the production environment. Perform first-level investigation for issues, leveraging logs and monitoring tools. Incident Management: Create and manage tickets for identified production issues, ensuring accurate documentation of details and impact analysis. Assign tickets to the appropriate development teams and follow up to ensure timely resolution. Communication of incidents within the Data Team. Platform Support: Participate in daily standup and team meetings and contribute to platform improvement initiatives. Contribute to enhancing the platform to streamline development workflows and improve system usability. Required Skills Bachelor‚Äôs degree with Minimum 1 year of experience working in supporting the production pipelines. Proficiency in SQL for debugging tasks. Familiarity with incident management tools like JIRA. Strong communication skills to interact with cross-functional teams and stakeholders. Good To Have Hands-on experience with Google Cloud Platform (GCP) services like BigQuery. Strong understanding of Apache Airflow and managing DAGs. Basic understanding of DevOps practices and automating CI/CD pipelines. Python Proficiency Note This role requires candidates to work in UK timings. Saturday and Sunday will be working. Rotational off will be provided. Qualifications Bachelors degree in full time.",,,"Python, SQL",
4250544618,Data Engineer,Tata Consultancy Services,"Bhubaneswar, Odisha, India (On-site)",On-site,Full-time,,"About the job Greetings! TCS India presents excellent opportunities for IT professionals. Mega Walk-in Drive for Bhubaneswar Location Role :- Azure Data Engineer Experience:- 4-10 years Location:- Bhubaneswar Required Technical Skill Set:- Azure, ADF, ADB, Synapse & Pyspark Date:- Saturday, 21st June, 2025 Time:- 9:30 AM to 11 AM",Associate,,,
4251116429,Data Engineer,Hexaware Technologies,India (Hybrid),Hybrid,Full-time,,"About the job Company Description At Hexaware, we are a global technology and business process services company with a community of over 31,600 Hexawarians. With 58 offices in 28 countries, we enable enterprises worldwide to fast-track their digital transformation. As an inclusive and diverse employer, we prioritize employee growth through strong learning and development programs. Our innovative culture supports our vision of being the world‚Äôs most loved digital transformation partner. Join us to embrace the potential of technology and make the digital world a better place. Role Description This is a full-time hybrid role as a Data Engineer located in India, with some work-from-home flexibility. The Data Engineer will be responsible for designing, developing, and maintaining data pipelines, ensuring data quality and integrity, and supporting data warehousing solutions. Daily tasks include collaborating with cross-functional teams, coding and debugging, optimizing data systems, and developing data integration solutions. Qualifications Experience in migrating existing datasets from BigQuery to Databricks using Python scripts. Conduct thorough data validation and QA to ensure accuracy, completeness, parity, and consistency in reporting. Monitor the stability and status of migrated data pipelines, applying fixes as needed. Migrate data pipelines from Airflow to Airbyte/Dagster based on provided frameworks. Develop Python scripts to facilitate data migration and pipeline transformation. Perform rigorous testing on migrated data and pipelines to ensure quality and reliability. Required Skills: Strong experience in working on Python for scripting Good experience in working on Data Bricks and Big Query Familiarity with data pipeline tools such as Airflow, Airbyte, and Dagster. Strong understanding of data quality principles and validation techniques. Ability to work collaboratively with cross-functional teams.",,,Python,
4240987633,Data Engineer I,FedEx,"Gurugram, Haryana, India",,Full-time,,"About the job Responsible for developing, optimize, and maintaining business intelligence and data warehouse systems, ensuring secure, efficient data storage and retrieval, enabling self-service data exploration, and supporting stakeholders with insightful reporting and analysis. Grade - T2 Please note that the Job will close at 12am on Posting Close date, so please submit your application prior to the Close Date What Your Main Responsibilities Are Support the development and maintenance of business intelligence and analytics systems to support data-driven decision-making. Implement of business intelligence and analytics systems, ensuring alignment with business requirements. Design and optimize data warehouse architecture to support efficient storage and retrieval of large datasets. Enable self-service data exploration capabilities for users to analyze and visualize data independently. Develop reporting and analysis applications to generate insights from data for business stakeholders. Design and implement data models to organize and structure data for analytical purposes. Implement data security and federation strategies to ensure the confidentiality and integrity of sensitive information. Optimize business intelligence production processes and adopt best practices to enhance efficiency and reliability. Assist in training and support to users on business intelligence tools and applications. Collaborate and maintain relationships with vendors and oversee project management activities to ensure timely and successful implementation of business intelligence solutions. What We Are Looking For Education: Bachelor's degree or equivalent in Computer Science, MIS, Mathematics, Statistics, or similar discipline. Master's degree or PhD preferred. Relevant work experience in data engineering based on the following number of years: Standard I: Two (2) years Standard II: Three (3) years Senior I: Four (4) years Senior II: Five (5) years Knowledge, Skills And Abilities Fluency in English Analytical Skills Accuracy & Attention to Detail Numerical Skills Planning & Organizing Skills Presentation Skills Data Modeling and Database Design ETL (Extract, Transform, Load) Skills Programming Skills FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone. All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances. Our Company FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World‚Äôs Most Admired Companies by ""Fortune"" magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding. Our Philosophy The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company. Our Culture Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970‚Äôs. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today‚Äôs global marketplace.",,,,
4245121446,Data Engineer,Centrilogic,India (Remote),Remote,Full-time,,"About the job Data Engineer Purpose: Over 15 years, we have become a premier global provider of multi-cloud management, cloud-native application development solutions, and strategic end-to-end digital transformation services. Headquartered in Canada and with regional headquarters in the U.S. and the United Kingdom, Centrilogic delivers smart, streamlined solutions to clients worldwide. We are looking for passionate and experienced Data Engineer to work with our other 70 Software, Data and DevOps engineers to guide and assist our clients‚Äô data modernization journey. Our team works with companies with ambitious missions - clients who are creating new, innovative products, often in uncharted markets. We work as embedded members and leaders of our clients' development and data teams. We bring experienced senior engineers, leading-edge technologies and mindsets, and creative thinking. We show our clients how to move to the modern frameworks of data infrastructures and processing, and we help them reach their full potentials with the power of data. In this role, you'll be the day-to-day primary point of contact with our clients to modernize their data infrastructures, architecture, and pipelines. Principal Responsibilities: Consulting clients on cloud-first strategies for core bet-the-company data initiatives Providing thought leadership on both process and technical matters Becoming a real champion and trusted advisor to our clients on all facets of Data Engineering Designing, developing, deploying, and supporting the modernization and transformation of our client‚Äôs end-to-end data strategy, including infrastructure, collection, transmission, processing, and analytics Mentoring and educating client‚Äôs teams to keep them up to speed with the latest approaches, tools and skills, and setting them up for continued success post delivery Required Experience and Skills: MUST have either Microsoft Certified Azure Data Engineer Associate or Fabric Data Engineer Associate. MUST have experience working in consulting or contracting capacity in large data management and modernization programs. Experience with SQL Servers, data engineering, on platforms such as Azure Data Factory, Databricks, Data Lake, and Synapse. Strong knowledge and demonstrated experience with Delta Lake and Lakehouse Architecture. Strong knowledge of securing Azure environment, such as RBAC, Key Vault, and Azure Security Center. Strong knowledge of Kafka and Spark and extensive experience using them in a production environment. Strong and demonstrable experience as DBA in large-scale MS SQL environments deployed in Azure. Strong problem-solving skills, with the ability to get to the route of an issue quickly. Strong knowledge of Scala or Python. Strong knowledge of Linux administration and networking. Scripting skills and Infrastructure as Code (IaC) experience using PowerShell, Bash, and ARM templates. Understanding of security and corporate governance issues related with cloud-first data architecture, as well as accepted industry solutions. Experience in enabling continuous delivery for development teams using scripted cloud provisioning and automated tooling. Experience working with Agile development methodology that is fit for purpose. Sound business judgment and demonstrated leadership",Manager,,"Python, SQL",
4247971524,Data Engineer,CodeVyasa,Greater Bengaluru Area (On-site),On-site,Full-time,,"About the job Job Summary We are seeking a talented Data Engineer (3+yrs) to join our team. If you're passionate about coding, problem-solving, and innovation, we'd love to hear from you! About Us CodeVyasa is a mid-sized product engineering company that works with top-tier product/solutions companies such as McKinsey, Walmart, RazorPay, Swiggy, and others. We are about 550+ people strong and we cater to Product & Data Engineering use-cases around Agentic AI, RPA, Full-stack and various other GenAI areas. What you will be doing: ‚óè Architect and Implement Data Infrastructure: Design, build, and maintain robust and scalable data pipelines and a data warehouse/lake solution using open-source and cloud-based technologies, optimized for both high-frequency small file and large file data ingestion, and real-time data streams. This includes implementing efficient mechanisms for handling high volumes of data arriving at frequent intervals. ‚óè Develop and Optimize Data Processes: Create custom tools, primarily using Python, for data validation, processing, analysis, and automation. Continuously improve ETL/ELT processe s for efficiency, reliability, and scalability. This includes building processes to bridge gaps between different databases and data sources, ensuring data consistency and accessibility. This also includes processing and integrating data from streaming sources. ‚óè Lead and Mentor: Collaborate with product, engineering, and business teams to understand data requirements and provide data-driven solutions. Mentor and guide junior data engineers (as the team grows) and foster a culture of data excellence. ‚óè Data Quality and Governance: Proactively identify and address data quality issues. Implement and maintain robust data quality monitoring, alerting, and measurement systems to ensure the accuracy, completeness, and consistency of our data assets. Implement and enforce data governance and security best practices, taking proactive ownership. ‚óè Research: Research and adapt newer technologies to suit the requirements. Must-have product-based experience or startup experience. Why Join CodeVyasa? Work on innovative, high-impact projects with a team of top-tier professionals. Continuous learning opportunities and professional growth. Flexible work environment with a supportive company culture. Competitive salary and comprehensive benefits package. Free healthcare coverage. Here's a glimpse of what life at CodeVyasa looks like Life at CodeVyasa .",,,Python,
4237454194,Data Engineer-Data Platforms,IBM,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As a BigData Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs Preferred Education Master's Degree Required Technical And Professional Expertise Big Data Developer, Hadoop, Hive, Spark, PySpark, Strong SQL. Ability to incorporate a variety of statistical and machine learning techniques. Basic understanding of Cloud (AWS,Azure, etc) . Ability to use programming languages like Java, Python, Scala, etc., to build pipelines to extract and transform data from a repository to a data consumer Ability to use Extract, Transform, and Load (ETL) tools and/or data integration, or federation tools to prepare and transform data as needed. Ability to use leading edge tools such as Linux, SQL, Python, Spark, Hadoop and Java Preferred Technical And Professional Experience Basic understanding or experience with predictive/prescriptive modeling skills You thrive on teamwork and have excellent verbal and written communication skills. Ability to communicate with internal and external clients to understand and define business needs, providing analytical solutions",,,"Python, SQL, Machine Learning",
4243631407,"Data Engineer(SQL,Scala,Spark,Pyspark)",Tata Consultancy Services,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Role: Data Engineer (Scala) Must Have Experience: 5+yrs Overall Exp, 3+ yrs Relevant Exp Must Have skills: Spark, SQL, Scala Spark, SQL, Pyspark Good To have : AWS, EMR, S3, Hadoop, Ctrl M. Key responsibilities (please specify if the position is an individual one or part of a team): 1) Should be able to design strategies and programs to collect, store, analyse and visualize data from various sources. 2) Should be able to develop big data solution recommendations and ensure implementation of the chosen big data solution. 3) Needs to be able to program, preferably in different programming/scripting languages such as Scala, Python, Java, Pig or SQL. 4) Proficient knowledge in Big data frameworks Spark, Map Reduce, 5) Should have an understanding of Hadoop, Hive, HBase, MongoDB and/or MapReduce. 6) Should also have experience with one of the large cloud-computing infrastructure solutions like Amazon Web Services or Elastic MapReduce. 7) Tuning the Spark Engine for high volume of data ( approx billion records) processing using BDM. 8) Troubleshoot data issues, deep dive into root cause analysis of any performance issue.",,,"Python, SQL",
4243170836,Data Engineer _Pune,Luxoft,"Pune, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Project Description: Are you passionate about leveraging the latest technologies for strategic change? Do you enjoy problem solving in clever ways? Are you organized enough to drive change across complex data systems? If so, you could be the right person for this role. As an experienced data engineer, you will join a global data analytics team in our Group Chief Technology Officer / Enterprise Architecture organization supporting our strategic initiatives which ranges from portfolio health to integration. Responsibilities: ‚Ä¢ Help Group Enterprise Architecture team to develop our suite of EA tools and workbenches ‚Ä¢ Work in the development team to support the development of portfolio health insights ‚Ä¢ Build data applications from cloud infrastructure to visualization layer ‚Ä¢ Produce clear and commented code ‚Ä¢ Produce clear and comprehensive documentation ‚Ä¢ Play an active role with technology support teams and ensure deliverables are completed or escalated on time ‚Ä¢ Provide support on any related presentations, communications, and trainings ‚Ä¢ Be a team player, working across the organization with skills to indirectly manage and influence ‚Ä¢ Be a self-starter willing to inform and educate others Mandatory Skills Description: ‚Ä¢ B.Sc./M.Sc. degree in computing or similar ‚Ä¢ 5-8+ years' experience as a Data Engineer, ideally in a large corporate environment ‚Ä¢ In-depth knowledge of SQL and data modelling/data processing ‚Ä¢ Strong experience working with Microsoft Azure ‚Ä¢ Experience with visualisation tools like PowerBI (or Tableau, QlikView or similar) ‚Ä¢ Experience working with Git, JIRA, GitLab ‚Ä¢ Strong flair for data analytics ‚Ä¢ Strong flair for IT architecture and IT architecture metrics ‚Ä¢ Excellent stakeholder interaction and communication skills ‚Ä¢ Understanding of performance implications when making design decisions to deliver performant and maintainable software. ‚Ä¢ Excellent end-to-end SDLC process understanding. ‚Ä¢ Proven track record of delivering complex data apps on tight timelines ‚Ä¢ Fluent in English both written and spoken. ‚Ä¢ Passionate about development with focus on data and cloud ‚Ä¢ Analytical and logical, with strong problem solving skills ‚Ä¢ A team player, comfortable with taking the lead on complex tasks ‚Ä¢ An excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences ‚Ä¢ Comfortable with working in cross-functional global teams to effect change ‚Ä¢ Passionate about learning and developing your hard and soft professional skills Nice-to-Have Skills Description: ‚Ä¢ Experience working in the financial industry ‚Ä¢ Experience in complex metrics design and reporting ‚Ä¢ Experience in using artificial intelligence for data analytics",,,"SQL, Tableau",
4249156582,Power BI Developer Intern,Lead India,India (Remote),Remote,Full-time,,"About the job Lead India is an innovative IT solutions provider specializing in software development, data analytics, and digital transformation services. We are committed to empowering emerging talent by providing real-world experience in cutting-edge technologies and tools. Role Overview: We are looking for a proactive and analytical Power BI Developer Intern to join our remote team. In this role, you'll assist in creating interactive dashboards, visualizations, and reports that provide actionable insights to internal teams and clients. Key Responsibilities: Design and develop interactive dashboards and visual reports using Power BI Connect Power BI to various data sources (Excel, SQL databases, APIs, etc.) Perform data cleaning, transformation, and modeling using Power Query and DAX Collaborate with teams to understand business requirements and translate them into technical solutions Optimize dashboards for performance and usability Maintain documentation related to Power BI projects and data sources Required Skills: Basic to intermediate knowledge of Power BI Strong understanding of data visualization principles and dashboard design Experience with DAX and Power Query Good analytical and problem-solving skills Familiarity with Excel and relational databases (e.g., SQL Server, MySQL) Ability to work independently and meet deadlines in a remote environment Nice to Have: Knowledge of data warehousing concepts Experience with cloud data platforms (Azure, Google BigQuery, etc.) Understanding of basic statistics and data storytelling Prior academic or freelance projects using Power BI What You'll Gain: Hands-on experience building dashboards and reports for real projects Mentorship from experienced BI professionals Flexible working hours and remote setup Internship certificate and Letter of Recommendation Opportunity for a full-time role based on performance You will get stipend Upto 23,000/- Per month.",,,"SQL, Excel, Power BI",
4229165064,Financial Markets- Data Engineer,PwC Acceleration Centers in India,"Bengaluru, Karnataka, India (On-site)",On-site,Full-time,,"About the job At PwC, our people in finance consulting specialise in providing consulting services related to financial management and strategy. These individuals analyse client needs, develop financial solutions, and offer guidance and support to help clients optimise their financial performance, improve decision-making, and achieve their financial goals. As a finance consulting generalist at PwC, you will possess a broad understanding of various aspects of finance consulting. Your work will involve providing comprehensive guidance and support to clients in optimising their financial performance, improving decision-making, and achieving their financial goals. You will be responsible for analysing client needs, developing financial solutions, and offering recommendations tailored to specific business requirements. Driven by curiosity, you are a reliable, contributing member of a team. In our fast-paced environment, you are expected to adapt to working with a variety of clients and team members, each presenting varying challenges and scope. Every experience is an opportunity to learn and grow. You are expected to take ownership and consistently deliver quality work that drives value for our clients and success as a team. As you navigate through the Firm, you build a brand for yourself, opening doors to more opportunities. Skills Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to: Apply a learning mindset and take ownership for your own development. Appreciate diverse perspectives, needs, and feelings of others. Adopt habits to sustain high performance and develop your potential. Actively listen, ask questions to check understanding, and clearly express ideas. Seek, reflect, act on, and give feedback. Gather information from a range of sources to analyse facts and discern patterns. Commit to understanding how the business works and building commercial awareness. Learn and apply professional and technical standards (e.g. refer to specific PwC tax and audit guidance), uphold the Firm's code of conduct and independence requirements. Role Overview We are seeking a highly motivated Data Engineer - Associate to join our dynamic team. The ideal candidate will have a strong foundation in data engineering, particularly with Python and SQL, and will have exposure to cloud technologies and data visualization tools such as Power BI, Tableau, or QuickSight. The Data Engineer will work closely with data architects and business stakeholders to support the design and implementation of data pipelines and analytics solutions. This role o∆Øers an opportunity to grow technical expertise in cloud and data solutions, contributing to projects that drive business insights and innovation. Key Responsibilities Data Engineering: ÔÇ∑ Develop, optimize, and maintain data pipelines and workflows to ensure e∆Øicient data integration from multiple sources. ÔÇ∑ Use Python and SQL to design and implement scalable data processing solutions. ÔÇ∑ Ensure data quality and consistency throughout data transformation and storage processes. ÔÇ∑ Collaborate with data architects and senior engineers to build data solutions that meet business and technical requirements. Cloud Technologies ÔÇ∑ Work with cloud platforms (e.g., AWS, Azure, or Google Cloud) to deploy and maintain data solutions. ÔÇ∑ Support the migration of on-premise data infrastructure to the cloud environment when needed. ÔÇ∑ Assist in implementing cloud-based data storage solutions, such as data lakes and data warehouses. Data Visualization ÔÇ∑ Provide data to business stakeholders for visualizations using tools such as Power BI, Tableau, or QuickSight. ÔÇ∑ Collaborate with analysts to understand their data needs and optimize data structures for reporting. Collaboration And Support ÔÇ∑ Work closely with cross-functional teams, including data scientists and business analysts, to support data-driven decision-making. ÔÇ∑ Troubleshoot and resolve issues in the data pipeline and ensure timely data delivery. ÔÇ∑ Document processes, data flows, and infrastructure for team knowledge sharing. Required Skills And Experience ÔÇ∑ 0+ years of experience in data engineering, working with Python and SQL. ÔÇ∑ Exposure to cloud platforms such as AWS, Azure, or Google Cloud is preferred. ÔÇ∑ Familiarity with data visualization tools (e.g., Power BI, Tableau, QuickSight) is a plus. ÔÇ∑ Basic understanding of data modeling, ETL processes, and data warehousing concepts. ÔÇ∑ Strong analytical and problem-solving skills, with attention to detail. Qualifications ÔÇ∑ Bachelor‚Äôs degree in Computer Science, Data Science, Information Technology, or related fields. ÔÇ∑ Basic knowledge of cloud platforms and services is advantageous. ÔÇ∑ Strong communication skills and the ability to work in a team-oriented environment.",Associate,,"Python, SQL, Tableau, Power BI",
4244573315,Cloud Data Engineer,Uplers,"Kanpur, Uttar Pradesh, India (Remote)",Remote,‚Çπ1.8M/yr - ‚Çπ2.8M/yr,,"About the job Experience : 5.00 + years Salary : INR 1800000-2800000 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Intelebee LLC) (*Note: This is a requirement for one of Uplers' client - Intelebee LLC) What do you need for this opportunity? Must have skills required: Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory Intelebee LLC is Looking for: Data Engineer:We are seeking a skilled and hands-on Cloud Data Engineer with 5-8 years of experience to drive end-to-end data engineering solutions. The ideal candidate will have a deep understanding of dimensional modeling, data warehousing (DW), Lakehouse architecture, and the Medallion architecture. This role will focus on leveraging Azure's/AWS ecosystem to build scalable, efficient, and secure data solutions. You will work closely with customers to understand requirements, create technical specifications, and deliver solutions that scale across both on-premise and cloud environments. Key Responsibilities: End-to-End Data Engineering Lead the design and development of data pipelines for large-scale data processing, utilizing Azure/AWS tools such as Azure Data Factory, Azure Synapse, Azure functions, Logic Apps , Azure Databricks, and Data Lake Storage. Tools, AWS Lambda, AWS Glue Develop and implement dimensional modeling techniques and data warehousing solutions for effective data analysis and reporting. Build and maintain Lakehouse and Medallion architecture solutions for streamlined, high-performance data processing. Implement and manage Data Lakes on Azure/AWS, ensuring that data storage and processing is both scalable and secure. Handle large-scale databases (both on-prem and cloud) ensuring high availability, security, and performance. Design and enforce data governance policies for data security, privacy, and compliance within the Azure ecosystem. 5 How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory",,,Data Analysis,
4240897606,Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job To be responsible for data modelling, design, and development of the batch and real-time extraction, load, transform (ELT) processes, and the setup of the data integration framework, ensuring best practices are followed during the integration development. Bachelors degree in CS/IT or related field (minimum) Azure Data Engineer (ADF, ADSL, MS Fabric), Databricks Azure DevOps, Confluence Desired Skills and Experience Python",,,Python,
4249788004,Big Data Engineer,Impetus Career Consultants,"Chennai, Tamil Nadu, India (Hybrid)",Hybrid,Full-time,,"About the job We are hiring for one the IT big4 consulting Designation: - Associate/Associate Consultant Location : - Chennai/Gurgaon/Pune Skills Req-: AWS (Big Data services) - S3, Glue, Athena, EMR Programming - Python, Spark, SQL, Mulesoft,Talend, Dbt Data warehouse - ETL, Redshift / Snowflake Key Responsibilities: - Work with business stakeholders to understand their business needs. - Create data pipelines that extract, transform, and load (ETL) from various sources into a usable format in a Data warehouse. - Clean, filter, and validate data to ensure it meets quality and format standards. - Develop data model objects (tables, views) to transform the data into unified format for downstream consumption. - Expert in monitoring, controlling, configuring, and maintaining processes in cloud data platform. - Optimize data pipelines and data storage for performance and efficiency. - Participate in code reviews and provide meaningful feedback to other team members. - Provide technical support and troubleshoot issue(s). Qualifications: - Bachelor‚Äôs degree in computer science, Information Technology, or a related field, or equivalent work experience. - Experience working in the AWS cloud platform. - Data engineer with expertise in developing big data and data warehouse platforms. - Experience working with structured and semi-structured data. - Expertise in developing big data solutions, ETL/ELT pipelines for data ingestion, data transformation, and optimization techniques. - Experience working directly with technical and business teams. - Able to create technical documentation. - Excellent problem-solving and analytical skills. - Strong communication and collaboration abilities. Skillset (good to have) - Experience in data modeling. - Certified in AWS platform for Data Engineer skills. - Experience with ITSM processes/tools such as ServiceNow, Jira - Understanding of Spark, Hive, Kafka, Kinesis, Spark Streaming, and Airflow",Executive,,"Python, SQL",
4247768532,Azure Data Engineer - Remote work & Immediate Joiners are Required,techolution,India (Remote),Save Azure Data Engineer - Remote work & Immediate Joiners are Required¬†  at techolution,Full-time,,"About the job We are looking for a Azure Data Engineer with a driving passion to ensure that our customers have the most pleasant experience using our platform.This position will directly contribute to the WoW customer experience by consistently delivering the best quality of work. Our ideal candidate enjoys a work environment that requires strong problem solving skills and independent self-direction, coupled with an aptitude for team collaboration and open communication. Title: Azure Data Enginee rLocation: Remote Shift: 2:00 PM-11: 00 PM IS T Please note : This is pure Azure specific role, if your expertise is into AWS/GCP. Please avoid to apply for this rol e. Key Responsibilit iesDesign and implement robust data pipelines usi ng Azure Data Fact ory, ensuring seamless data flow across enterprise syste ms.Le ad data migrat ion initiatives, translating complex business requirements into efficient and scalable ETL process es.Architect and optimi ze Azure Data L ake solutions to support scalable storage and advanced analytics for big data workloa ds.Develop high-performan ce data transformation scri pts usi ng Pyt hon a nd PySp ark, enhancing data processing efficien cy.Write compl ex SQL quer ies and stored procedures to extract actionable insights from diverse data sourc es.Troubleshoot data integration issues and optimize system performance using advanc ed problem-solv ing techniqu es.Collaborate with cross-functional teams to align data engineering solutions with business objectives, demonstrating strong ownership and communication skil ls.Stay up to date with emergi ng Azure technolog ies and implement innovative solutions, embodying a continuous learning minds et.Deliver data-driven insights through innovati ve data model ing techniques to support informed decision-maki ng.Mentor junior engine ers, promoting a culture of excellence and continuous improvement in data engineering practic es.Foundational Ski llsAzure Data Fact ory: Proven expertise in designing, implementing, and managing scalable data integration workflo ws.Data Migrat ion: Successful track record in planning and executing large-scale migrations with a focus on data integrity and minimal downti me.Azure Data L ake: Deep understanding of architecture and best practices for scalable data storage and processi ng.Pyt hon: Strong programming skills for data manipulation, automation, and engineering workflo ws.PySp ark: Hands-on experience in distributed data processing for efficient big data handli ng. SQL: Advanced skills in query writing, data manipulation, and performance tuning across multiple database platfor ms.Analytical Think ing: Exceptional ability to resolve complex engineering challenges with scalable, efficient solutio ns. About Techolut ion:Techolution is a leading innovation consulting company on track to become one of the most admired brands in the world for ""innovation done right"". Our purpose is to harness our expertise in novel technologies to deliver more profits for our enterprise clients while helping them deliver a better h umanexperience for the communities they serve. With that, we are now fully committed to helping our clients build the enterprise of tomorrow by making the leap from Lab Grade AI to Real World AI. In 2019, we won the prestigious Inc. 500 Fastest-Growing Companies in America award, only 4 years after its formation. In 2022, Techolution was honored with the ‚ÄúBest-in-Business‚Äù title by Inc. for ‚ÄúInnovation Done Right‚Äù. Most recently, we received the ‚ÄúAIConics‚Äù trophy for being the Top AI Solution Provider of the Year at the AI Summit in New Y ork.Let‚Äôs give you more insig hts!One of our amazing products with Artificial Intellige nce :1. https://faceopen. co m / : Our proprietary and powerful AI Powered user identification system which is built on artificial intelligence technologies such as image recognition, deep neural networks, and robotic process automation. (No more touching keys, badges or fingerprint scanners ever aga in!)Some videos you wanna wa tch!Life at Techolu tionGoogleNext 2023Ai4 - Artificial Intelligence Conferences 2023WaWa - Solving Food Was t age Saving lives - Brooklyn Hosp italInnovation Done Right on Google C loudTecholution featured on Worldwide Business with KathyIre landTecholution presented by ION World‚Äôs Grea test Visi t us @www.techolutio n.c om : To know more about our revolutionary core practices and getting to know in detail about how we enrich the human experience with techno logy.",,,SQL,
4227089833,Associate Data Engineer,Amgen,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job About Amgen Amgen harnesses the best of biology and technology to fight the world‚Äôs toughest diseases, and make people‚Äôs lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what‚Äôs known today. About The Role Role Description: We are looking for an Associate Data Engineer with deep expertise in writing data pipelines to build scalable, high-performance data solutions. The ideal candidate will be responsible for developing, optimizing and maintaining complex data pipelines, integration frameworks, and metadata-driven architectures that enable seamless access and analytics. This role prefers deep understanding of the big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management. Roles & Responsibilities: Data Engineer who owns development of complex ETL/ELT data pipelines to process large-scale datasets Contribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions Ensuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring Exploring and implementing new tools and technologies to enhance ETL platform and performance of the pipelines Proactively identify and implement opportunities to automate tasks and develop reusable frameworks Eager to understand the biotech/pharma domains & build highly efficient data pipelines to migrate and deploy complex data across systems Work in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value Use JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories. Support continuous improvement, test automation, and DevOps practices in the data engineering lifecycle Collaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions Must-Have Skills: Experience in Data Engineering with a focus on Databricks, AWS, Python, SQL, and Scaled Agile methodologies Proficiency & Strong understanding of data processing and transformation of big data frameworks (Databricks, Apache Spark, Delta Lake, and distributed computing concepts) Strong understanding of AWS services and can demonstrate the same Ability to quickly learn, adapt and apply new technologies Strong problem-solving and analytical skills Excellent communication and teamwork skills Experience with Scaled Agile Framework (SAFe), Agile delivery, and DevOps practices Good-to-Have Skills: Data Engineering experience in Biotechnology or pharma industry Exposure to APIs, full stack development Experienced with SQL/NOSQL database, vector database for large language models Experienced with data modeling and performance tuning for both OLAP and OLTP databases Experienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops Education and Professional Certifications Any degree and 2-5 years of experience AWS Certified Data Engineer preferred Databricks Certificate preferred Scaled Agile SAFe certification preferred Soft Skills: Excellent analytical and troubleshooting skills. Strong verbal and written communication skills Ability to work effectively with global, virtual teams High degree of initiative and self-motivation. Ability to manage multiple priorities successfully. Team-oriented, with a focus on achieving team goals. Ability to learn quickly, be organized and detail oriented. Strong presentation and public speaking skills. EQUAL OPPORTUNITY STATEMENT Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.",Associate,,"Python, SQL",
4250113691,Data Engineer,Intellectt Inc,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job We are looking for a hands-on Data Engineer who blends strong data engineering foundations with modern application development skills. This role is ideal for someone who can design and build scalable data pipelines and backend services independently while working on long-term platform-level initiatives. Key Responsibilities: Design, develop, and optimize data pipelines using Spark, Pandas, and Databricks Build and deploy backend services & APIs to power data-driven applications Work with Azure cloud infrastructure to ensure scalable and secure data operations Collaborate with cross-functional teams to solve real-world business problems using data Own your solutions end-to-end‚Äîfrom design to deployment and monitoring Maintain code quality, performance tuning, and documentation Required Skills: Strong programming skills in Python (advanced-level preferred) Experience with Spark and Pandas for data transformation and processing Hands-on experience with Databricks and Azure (Data Factory, Blob, etc.) Solid understanding of API/Service development using Python (Flask/FastAPI preferred) Good grasp of data engineering principles, version control (Git), and CI/CD workflows Ability to work independently, think critically, and contribute to strategic initiatives Preferred Qualities: Self-starter with a learning mindset and curiosity to solve complex problems Adaptable to evolving technologies‚Äînot limited by fixed tools or tech stacks Experience in designing long-term, reusable data platform components",Manager,,Python,
4233715116,Data Engineer + Power BI - Immediate Joiner,Deloitte,"Mumbai, Maharashtra, India (On-site)",On-site,Full-time,,"About the job Notice Period - Immediate Joiner to 30 Days Location - Mumbai / Pune/Bangalore/Delhi/Hyderabad/Chennai Responsibilities: ‚Ä¢ 2+ yrs of experience with Microsoft Azure and Microsoft BI stack ‚Ä¢ Experience developing Azure Data Factory, SQL, Databricks (PySpark, Scala, SQL), Stream Analytics, Components experience in working on data lake & DW solutions on Azure ‚Ä¢ Experience in configuring and managing Azure DevOps pipelines ‚Ä¢ Lead and implement data pipelines for software solutions and ensure that technology deliverables are met based on business requirements ‚Ä¢ ETL Architecture design and implementation at enterprise level ‚Ä¢ Work with subject matter experts and Data/ Business Analysts to understand business data and related processes ‚Ä¢ Using Agile and DevOps methods, build platform architecture using a variety of sources (on-premises such as SQL Server, cloud IaaS/ SaaS such as Azure). ‚Ä¢ Implement rules & automate data cleansing, mapping, transformation, logging, and exception handling. ‚Ä¢ Experience in designing and implementing BI solution on Azure, (ADLS, Azure Databricks, ADF, Azure Synapse, Azure SQL, etc.) ‚Ä¢ Design, build, and deploy databases and data stores ‚Ä¢ Participate in cross-functional teams to promote technology strategies, analyze, and test products, perform proofs-of-concept, and pilot new technologies and/or methods. ‚Ä¢ Establish and document standards, guidelines, and best practices for teams utilizing the solutions. ‚Ä¢ Implement / operationalize Analytics by developing representations and visualizations in BI. ‚Ä¢ Work with clients (internal or external) to determine business requirements, priorities, define key performance indicators (KPI) ‚Ä¢ Data management experience, strong excel skills (pivot tables, VLOOKUP‚Äôs, etc.), stronghold in Power BI ‚Ä¢ Design, and document dashboards, alerts, and reports either on a regular recurring basis or as needed. Primary Skillsets: Azure Data Factory, Azure Data Lake Storage,Azure Databricks,Azure Synapse ,SQL Server,Azure Event Hub,Azure Function,Azure App Services Secondary Skillset: Power Apps,Microsoft SharePoint,Power Automate,Dynamic 365,Understanding of DevOps practices,Demonstrates teamwork and collaboration,Passionate about new technologies, Certification in DP-200, DP-201, DP-203, DA-100, PL-300, AZ-900 Desired qualifications ‚Ä¢ Education: bachelor‚Äôs degree in B Tech, MCA and Software Engineering, or a related field. Advanced degrees are a plus. Experience: 2+ years of experience in software development.",,,"SQL, Excel, Power BI",
4245423092,Data Engineer-Data Platforms-AWS,IBM,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Experience in Big Data Technology like Hadoop, Apache Spark, Hive. Practical experience in Core Java (1.8 preferred) /Python/Scala. Having experience in AWS cloud services including S3, Redshift, EMR etc. Strong expertise in RDBMS and SQL. Good experience in Linux and shell scripting. Experience in Data Pipeline using Apache Airflow Preferred Technical And Professional Experience You thrive on teamwork and have excellent verbal and written communication skills. Ability to communicate with internal and external clients to understand and define business needs, providing analytical solutions Ability to communicate results to technical and non-technical audiences",Associate,,"Python, SQL, Machine Learning",
4248023370,Data Engineer,bp,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job About bp/team Bp's Technology organization is the central organization for all software and platform development. We build all the technology that powers bp‚Äôs businesses, from upstream energy production to downstream energy delivery to our customers. We have a variety of teams depending on your areas of interest, including infrastructure and backend services through to customer-facing web and native applications. We encourage our teams to adapt quickly by using native AWS and Azure services, including serverless, and enable them to pick the best technology for a given problem. This is meant to empower our software and platform engineers while allowing them to learn and develop themselves. Responsibilities Part of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners. Architects, designs, implements and maintains reliable and scalable data infrastructure to move, process and serve data. Writes, deploys and maintains software to build, integrate, manage, maintain, and quality-assure data at bp. Adheres to and advocates for software engineering standard methodologies (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation) Responsible for deploying secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI / CD pipeline. Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments. Actively contributes to improve developer velocity. Mentors' others. Qualifications BS degree or equivalent experience in computer science or related field Deep and hands-on experience designing, planning, building, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments Development experience in one or more object-oriented programming languages (e.g. Python, Scala, Java, C#) Sophisticated database and SQL knowledge Experience designing and implementing large-scale distributed data systems Deep knowledge and hands-on experience in technologies across all data lifecycle stages Strong stakeholder management and ability to lead initiatives through technical influence Continuous learning and improvement mindset Desired No prior experience in the energy industry required Skills: Commercial Acumen, Communication, Data Analysis, Data cleansing and transformation, Data domain knowledge, Data Integration, Data Management, Data Manipulation, Data Sourcing, Data strategy and governance, Data Structures and Algorithms (Inactive), Data visualization and interpretation, Digital Security, Extract, transform and load, Group Problem Solving Legal Disclaimer: We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, socioeconomic status, neurodiversity/neurocognitive functioning, veteran status or disability status. Individuals with an accessibility need may request an adjustment/accommodation related to bp‚Äôs recruiting process (e.g., accessing the job application, completing required assessments, participating in telephone screenings or interviews, etc.). If you would like to request an adjustment/accommodation related to the recruitment process, please contact us . If you are selected for a position and depending upon your role, your employment may be contingent upon adherence to local policy. This may include pre-placement drug screening, medical review of physical fitness for the role, and background checks.",manager,,"Python, SQL, Data Analysis",
4240039512,Data Engineer-Data Integration,IBM,"Pune, Maharashtra, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology Your Role And Responsibilities As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You‚Äôll contribute to data gathering, storage, and both batch and real-time processing. Collaborating closely with diverse teams, you‚Äôll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you‚Äôll tackle obstacles related to database integration and untangle complex, unstructured data sets. In This Role, Your Responsibilities May Include Implementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques Designing and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour‚Äôs. Build teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results Preferred Education Master's Degree Required Technical And Professional Expertise Expertise in designing and implementing scalable data warehouse solutions on Snowflake, including schema design, performance tuning, and query optimization. Strong experience in building data ingestion and transformation pipelines using Talend to process structured and unstructured data from various sources. Proficiency in integrating data from cloud platforms into Snowflake using Talend and native Snowflake capabilities. Hands-on experience with dimensional and relational data modelling techniques to support analytics and reporting requirements Preferred Technical And Professional Experience Understanding of optimizing Snowflake workloads, including clustering keys, caching strategies, and query profiling. Ability to implement robust data validation, cleansing, and governance frameworks within ETL processes. Proficiency in SQL and/or Shell scripting for custom transformations and automation tasks",,,"SQL, Machine Learning",
4245486658,Data Partner - Mechanical Engineer - Remote - Asia,TELUS Digital,India (Remote),Save Data Partner - Mechanical Engineer - Remote - Asia¬†  at TELUS Digital,Full-time,,"About the job Requirements Description and Requirements Location: Asia - Remote Are you ready to use your domain knowledge to advance AI? Join us as a Freelance Data Partner and work remotely with flexible hours. We are seeking highly knowledgeable Subject Matter Expert to design advanced, domain-specific questions and solutions. The role involves creating challenging problem sets that test deep reasoning and expertise in the assigned field. Key Responsibilities: Develop complex, original question-and-answer pairs based on advanced topics in your area of expertise. Ensure questions involve multi-step problem-solving and critical thinking. Provide detailed, clear solutions that meet high academic standards. Collaborate with cross-functional teams to refine and enhance content. Basic Requirements: A completed Masters Degree or Bachelors Degree or Associates Degree in Mechanical Engineering is essential Strong proficiency in English writing with excellent grammar, clarity, and the ability to explain complex concepts concisely Previous experience working in similar AI projects is advantageous Assessment: In order to be hired into the program, you‚Äôll go through a subject-specific qualification exam that will determine your suitability for the position and complete ID verification. Payment : Experts pay rates typically range from $10 to $12 USD per hour . Rates may vary depending on several factors, including: Level of expertise and education (e.g., Masters Degree holders may qualify for higher rates) Results of skills assessments Geographic location Specific project requirements and urgency Other relevant considerations Please review the payment terms listed for each individual project , as they may specify a different rate within or outside this range. TELUS Digital AI Community Our global AI Community is a vibrant network of 1 million+ contributors from diverse backgrounds who help our customers collect, enhance, train, translate, and localize content to build better AI models. Become part of our growing community and make an impact supporting the machine learning models of some of the world‚Äôs largest brands. Additional Job Description Join us as a Freelance Data Partner to work remotely on impactful AI projects. Use your expertise to create domain-specific content and gain hands-on experience in the AI space. EEO Statement At TELUS International, we enable customer experience innovation through spirited teamwork, agile thinking, and a caring culture that puts customers first. TELUS International is the global arm of TELUS Corporation, one of the largest telecommunications service providers in Canada. We deliver contact center and business process outsourcing (BPO) solutions to some of the world's largest corporations in the consumer electronics, finance, telecommunications and utilities sectors. With global call center delivery capabilities, our multi-shore, multi-language programs offer safe, secure infrastructure, value-based pricing, skills-based resources and exceptional customer service - all backed by TELUS, our multi-billion dollar telecommunications parent. Equal Opportunity Employer At TELUS International, we are proud to be an equal opportunity employer and are committed to creating a diverse and inclusive workplace. All aspects of employment, including the decision to hire and promote, are based on applicants‚Äô qualifications, merits, competence and performance without regard to any characteristic related to diversity.",Associate,,Machine Learning,
4231478265,Data Engineer,"TWO95 International, Inc","Gurugram, Haryana, India (Hybrid)",Hybrid,Full-time,,"About the job Job Title: Data Engineer Location: Hybrid / Gurugram, India Type: Full Time with our client Salary: Market Job Purpose: As a Data Engineer.. You will help in developing high performance & high scalability enterprise applications using one or more leading cloud platforms across SaaS and PaaS. You will be working closely with the data architects to jointly develop the data architecture, ensure security and maintenance. Your role involves diagnosis of existing architecture and data maturity and help the organization in identifying gaps and possible solutions. You will also work with data scientists and business leadership to develop data pipelines for data science models. Your work will involve high level of interface with business heads and corporate leadership. Profile & Eligible Criteria: Understanding of how to design technological solutions to complex data problems, developing & testing modular, reusable, efficient and scalable code to implement those solutions Understand, implement, and automate ETL pipelines with better industry standards Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, design infrastructure for greater scalability, etc. Developing, integrating, testing, and maintaining existing and new applications Design, and create data pipelines (data lake / data warehouses) for real world energy analytical solutions Expert-level proficiency in Python (preferred) for automating everyday tasks Strong understanding in distributed computing frameworks, particularly Spark, Spark-SQL, Kafka, Spark Streaming, Hive, Azure Databricks etc Knowledge in using other leading cloud platforms preferably Azure. Knowledge of Azure data factory, logic app, Analysis service, Azure blob storage etc. Ability to work in a team in an agile setting, familiarity with JIRA and clear understanding of how Git works Main Interfaces: The role would require extensive internal interfacing with tech lead, senior data scientists, business leadership.",,,"Python, SQL",
4241889319,Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Responsibilities Design and Develop Scalable Data Pipelines: Build and maintain robust data pipelines using Python to process, transform, and integrate large-scale data from diverse sources. Orchestration and Automation: Implement and manage workflows using orchestration tools such as Apache Airflow to ensure reliable and efficient data operations. Data Warehouse Management: Work extensively with Snowflake to design and optimize data models, schemas, and queries for analytics and reporting. Queueing Systems: Leverage message queues like Kafka, SQS, or similar tools to enable real-time or batch data processing in distributed environments. Collaboration: Partner with Data Science, Product, and Engineering teams to understand data requirements and deliver solutions that align with business objectives. Performance Optimization: Optimize the performance of data pipelines and queries to handle large scales of data efficiently. Data Governance and Security: Ensure compliance with data governance and security standards to maintain data integrity and privacy. Documentation: Create and maintain clear, detailed documentation for data solutions, pipelines, and workflows. Qualifications Required Skills: 5+ years of experience in data engineering roles with a focus on building scalable data solutions. Proficiency in Python for ETL, data manipulation, and scripting. Hands-on experience with Snowflake or equivalent cloud-based data warehouses. Strong knowledge of orchestration tools such as Apache Airflow or similar. Expertise in implementing and managing messaging queues like Kafka, AWS SQS, or similar. Demonstrated ability to build and optimize data pipelines at scale, processing terabytes of data. Experience in data modeling, data warehousing, and database design. Proficiency in working with cloud platforms like AWS, Azure, or GCP. Strong understanding of CI/CD pipelines for data engineering workflows. Experience working in an Agile development environment, collaborating with cross-functional teams. Preferred Skills Familiarity with other programming languages like Scala or Java for data engineering tasks. Knowledge of containerization and orchestration technologies (Docker, Kubernetes). Experience with stream processing frameworks like Apache Flink. Experience with Apache Iceberg for data lake optimization and management. Exposure to machine learning workflows and integration with data pipelines. Soft Skills Strong problem-solving skills with a passion for solving complex data challenges. Excellent communication and collaboration skills to work with cross-functional teams. Ability to thrive in a fast-paced, innovative environment. Desired Skills and Experience CI & CD, ETL",,,"Python, Machine Learning",
4244570678,Cloud Data Engineer,Uplers,"Kolkata, West Bengal, India (Remote)",Remote,‚Çπ1.8M/yr - ‚Çπ2.8M/yr,,"About the job Experience : 5.00 + years Salary : INR 1800000-2800000 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: Intelebee LLC) (*Note: This is a requirement for one of Uplers' client - Intelebee LLC) What do you need for this opportunity? Must have skills required: Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory Intelebee LLC is Looking for: Data Engineer:We are seeking a skilled and hands-on Cloud Data Engineer with 5-8 years of experience to drive end-to-end data engineering solutions. The ideal candidate will have a deep understanding of dimensional modeling, data warehousing (DW), Lakehouse architecture, and the Medallion architecture. This role will focus on leveraging Azure's/AWS ecosystem to build scalable, efficient, and secure data solutions. You will work closely with customers to understand requirements, create technical specifications, and deliver solutions that scale across both on-premise and cloud environments. Key Responsibilities: End-to-End Data Engineering Lead the design and development of data pipelines for large-scale data processing, utilizing Azure/AWS tools such as Azure Data Factory, Azure Synapse, Azure functions, Logic Apps , Azure Databricks, and Data Lake Storage. Tools, AWS Lambda, AWS Glue Develop and implement dimensional modeling techniques and data warehousing solutions for effective data analysis and reporting. Build and maintain Lakehouse and Medallion architecture solutions for streamlined, high-performance data processing. Implement and manage Data Lakes on Azure/AWS, ensuring that data storage and processing is both scalable and secure. Handle large-scale databases (both on-prem and cloud) ensuring high availability, security, and performance. Design and enforce data governance policies for data security, privacy, and compliance within the Azure ecosystem. 5 How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Data Governance, Lakehouse architecture, Medallion Architecture, AWS Lambda, Azure DataBricks, Azure synapse, Data Lake Storage, Azure Data Factory",,,Data Analysis,
4178281814,Data Quality Engineer,Anblicks,"Hyderabad, Telangana, India",,Full-time,,"About the job Summary We are seeking a motivated and detail-oriented Data Quality Engineer to join our team and contribute to ensuring the accuracy, completeness, and reliability of our data. In this role, you will be responsible for implementing data quality checks, monitoring data quality, and identifying and resolving data quality issues. Experience with SODA (or similar data quality frameworks) is preferred. Responsibilities Implement and maintain data quality checks and validations using various tools and techniques. Develop and execute data quality test plans and test cases. Automate data quality processes, including data profiling, data quality checks, and reporting. Contribute to the implementation and maintenance of data quality frameworks and tools. (Preferred) Utilize SODA (or similar frameworks) to define data quality checks, configure data sources, and generate data quality reports. Collaborate with data engineers, data analysts, and other stakeholders to ensure data quality requirements are met. Communicate data quality issues and findings to relevant stakeholders. Investigate and analyze data quality problems to determine root causes.",,,,
4249393424,Data Engineer,Tata Consultancy Services,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Hi {fullName} There is an opportunity for Data Engineer IN KOLKATA for which WALKIN interview at KOLKATA is there on 21st JUN 25 between 9:30 AM to 12:30 PM PLS SHARE below details to mamidi.p@tcs.com with subject line as Data Engineer 21st jun 25 if you are interested Email id: Contact no: Total EXP: Preferred Location: CURRENT CTC: EXPECTED CTC: NOTICE PERIOD: CURRENT ORGANIZATION: HIGHEST QUALIFICATION THAT IS FULL TIME : HIGHEST QUALIFICATION UNIVERSITY: ANY GAP IN EDUCATION OR EMPLOYMENT: IF YES HOW MANY YEARS AND REASON FOR GAP: ARE U AVAILABLE FOR WALKIN INTERVIEW AT KOLKATA ON 21ST JUN 25(YES/NO): Desired Competencies (Technical/Behavioral Competency) Must-Have** (Ideally should not be more than 3-5) 1. Azure Data Factory 2. Azure Data Bricks 3. Python 4. Sql Query writing Good-to-Have 1. Pysspark 2. SQL query optimization 3. Power shell SN Responsibility of / Expectations from the Role 1 Developing/design solution from detail design specification. 2 Playing an active role in defining standard in coding, system design and architecture. 3 Revise, refactor, update and debug code. 4 Customer interaction. 5 Must have strong technical background and hands on coding experience in Azure Data Factory. Azure Databricks and SQL. THANKS & REGARDS PRIYANKA MAMIDI",,,"Python, SQL",
4095428529,Data Engineer - X Delivery,BCG X,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Locations : Bengaluru | Gurgaon Who We Are Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact. To succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital ventures‚Äîand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive. What You'll Do As a part of BCG's X team, you will work closely with consulting teams on a diverse range of advanced analytics and engineering topics. You will have the opportunity to leverage analytical methodologies to deliver value to BCG's Consulting (case) teams and Practice Areas (domain) through providing analytical and engineering subject matter expertise.As a Data Engineer, you will play a crucial role in designing, developing, and maintaining data pipelines, systems, and solutions that empower our clients to make informed business decisions. You will collaborate closely with cross-functional teams, including data scientists, analysts, and business stakeholders, to deliver high-quality data solutions that meet our clients' needs. YOU'RE GOOD AT Delivering original analysis and insights to case teams, typically owning all or part of an analytics module whilst integrating with a case team. Design, develop, and maintain efficient and robust data pipelines for extracting, transforming, and loading data from various sources to data warehouses, data lakes, and other storage solutions. Building data-intensive solutions that are highly available, scalable, reliable, secure, and cost-effective using programming languages like Python and PySpark. Deep knowledge of Big Data querying and analysis tools, such as PySpark, Hive, Snowflake and Databricks. Broad expertise in at least one Cloud platform like AWS/GCP/Azure.* Working knowledge of automation and deployment tools such as Airflow, Jenkins, GitHub Actions, etc., as well as infrastructure-as-code technologies like Terraform and CloudFormation. Good understanding of DevOps, CI/CD pipelines, orchestration, and containerization tools like Docker and Kubernetes. Basic understanding on Machine Learning methodologies and pipelines. Communicating analytical insights through sophisticated synthesis and packaging of results (including PPT slides and charts) with consultants, collecting, synthesizing, analyzing case team learning & inputs into new best practices and methodologies. Communication Skills Strong communication skills, enabling effective collaboration with both technical and non-technical team members. Thinking Analytically You should be strong in analytical solutioning with hands on experience in advanced analytics delivery, through the entire life cycle of analytics. Strong analytics skills with the ability to develop and codify knowledge and provide analytical advice where required. What You'll Bring Bachelor's / Master's degree in computer science engineering/technology At least 2-4 years within relevant domain of Data Engineering across industries and work experience providing analytics solutions in a commercial setting. Consulting experience will be considered a plus. Proficient understanding of distributed computing principles including management of Spark clusters, with all included services - various implementations of Spark preferred. Basic hands-on experience with Data engineering tasks like productizing data pipelines, building CI/CD pipeline, code orchestration using tools like Airflow, DevOps etc.Good to have:- Software engineering concepts and best practices, like API design and development, testing frameworks, packaging etc. Experience with NoSQL databases, such as HBase, Cassandra, MongoDB Knowledge on web development technologies. Understanding of different stages of machine learning system design and development #BCGXjob Who You'll Work With You will work with the case team and/or client technical POCs and border X team. Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws. BCG is an E - Verify Employer. Click here for more information on E-Verify.",,,"Python, Machine Learning",
4247148242,Data Engineer,ConnectedX Inc.,"Hyderabad, Telangana, India (On-site)",On-site,Full-time,,"About the job We are seeking a motivated Data Engineer with 1-2 years of experience to join our team. The ideal candidate will have hands-on experience in Databricks, AWS, Scala, and Spark and will be responsible for building, optimizing, and maintaining scalable data pipelines. You'll also collaborate with global teams, work with advanced cloud technologies, and contribute to a forward-thinking data ecosystem that powers the future of automotive engineering. Company profile: ConnectedX: Empowering Tech Leaders with Tailored Solutions Key Responsibilities: ¬∑ Design, develop, and maintain robust ETL pipelines for data processing. ¬∑ Work with Databricks to perform data transformations and analytics. ¬∑ Develop scalable solutions using Scala and Apache Spark for big data processing. ¬∑ Collaborate with teams to ensure efficient data flow and integration. ¬∑ Optimize data storage and processing on AWS cloud infrastructure. ¬∑ Monitor and troubleshoot performance issues in data pipelines. Required Skills & Qualifications: ¬∑ Excellent communication and ability to work collaboratively in a fast-paced environment. ¬∑ 1-2 years of experience in Data Engineering. ¬∑ Proficiency in Databricks, AWS, Scala, and Spark. ¬∑ Strong understanding of data modeling, ETL processes, and performance tuning. ¬∑ Familiarity with SQL and database management systems Who Can Apply: Only candidates who can join immediately or within 2 weeks can apply. Ideal for those seeking technical growth and work on a global project with cutting-edge technologies. Best suited for professionals passionate about innovation and problem-solving",Director,,SQL,
4219149740,"Data Engineer, FinOps FP&A, FinOps FP&A",Amazon,"Bengaluru, Karnataka, India",,Full-time,,"About the job Description Are you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to build data engineering solutions that process billions of records a day in a scalable fashion using AWS technologies? Do you want to create the next-generation tools for intuitive data access? If so, Amazon Finance Technology (FinTech) is for you! FinTech is seeking a Data Engineer to join the team that is shaping the future of the finance data platform. The team is committed to building the next generation big data platform that will be one of the world's largest finance data warehouse to support Amazon's rapidly growing and dynamic businesses, and use it to deliver the BI applications which will have an immediate influence on day-to-day decision making. Amazon has culture of data-driven decision-making, and demands data that is timely, accurate, and actionable. Our platform serves Amazon's finance, tax and accounting functions across the globe. As a Data Engineer, you should be an expert with data warehousing technical components (e.g. Data Modeling, ETL and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of large data-sets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The candidate is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions. You should be enthusiastic about learning new technologies and be able to implement solutions using them to provide new functionality to the users or to scale the existing platform. Excellent written and verbal communication skills are required as the person will work very closely with diverse teams. Having strong analytical skills is a plus. Above all, you should be passionate about working with huge data sets and someone who loves to bring data-sets together to answer business questions and drive change. Our ideal candidate thrives in a fast-paced environment, relishes working with large transactional volumes and big data, enjoys the challenge of highly complex business contexts (that are typically being defined in real-time), and, above all, is a passionate about data and analytics. In this role you will be part of a team of engineers to create world's largest financial data warehouses and BI tools for Amazon's expanding global footprint. Key job responsibilities Design, implement, and support a platform providing secured access to large datasets. Interface with tax, finance and accounting customers, gathering requirements and delivering complete BI solutions. Model data and metadata to support ad-hoc and pre-built reporting. Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation. Tune application and query performance using profiling tools and SQL. Analyze and solve problems at their root, stepping back to understand the broader context. Learn and understand a broad range of Amazon‚Äôs data resources and know when, how, and which to use and which not to use. Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets. Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment. Basic Qualifications Experience with SQL 1+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Preferred Qualifications Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc. Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you‚Äôre applying in isn‚Äôt listed, please contact your Recruiting Partner. Company - ADCI - Karnataka Job ID: A2968106",,,"Python, SQL",
4245422254,Data Engineer-Data Platforms-AWS,IBM,"Bengaluru, Karnataka, India (Hybrid)",Hybrid,Full-time,,"About the job Introduction In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe. You'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience Your Role And Responsibilities Design, develop, and manage our data infrastructure on AWS, with a focus on data warehousing solutions. Write efficient, complex SQL queries for data extraction, transformation, and loading. Utilize DBT for data modelling and transformation. Use Python for data engineering tasks, demonstrating strong work experience in this area. Implement scheduling tools like Airflow, Control M, or shell scripting to automate data processes and workflows. Participate in an Agile environment, adapting quickly to changing priorities and requirements Preferred Education Master's Degree Required Technical And Professional Expertise Mandatory Skills: Candidate should have worked on traditional Data warehousing with any database (Oracle or DB2 or SQL Server) (Redshift optional) Candidate should have string SQL skills and ability to write complex queries using analytical functions. Prior working experience on AWS platform Python programming experience for data engineering .Experience in PySpark/Spark Working knowledge of Data Pipelines tool Airflow The below skills are nice to have: Experience with DBT, Exposure to working in an Agile environment. Proven ability to troubleshoot and resolve production issues under a DevOps model A track record of continuously identify opportunities to improve the performance and quality of your ecosystem. Experience monitoring performance and ensuring Preferred Technical And Professional Experience Knowledge of DBT for data modelling and transformation is a plus. Experience with PySpark or Spark is highly desirable",,,"Python, SQL",
4217290214,Azure Data Engineer (Trainee),Infogain,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job Roles & Responsibilities Understanding of Azure data services, including Azure SQL Database, Azure Data Lake Storage, and Azure Databricks Knowledge of data integration and ETL concepts Familiarity with SQL and programming languages such as Python or Scala Basic understanding of data modeling and database design. Good communication skills. Experience 0-2 Years Skills Primary Skill: Data Engineering Sub Skill(s): Data Engineering Additional Skill(s): databricks, Azure Data Factory, Pyspark About The Company Infogain is a human-centered digital platform and software engineering company based out of Silicon Valley. We engineer business outcomes for Fortune 500 companies and digital natives in the technology, healthcare, insurance, travel, telecom, and retail & CPG industries using technologies such as cloud, microservices, automation, IoT, and artificial intelligence. We accelerate experience-led transformation in the delivery of digital platforms. Infogain is also a Microsoft (NASDAQ: MSFT) Gold Partner and Azure Expert Managed Services Provider (MSP). Infogain, an Apax Funds portfolio company, has offices in California, Washington, Texas, the UK, the UAE, and Singapore, with delivery centers in Seattle, Houston, Austin, Krak√≥w, Noida, Gurgaon, Mumbai, Pune, and Bengaluru.",,,"Python, SQL",
4251674044,Artificial Intelligence Engineer,Homans,APAC (Remote),Remote,Full-time,,"About the job About Homans: Homans is at the forefront of AI-powered recruitment solutions, transforming the hiring landscape through our innovative AI technologies. We are looking for a passionate and motivated Artificial Intelligence Engineer to join our team and help shape the future of hiring automation. Role Overview: As an Artificial Intelligence Engineer at Homans, you will work with a team of talented engineers to develop, implement, and maintain machine learning algorithms that drive our AI recruitment products. You will have the opportunity to make a significant impact in the recruitment industry by creating and improving AI systems that enhance candidate screening, evaluation, and selection processes. Key Responsibilities: Design, develop, and deploy machine learning models to enhance Homans' AI recruiter products. Work closely with cross-functional teams to integrate AI models into product features. Collaborate on data preprocessing, feature engineering, and model evaluation to improve accuracy and performance. Keep up to date with the latest AI and machine learning trends, applying them to improve Homans' products. Ensure scalability and reliability of AI solutions deployed in production. Conduct research to explore new AI techniques and methods that can be applied to recruitment automation. Qualifications: Bachelor's degree in Computer Science, Engineering, Mathematics, or a related field. Strong foundation in machine learning algorithms, data structures, and algorithms. Familiarity with programming languages such as Python, Java, or similar. Experience with machine learning libraries like TensorFlow, PyTorch, or scikit-learn. Knowledge of natural language processing (NLP) or computer vision is a plus. Excellent problem-solving and analytical skills. Ability to work independently and as part of a team in a remote environment. Preferred Skills: Familiarity with cloud platforms like AWS, Google Cloud, or Azure. Experience with AI product deployment and optimization. Strong communication skills and the ability to work with diverse teams. Why Join Us: Work remotely from anywhere within the APAC region. Opportunity to work on cutting-edge AI technologies in the recruitment industry. Collaborative and innovative work environment. Competitive salary and benefits.",,,"Python, Machine Learning",
4230052307,Data Engineer,Uplers,"Ranchi, Jharkhand, India (Remote)",Remote,‚Çπ2.5M/yr,,"About the job Experience : 3.00 + years Salary : INR 2500000.00 / year (based on experience) Expected Notice Period : 15 Days Shift : (GMT+05:30) Asia/Kolkata (IST) Opportunity Type : Remote Placement Type : Full Time Permanent position(Payroll and Compliance to be managed by: NA) (*Note: This is a requirement for one of Uplers' client - Nomupay) What do you need for this opportunity? Must have skills required: Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL Nomupay is Looking for: üìà Opportunity in a company with a solid track record of performance ü§ù Opportunity to work with diverse, global teams üöÄ Rapid career advancement with opportunities to learn üí∞ Competitive salary and Performance bonus Design, build, and optimize scalable ETL pipelines using Apache Airflow or similar frameworks to process and transform large datasets efficiently. Utilize Spark (PySpark), Kafka, Flink, or similar tools to enable distributed data processing and real-time streaming solutions. Deploy, manage, and optimize data infrastructure on cloud platforms such as AWS, GCP, or Azure, ensuring security, scalability, and cost-effectiveness. Design and implement robust data models, ensuring data consistency, integrity, and performance across warehouses and lakes. Enhance query performance through indexing, partitioning, and tuning techniques for large-scale datasets. Manage cloud-based storage solutions (Amazon S3, Google Cloud Storage, Azure Blob Storage) and ensure data governance, security, and compliance. Work closely with data scientists, analysts, and software engineers to support data-driven decision-making, while maintaining thorough documentation of data processes. Strong proficiency in Python and SQL, with additional experience in languages such as Java or Scala. Hands-on experience with frameworks like Spark (PySpark), Kafka, Apache Hudi, Iceberg, Apache Flink, or similar tools for distributed data processing and real-time streaming. Familiarity with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure for building and managing data infrastructure. Strong understanding of data warehousing concepts and data modeling principles. Experience with ETL tools such as Apache Airflow or comparable data transformation frameworks. Proficiency in working with data lakes and cloud based storage solutions like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Expertise in Git for version control and collaborative coding. Expertise in performance tuning for large-scale data processing, including partitioning, indexing, and query optimization. NomuPay is a newly established company that through its subsidiaries will provide state of the art unified payment solutions to help its clients accelerate growth in large high growth countries in Asia, Turkey, and the Middle East region. NomuPay is funded by Finch Capital, a leading European and South East Asian Financial Technology investor. Nomu Pay has acquired WireCard Turkey on Apr 21, 2021 for an undisclosed amount. Founders Peter Burridge, CEO Investor, board member, and strategic executive, Peter has more than 30 years of management and leadership experience at rapid growth technology companies. His unique hands-on approach to business development and corporate governance has made him a trusted advisor and authority in the enterprise software industry and the financial technology sector. As President of Hyperwallet, Peter guided the organization through a successful recapitalization, followed by global expansion and the ultimate sale of the business to PayPal. Peter is a recognizable figure in the San Francisco fintech community and global payments industry. Peter has previously served in leadership roles at Oracle, Siebel, Travelex Global Business Payments, and as an investor and advisor in the technology sector. Outside the office, Peter‚Äôs passions include racing cars, golf and rugby union. How to apply for this opportunity? Step 1: Click On Apply! And Register or Login on our portal. Step 2: Complete the Screening Form & Upload updated Resume Step 3: Increase your chances to get shortlisted & meet the client for the Interview! About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. (Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well). So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you! Desired Skills and Experience Apache Hudi, Flink, Iceberg, Apache Airflow, Spark, AWS, Azure, GCP, Kafka, SQL",executive,,"Python, SQL",
4251685734,Machine Learning/AI Engineer,micro1,India (Remote),Remote,Contract,,"About the job Job Title: Machine Learning Engineer Job Type: Full-time, Contractor About Us: Our mission at micro1 is to match the most talented people in the world with their dream jobs. If you are looking to be at the forefront of AI innovation and work with some of the fastest-growing companies in Silicon Valley, we invite you to apply for a role. By joining the micro1 community, your resume will become visible to top industry leaders, unlocking access to the best career opportunities on the market. Job Summary: Join our dynamic team as a Machine Learning Engineer dedicated to pushing the boundaries of AI technologies. We are seeking an insightful and innovative engineer to contribute to our ongoing project with a focus on end-to-end development and deployment of ML/AI solutions involving OCR technologies. Key Responsibilities Design and implement scalable machine learning models and AI solutions. Develop, test, and deploy end-to-end ML pipelines with a focus on OCR technologies. Collaborate with cross-functional teams to understand project objectives and deliver impactful solutions. Ensure data integrity and proper data management using MongoDB and other tools. Analyze and improve the efficiency, scalability, and stability of deployed models. Document and communicate project progress, highlighting technical challenges and solutions. Stay updated with the latest advancements in machine learning, AI, and OCR technologies. Deployment on AWS environment. Model training. Reinforced learning. Required Skills and Qualifications Proficiency in Python and its libraries for machine learning. Strong understanding and practical experience with machine learning algorithms and AI principles. Proven experience with MongoDB for data management and integration. Excellent written and verbal communication skills, demonstrating the ability to convey technical concepts clearly. Experience in the complete development and deployment lifecycle of an ML/AI project. Capacity to work independently and collaboratively in a remote environment. Preferred Qualifications Experience with Optical Character Recognition (OCR) technologies. Familiarity with cloud platforms and deploying models on cloud infrastructure. Previous experience in building models for large-scale data applications.",,,"Python, Machine Learning",
4249779333,Database Engineer,Aptita,Mumbai Metropolitan Region (On-site),On-site,Full-time,,"About the job Role : Database Engineer Location : Remote Notice Period : 30 Days Skills And Experience Bachelor's degree in Computer Science, Information Systems, or a related field is desirable but not essential. Experience with data warehousing concepts and tools (e.g., Snowflake, Redshift) to support advanced analytics and reporting, aligning with the team‚Äôs data presentation goals. Skills in working with APIs for data ingestion or connecting third-party systems, which could streamline data acquisition processes. Proficiency with tools like Prometheus, Grafana, or ELK Stack for real-time database monitoring and health checks beyond basic troubleshooting. Familiarity with continuous integration/continuous deployment (CI/CD) tools (e.g., Jenkins, GitHub Actions). Deeper expertise in cloud platforms (e.g., AWS Lambda, GCP Dataflow) for serverless data processing or orchestration. Knowledge of database development and administration concepts, especially with relational databases like PostgreSQL and MySQL. Knowledge of Python programming, including data manipulation, automation, and object-oriented programming (OOP), with experience in modules such as Pandas, SQLAlchemy, gspread, PyDrive, and PySpark. Knowledge of SQL and understanding of database design principles, normalization, and indexing. Knowledge of data migration, ETL (Extract, Transform, Load) processes, or integrating data from various sources. Knowledge of cloud-based databases, such as AWS RDS and Google BigQuery. Eagerness to develop import workflows and scripts to automate data import processes. Knowledge of data security best practices, including access controls, encryption, and compliance standards. Strong problem-solving and analytical skills with attention to detail. Creative and critical thinking. Strong willingness to learn and expand knowledge in data engineering. Familiarity with Agile development methodologies is a plus. Experience with version control systems, such as Git, for collaborative development. Ability to thrive in a fast-paced environment with rapidly changing priorities. Ability to work collaboratively in a team environment. Good and effective communication skills. Comfortable with autonomy and ability to work independently.",,,"Python, SQL",
4222243406,Data Engineer,Virtusa,"Andhra Pradesh, India (Hybrid)",Hybrid,Full-time,,"About the job Seeking a skilled Data Engineer to work on cloud-based data pipelines and analytics platforms. The ideal candidate will have hands-on experience in PySpark and AWS, with proficiency in designing Data Lakes and working with modern data orchestration tools. Data Engineer to work on cloud-based data pipelines and analytics platforms PySpark and AWS, with proficiency in designing Data Lakes working with modern data orchestration tools Desired Skills and Experience Docker, Terraform, Jenkins, DevOps, CloudFormation, Cloudera Data platform, CI & CD, PySpark",,,,
4234851294,GCP Data Engineer,TELUS Digital,"Noida, Uttar Pradesh, India (On-site)",On-site,Full-time,,"About the job About Us : At TELUS Digital, we enable customer experience innovation through spirited teamwork, agile thinking, and a caring culture that puts customers first. TELUS Digital is the global arm of TELUS Corporation, one of the largest telecommunications service providers in Canada. We deliver contact center and business process outsourcing (BPO) solutions to some of the world's largest corporations in the consumer electronics, finance, telecommunications and utilities sectors. With global call center delivery capabilities, our multi-shore, multi-language programs offer safe, secure infrastructure, value-based pricing, skills-based resources and exceptional customer service - all backed by TELUS, our multi-billion dollar telecommunications parent. Required Skills : 4+ years of industry experience in software development, data engineering, business intelligence, or related field with experience in manipulating, processing, and extracting value from datasets. Design, build and deploy internal applications to support our technology life cycle, collaboration and spaces, service delivery management, data and business intelligence among others Building Modular code for multi usable pipeline or any kind of complex Ingestion Framework used to ease the job to load the data into Datalake or Data Warehouse from multiple sources Work closely with analysts and business process owners to translate business requirements into technical solutions Coding experience in scripting and languages (Python, SQL, PySpar k) Expertise in Google Cloud Platform (GCP) technologies in the data warehousing space (BigQuer y, Dataproc, GCP Workflows, Dataflow, Cloud Scheduler, Secret Manager , Batch, Cloud Logging, Cloud SDK, Google Cloud Storage, IAM, Vertex AI) Maintain highest levels of development practices including: technical design, solution development, systems configuration, test documentation/execution, issue identification and resolution, writing clean, modular and self-sustaining code, with repeatable quality and predictability Understanding CI/CD Processes using Pulumi , Github, Cloud Build, Cloud SDK, Docker Experience with SAS/SQL Server/SSIS is an added advantage . Soft Sk i lls : Strong analytical and problem-solving abilities Excellent communication skills, both written and verbal Ability to work effectively in a team environment Self-motivated with a proactive approach to learning new technologies",Manager,,"Python, SQL",
4241692289,Data Engineer- Azure Databricks,Tata Consultancy Services,"Kolkata, West Bengal, India (On-site)",On-site,Full-time,,"About the job Build the solution for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure data ingestion and transformation components. Following technology skills are required ‚Äì Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience with ADF, Dataflow Experience with big data tools like Delta Lake, Azure Databricks Experience with Synapse Designing an Azure Data Solution skills Assemble large, complex data sets that meet functional / non-functional business requirements.",,,SQL,
4243903147,Data Engineer,Astreya,India (Remote),Remote,Full-time,,"About the job Data Engineer Astreya offers comprehensive IT support and managed services. These services include Data Center and Network Management, Digital Workplace Services (like Service Desk, Audio Visual, and IT Asset Management), as well as Next-Gen Digital Engineering services encompassing Software Engineering, Data Engineering, and cybersecurity solutions. Astreya's expertise lies in creating seamless interactions between people and technology to help organizations achieve operational excellence and growth. Job Description We are seeking experienced Data Engineer to join our analytics division. You will be aligned with our Data Analytics and BI vertical. You will have to conceptualize and own the build out of problem-solving data marts for consumption by data science and BI teams, evaluating design and operational tradeoffs within systems. Design, develop, and maintain robust data pipelines and ETL processes using data platforms for the organization's centralized data warehouse. Create or contribute to frameworks that improve the efficacy of logging data, while working with the Engineering team to triage issues and resolve them. Validate data integrity throughout the collection process, performing data profiling to identify and comprehend data anomalies. Influence product and cross-functional (engineering, data science, operations, strategy) teams to identify data opportunities to drive impact. Requirements Experience & Education Bachelor's degree in Computer Science, Mathematics, a related field, or equivalent practical experience. 5 years of experience coding with SQL or one or more programming languages (e.g., Python, Java, R, etc.) for data manipulation, analysis, and automation 5 years of experience designing data pipelines (ETL) and dimensional data modeling for synchronous and asynchronous system integration and implementation. Experience in managing troubleshooting technical issues, and working with Engineering and Sales Services teams. Preferred qualifications: Master‚Äôs degree in Engineering, Computer Science, Business, or a related field. Experience with cloud-based services relevant to data engineering, data storage, data processing, data warehousing, real-time streaming, and serverless computing. Experience with experimentation infrastructure, and measurement approaches in a technology platform. Experience with data processing software (e.g., Hadoop, Spark) and algorithms (e.g., MapReduce, Flume).",manager,,"Python, SQL, R",
4251670786,Remote Python AI Engineer - 17852,Turing,"Bengaluru, Karnataka, India (Remote)",Save Remote Python AI Engineer - 17852¬†  at Turing,Contract,,"About the job Work on Real-World Problems with Global Tech Experts Join a leading U.S.-based technology company as a Python Developer / AI Engineer, where you‚Äôll tackle real-world challenges and build innovative solutions alongside top global experts. This is a fully remote, contract-based opportunity ideal for developers passionate about Python, data analysis, and AI-driven work. Key Responsibilities: Write efficient, production-grade Python code to solve complex problems. Analyze public datasets and extract meaningful insights using Python and SQL. Collaborate with researchers and global teams to iterate on data-driven ideas. Document all code and development decisions in Jupyter Notebooks or similar platforms. Maintain high-quality standards and contribute to technical excellence. Job Requirements: Open to all levels: junior, mid-level, or senior engineers. Degree in Computer Science, Engineering, or equivalent practical experience. Proficient in Python programming for scripting, automation, or backend development. Experience with SQL/NoSQL databases is a plus. Familiarity with cloud platforms (AWS, GCP, Azure) is advantageous. Must be able to work 5+ hours overlapping with Pacific Time (PST/PT). Strong communication and collaboration skills in a remote environment. Perks & Benefits: Work on cutting-edge AI and data projects impacting real-world use cases. Collaborate with top minds from Meta, Stanford, and Google. 100% remote ‚Äì work from anywhere. Contract role with flexibility and no traditional job constraints. Competitive compensation in USD, aligned with global tech standards. Selection Process: Shortlisted developers may be asked to complete an assessment. If you clear the assessment, you will be contacted for contract assignments with expected start dates, durations, and end dates. Some contract assignments require fixed weekly hours, averaging 20/30/40 hours per week for the duration of the contract assignment.",,,"Python, SQL, Data Analysis",
4233120139,Data Engineer,Aryng,"Bengaluru, Karnataka, India (Remote)",Remote,Full-time,,"About the job Welcome! You made it to the job description page! Aryng is looking for a Data Engineer with experience in developing enterprise-class distributed data engineering solutions on the cloud. We are seeking an entrepreneurial and technology-proficient Data Engineer who is an expert in the implementation of a large-scale, highly efficient data platform, batch, and real-time pipelines and tools for Aryng clients. This role is based out of India. You will work closely with a team of highly qualified data scientists, business analysts, and engineers to ensure we build effective solutions for our clients. Your biggest strength is creative and effective problem-solving. Key Responsibilities: Should have implement asynchronous data ingestion, high volume stream data processing, and real-time data analytics using various Data Engineering Techniques Implement application components using Cloud technologies and infrastructure Assist in defining the data pipelines and able to identify bottlenecks to enable the adoption of data management methodologies Implementing cutting edge cloud platform solutions using the latest tools and platforms offered by GCP, AWS, and Azure. (AWS is preferred) Functional capabilities : Requirement gathering, Client Mgt, team handling, Program delivery, Project Management (Project Estimation, Scope of Project, Agile methodology). Requirements 3-5 years of data engineering experience is a must 3+ years implementing and managing data engineering solutions using Cloud solutions GCP/AWS/Azure or on-premise distributed servers. AWS is preferred. Should be comfortable working and interacting with clients 2+ years' experience in Python Must be strong in SQL and its concepts Experience in Big Query, Snowflake, Redshift, DBT Strong understanding of data warehousing, data lake, and cloud concepts Excellent communication and presentation skills Excellent problem-solving skills, highly proactive and self-driven Consulting background is a big plus Must have a B.S. in computer science, software engineering, computer engineering, electrical engineering, or a related area of study Working knowledge of Airflow is preferred Good to have: Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark Experience in Tableau, Looker, or other BI tools is preferred Availability: Available to join immediately This role requires mandatory overlap hours with clients in the US from 8 am to 1 pm PST. Benefits Direct Client Access Flexible work hours Rapidly Growing Company Awesome work culture Learn From Experts Work-life Balance Competitive Salary Executive Presence End to End Problem Solving 50%+ Tax Benefit 100% Remote company Flat Hierarchy Opportunity to become a thought leader Why Join Aryng : Click on the Youtube link",Executive,,"Python, SQL, Tableau",
