# Project Presentation Q&A

## Possible Questions from Teachers

### Technical Questions
1. How does your web scraping script handle dynamic content and pagination on LinkedIn?
2. Why did you choose Playwright over other automation tools like Selenium?
3. How do you ensure your script does not get blocked by LinkedIn’s anti-bot measures?
4. What challenges did you face while extracting structured data from HTML files?
5. How do you handle missing or inconsistent data in your extraction process?
6. Can your script be easily adapted to scrape other job roles or websites? How?
7. What are the main fields you extract from each job posting, and why are they important?
8. How do you ensure the accuracy and reliability of your extracted data?
9. What libraries did you use for HTML parsing and why?
10. How do you automate the process for multiple job roles?

### Data & Visualization Questions
1. How did you clean and preprocess the data before importing it into Power BI?
2. What types of visualizations did you create in Power BI, and what insights do they provide?
3. How do you handle duplicate job postings in your dataset?
4. What trends did you observe in the job market for different IT roles?
5. How would you update your dashboards if new data is collected?

### General & Conceptual Questions
1. What are the ethical considerations of web scraping from platforms like LinkedIn?
2. How scalable is your solution for larger datasets or more frequent data collection?
3. What improvements would you make if you had more time or resources?
4. How can this project be useful for job seekers or recruiters?
5. What are the limitations of your current approach?

### Team & Process Questions
1. How did you divide the work among team members?
2. What was the most challenging part of the project for your team?
3. How did you test and validate your scripts and dashboards?
4. How would you document this project for someone else to use or extend?

---

## Model Answers (Based on Our Project)

1. **How does your web scraping script handle dynamic content and pagination on LinkedIn?**
   - Our script uses Playwright, which can interact with dynamic web pages just like a real user. For pagination, after scraping all jobs on the current page, we locate and click the “View next page” button using its aria-label. We repeat this process in a loop until the last page is reached, as detected by the button’s disabled state or an exception.

2. **Why did you choose Playwright over other automation tools like Selenium?**
   - Playwright is modern, supports multiple browsers, and handles dynamic content and JavaScript-heavy sites more reliably than Selenium. It also provides features like auto-waiting for elements and better support for headless and headed modes, which is useful for scraping LinkedIn.

3. **How do you ensure your script does not get blocked by LinkedIn’s anti-bot measures?**
   - We use slow motion (`slow_mo=50`) and run the browser in non-headless mode to mimic human behavior. We also add timeouts between actions. However, for production use, we would recommend rotating user agents, using proxies, and adding random delays to further reduce the risk of being blocked.

4. **What challenges did you face while extracting structured data from HTML files?**
   - The main challenge is inconsistent HTML structure across job postings. Some fields may be missing or formatted differently. We use BeautifulSoup and regular expressions in our extraction script to handle these variations, but some manual adjustments are sometimes needed.

5. **How do you handle missing or inconsistent data in your extraction process?**
   - If a field is missing in a job posting, our script assigns a default value (like an empty string or “N/A”). We also clean the data in Power BI, where we can filter out or highlight incomplete records.

6. **Can your script be easily adapted to scrape other job roles or websites? How?**
   - Yes. To scrape other roles, we just change the search keywords in the script. For other websites, we would need to update the selectors and possibly the login process, but the overall structure of the script would remain similar.

7. **What are the main fields you extract from each job posting, and why are they important?**
   - We extract job title, company, location, required skills, qualifications, experience, and industry domain. These fields help us analyze demand, skill requirements, and trends in the job market.

8. **How do you ensure the accuracy and reliability of your extracted data?**
   - We validate the extraction by manually checking a sample of the output. In Power BI, we also look for outliers or missing values, which can indicate extraction errors.

9. **What libraries did you use for HTML parsing and why?**
   - We use BeautifulSoup for HTML parsing because it is easy to use, efficient, and well-suited for extracting data from complex HTML structures.

10. **How do you automate the process for multiple job roles?**
    - We run the script multiple times with different search keywords and output directories. Each run saves HTML files and extracted CSVs for a specific role, which we later combine for analysis.

11. **How did you clean and preprocess the data before importing it into Power BI?**
    - We remove duplicates, handle missing values, and standardize field formats in Power BI after importing the CSVs. This ensures the data is ready for visualization.

12. **What types of visualizations did you create in Power BI, and what insights do they provide?**
    - We created bar charts for job counts by role and location, pie charts for skill distribution, and line charts for trends over time. These visualizations help us identify in-demand skills, top hiring locations, and overall job market trends.

13. **How do you handle duplicate job postings in your dataset?**
    - We use Power BI's built-in tools to remove duplicate job postings by checking for unique job IDs or a combination of job title, company, and location. This ensures that each job is only counted once in our analysis.

14. **What trends did you observe in the job market for different IT roles?**
    - We observed that roles like Data Scientist, Full Stack Developer, and Cloud Engineer are in high demand. There is also a noticeable increase in remote job postings and a growing emphasis on skills like Python, Machine Learning, and cloud technologies.

15. **How would you update your dashboards if new data is collected?**
    - We would rerun our extraction and cleaning scripts to generate updated CSV files, then refresh the data source in Power BI. The dashboards are designed to update automatically when new data is loaded.

16. **What are the ethical considerations of web scraping from platforms like LinkedIn?**
    - Ethical considerations include respecting the website’s terms of service, not overloading servers, and ensuring that scraped data is used responsibly and not for spamming or unauthorized redistribution. We also avoid collecting personal or sensitive information.

17. **How scalable is your solution for larger datasets or more frequent data collection?**
    - Our solution can be scaled by running the script on more powerful hardware or in the cloud. For frequent data collection, we can schedule the script to run at regular intervals and use parallel processing to speed up extraction.

18. **What improvements would you make if you had more time or resources?**
    - We would automate the entire pipeline, add more robust error handling, use proxies and user-agent rotation for better anti-bot evasion, and expand the analysis to more job roles and regions. We would also consider using a database instead of CSV files for better scalability.

19. **How can this project be useful for job seekers or recruiters?**
    - Job seekers can use the insights to identify in-demand skills and locations, while recruiters can spot hiring trends and skill gaps. The dashboards provide a clear overview of the job market, helping both groups make informed decisions.

20. **What are the limitations of your current approach?**
    - Limitations include possible missing or inaccurate data due to changes in website structure, anti-bot measures, and the static nature of CSV files. The script may also miss jobs that require more complex navigation or authentication.

21. **How did you divide the work among team members?**
    - We divided the work based on individual strengths: one member focused on web scraping, another on data cleaning and analysis, and another on Power BI visualization and documentation. We collaborated on testing and troubleshooting.

22. **What was the most challenging part of the project for your team?**
    - The most challenging part was handling dynamic web content and anti-bot measures on LinkedIn. Ensuring data consistency and dealing with missing fields also required extra effort.

23. **How did you test and validate your scripts and dashboards?**
    - We tested the scripts on multiple job roles and manually checked the extracted data for accuracy. In Power BI, we validated the dashboards by comparing results with known job postings and checking for anomalies.

24. **How would you document this project for someone else to use or extend?**
    - We would provide a detailed README with setup instructions, code comments, and sample configuration files. We would also include usage examples, troubleshooting tips, and guidelines for adapting the scripts to other roles or websites.
